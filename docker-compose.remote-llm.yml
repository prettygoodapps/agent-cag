version: '3.9'

# Override configuration for remote LLM API mode
# Use this when you want to use a remote LLM API instead of local Ollama
# Usage: docker-compose -f docker-compose.yml -f docker-compose.remote-llm.yml up -d

services:
  # Remove the containerized Ollama service
  ollama:
    deploy:
      replicas: 0

  # Update LLM service for remote API
  llm:
    environment:
      - PYTHONPATH=/app
      - LLM_PROVIDER=groq
      - MODEL_NAME=llama3-8b-8192
      - LLM_API_KEY=${GROQ_API_KEY:-your_groq_api_key_here}
      - LLM_BASE_URL=https://api.groq.com/openai/v1
    depends_on: []